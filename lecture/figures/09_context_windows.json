{
  "title": "Context Windows in 2026",
  "slide": "32 — Context Window Growth",

  "concept": "A horizontal bar chart showing the dramatic growth of LLM context windows from GPT-4 (8K tokens in 2023) to Llama 4 Scout (10M tokens in 2025). The bars should use a logarithmic-feeling visual treatment — the last bar (10M) is so much larger than the first (8K) that a linear chart won't work. Solution: use a broken/compressed x-axis or explicitly label the scale jump. The takeaway: we went from 'a few pages' to 'an entire library' in 2 years.",

  "format": {
    "dimensions": "1920 × 1080 pixels (16:9 ratio)",
    "orientation": "Horizontal / Landscape",
    "bleed": "Full bleed, no borders, no frames"
  },

  "background": {
    "color": "#F5EFE6 (warm cream)",
    "texture": "None"
  },

  "chart": {
    "type": "Horizontal bar chart with scale annotations",
    "sort_order": "Smallest context at top, largest at bottom (chronological + dramatic build)",

    "data": [
      {"model": "GPT-4 (2023)", "tokens_millions": 0.008, "label": "8K", "real_world": "~6 pages", "bar_color": "#9CA3AF", "year": "2023"},
      {"model": "Claude 3", "tokens_millions": 0.2, "label": "200K", "real_world": "~150 pages", "bar_color": "#9CA3AF", "year": "2024"},
      {"model": "GPT-5.2", "tokens_millions": 0.4, "label": "400K", "real_world": "~300 pages", "bar_color": "#E07850", "year": "2025"},
      {"model": "Qwen 2.5-1M", "tokens_millions": 1.0, "label": "1M", "real_world": "~750 pages", "bar_color": "#D4A855", "year": "2025"},
      {"model": "Gemini 3 Pro", "tokens_millions": 2.0, "label": "2M", "real_world": "~1,500 pages", "bar_color": "#E07850", "year": "2025"},
      {"model": "Llama 4 Maverick", "tokens_millions": 1.0, "label": "1M", "real_world": "~750 pages", "bar_color": "#D4A855", "year": "2025"},
      {"model": "Llama 4 Scout", "tokens_millions": 10.0, "label": "10M", "real_world": "~7,500 pages", "bar_color": "#E07850", "year": "2025"}
    ],

    "x_axis": {
      "label": "Context window (tokens)",
      "scale": "Log-like or broken — show the 1000x growth clearly",
      "note": "Use a visual break or compressed middle section if linear doesn't work"
    },

    "annotations": [
      {
        "type": "callout on Llama 4 Scout bar",
        "text": "10M tokens ≈ 7,500 pages — an entire library",
        "color": "#E07850",
        "style": "Bold annotation with arrow to the bar"
      },
      {
        "type": "callout on GPT-4 bar",
        "text": "8K tokens ≈ 6 pages (2023)",
        "color": "#9CA3AF",
        "style": "Smaller, muted"
      },
      {
        "type": "growth label",
        "text": "1,250x growth in 2 years",
        "position": "Right side, spanning from top bar to bottom bar",
        "color": "#E07850",
        "style": "Large, with vertical double-arrow"
      }
    ],

    "right_column": {
      "what": "Real-world equivalents next to each bar",
      "examples": "pages, book equivalents",
      "color": "#6B7280",
      "note": "Helps the audience grasp abstract token counts"
    }
  },

  "typography": {
    "title": {
      "text": "Context Windows in 2026",
      "position": "Top-left",
      "keyword": "2026",
      "keyword_color": "#E07850",
      "other_color": "#1E1E2E",
      "font": "Heavy sans-serif"
    },
    "subtitle": {
      "text": "From 6 pages to 7,500 pages in 2 years",
      "color": "#6B7280"
    }
  },

  "color_palette_summary": {
    "background": "#F5EFE6",
    "text_primary": "#1E1E2E",
    "text_secondary": "#6B7280",
    "bars_2023": "#9CA3AF",
    "bars_open_source": "#D4A855",
    "bars_frontier": "#E07850",
    "gridlines": "#D8D0C4",
    "note": "Color progression matches model recency"
  },

  "critical_requirements": [
    "Background MUST be warm cream #F5EFE6",
    "The 1000x+ growth must be VISUALLY dramatic",
    "Real-world page equivalents next to each bar",
    "The '1,250x growth' annotation is the key takeaway",
    "16:9 landscape format",
    "Readable from back of lecture hall"
  ]
}
