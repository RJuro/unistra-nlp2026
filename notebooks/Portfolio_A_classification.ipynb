{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio A: Text Classification\n",
    "**Build a content moderation pipeline for an AI agent social network**\n",
    "\n",
    "You've been hired to build a content classifier for *Moltbook* \u2014 a leaked social network where AI agents post, argue, and share memes. Your job: classify posts into 9 content categories and identify patterns in how agents communicate.\n",
    "\n",
    "**Dataset**: Moltbook (44K AI agent posts, 9 categories)\n",
    "**Your goal**: Build and compare at least 2 classification approaches, evaluate rigorously, and explain what works and why.\n",
    "\n",
    "### Deliverables\n",
    "- Working classification pipeline with at least 2 approaches\n",
    "- Evaluation: confusion matrix, per-class F1, macro F1\n",
    "- Error analysis: 5+ misclassified examples with explanations\n",
    "- Brief model card (data, method, limits, failure modes)\n",
    "\n",
    "**Estimated time**: Sprint 1 (55 min) + Sprint 2 (90 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets sentence-transformers openai scikit-learn matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"TrustAIRLab/Moltbook\", \"posts\", split=\"train\")\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Flatten nested post column\n",
    "df[\"title\"] = df[\"post\"].apply(lambda x: x.get(\"title\", \"\") if isinstance(x, dict) else \"\")\n",
    "df[\"content\"] = df[\"post\"].apply(lambda x: x.get(\"content\", \"\") if isinstance(x, dict) else \"\")\n",
    "df[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\n",
    "df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n",
    "df[\"text\"] = (df[\"title\"].str.strip() + \" . \" + df[\"content\"].str.strip()).str.strip()\n",
    "\n",
    "# Use topic_label as classification target\n",
    "df = df[df[\"text\"].str.len() > 10].reset_index(drop=True)\n",
    "print(f\"Dataset: {len(df)} posts\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df[\"topic_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "for label in df[\"topic_label\"].unique()[:3]:\n",
    "    example = df[df[\"topic_label\"] == label].iloc[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Text: {example['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing & Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Subsample for speed (use full dataset if time allows)\n",
    "df_work = df.sample(5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_work[\"text_clean\"],\n",
    "    df_work[\"topic_label\"],\n",
    "    test_size=0.25,\n",
    "    stratify=df_work[\"topic_label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: TF-IDF + Logistic Regression\n",
    "This is your floor \u2014 every other approach should beat this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=10_000)),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, random_state=42)),\n",
    "])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred_baseline = pipe_lr.predict(X_test)\n",
    "\n",
    "print(f\"TF-IDF + LR Accuracy: {accuracy_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(classification_report(y_test, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_baseline, ax=ax, cmap=\"Blues\", xticks_rotation=45)\n",
    "ax.set_title(\"Baseline: TF-IDF + Logistic Regression\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your Turn: Add a Second Approach\n",
    "\n",
    "Choose one (or more):\n",
    "- **Sentence embeddings** (from NB02): Encode with `all-MiniLM-L6-v2`, train a classifier on embeddings\n",
    "- **LLM zero-shot** (from NB03): Use Groq API to classify without training data\n",
    "- **SetFit few-shot** (from NB05): Train on just 8-32 examples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE \u2014 Approach 2\n",
    "# Example: Sentence Embeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# X_train_emb = model.encode(X_train.tolist(), show_progress_bar=True)\n",
    "# X_test_emb = model.encode(X_test.tolist(), show_progress_bar=True)\n",
    "# clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# clf.fit(X_train_emb, y_train)\n",
    "# y_pred_emb = clf.predict(X_test_emb)\n",
    "# print(f\"SBERT + LR Accuracy: {accuracy_score(y_test, y_pred_emb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE \u2014 fill in your results\n",
    "results = {\n",
    "    \"TF-IDF + LR\": accuracy_score(y_test, y_pred_baseline),\n",
    "    # \"SBERT + LR\": accuracy_score(y_test, y_pred_emb),\n",
    "    # \"LLM Zero-shot\": ...,\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(list(results.items()), columns=[\"Method\", \"Accuracy\"])\n",
    "results_df = results_df.sort_values(\"Accuracy\", ascending=False)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "Look at the hardest examples \u2014 where does each model fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = pipe_lr.predict_proba(X_test)\n",
    "error_df = pd.DataFrame({\n",
    "    \"text\": X_test.values,\n",
    "    \"true_label\": y_test.values,\n",
    "    \"predicted\": y_pred_baseline,\n",
    "    \"confidence\": proba.max(axis=1),\n",
    "})\n",
    "errors = error_df[error_df[\"true_label\"] != error_df[\"predicted\"]].sort_values(\"confidence\", ascending=False)\n",
    "print(f\"Misclassified: {len(errors)}/{len(error_df)} ({len(errors)/len(error_df)*100:.1f}%)\\n\")\n",
    "\n",
    "for i, row in errors.head(5).iterrows():\n",
    "    print(f\"True: {row['true_label']} \\u2192 Predicted: {row['predicted']} (conf: {row['confidence']:.2f})\")\n",
    "    print(f\"  {row['text'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Card\n",
    "\n",
    "Fill this in when you're done:\n",
    "\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| **Task** | 9-class content classification |\n",
    "| **Dataset** | Moltbook (N=5000 sample) |\n",
    "| **Best method** | _your best approach_ |\n",
    "| **Macro F1** | _score_ |\n",
    "| **Key strength** | _what it gets right_ |\n",
    "| **Key weakness** | _what it gets wrong_ |\n",
    "| **Failure mode** | _describe a systematic error pattern_ |\n",
    "| **Improvement idea** | _what you'd try next_ |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}