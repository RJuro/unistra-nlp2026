{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB08: Distillation — LLM Label Synthesis + Structured Extraction\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB08_distillation.ipynb)\n",
    "\n",
    "**Time:** ~65 minutes\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Generate structured training data using LLMs** — use a large model as a teacher to produce rich JSON annotations, not just labels\n",
    "2. **Build a synthetic data pipeline** — generate realistic synthetic examples for underrepresented classes\n",
    "3. **Apply confidence filtering and quality controls** — filter noisy LLM outputs before training\n",
    "4. **Distill to a fast classifier** — train a lightweight sklearn model on the teacher's labels\n",
    "5. **Understand the limits of distillation** — see why classification-only distillation loses the structured extraction capability\n",
    "\n",
    "---\n",
    "\n",
    "> **Dataset:** 54K EUIPO trademark filings — we classify them under the **EU AI Act** risk framework.\n",
    ">\n",
    "> **Teacher model:** `moonshotai/kimi-k2-instruct-0905` via Groq (1T params, 32B active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pydantic pandas scikit-learn sentence-transformers tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Idea: Structured Distillation\n",
    "\n",
    "In NB03 we saw that LLMs can classify text in zero-shot mode. But real-world NLP often needs more than a single label — it needs **structured extraction**: pulling out multiple fields, lists, and rationales from text.\n",
    "\n",
    "The problem: large LLMs that can do this well are **expensive and slow**. The solution is **knowledge distillation**:\n",
    "\n",
    "1. **Teacher:** Use a large LLM to produce **rich structured annotations** (one-time cost)\n",
    "2. **Student (classification):** Train a fast sklearn classifier on just the labels — works for simple classification\n",
    "3. **Student (structured):** Fine-tune a small LM to reproduce the *full* structured output (→ NB09)\n",
    "\n",
    "The key insight: a TF-IDF or embedding classifier can learn the *label*, but only a language model can learn to produce the full structured extraction. That's why we need both NB08 and NB09."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# If not set above, try Colab secrets → then environment variable\n",
    "if not GROQ_API_KEY:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "    except (ImportError, Exception):\n",
    "        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "TEACHER_MODEL = \"moonshotai/kimi-k2-instruct-0905\"\n",
    "\n",
    "# Test the connection\n",
    "resp = client.chat.completions.create(\n",
    "    model=TEACHER_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'ready'\"}],\n",
    "    max_tokens=5\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Dataset: EUIPO Trademark Filings\n",
    "\n",
    "We use 54,051 trademark filings from the **European Union Intellectual Property Office** (EUIPO), covering digital products filed between 2015 and 2020.\n",
    "\n",
    "Each filing has a `full_description` field — standardized goods/services terms accepted by EUIPO examiners. Our task: classify each trademark under the **EU AI Act** risk framework *and* extract structured information about its AI capabilities.\n",
    "\n",
    "**Why this matters:** The EU AI Act (entered into force August 2024) creates legal obligations based on the *risk tier* of an AI system. Trademark filings from 2015–2020 tell us what AI products companies were *already building* before the regulation existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trademark data\n",
    "DATA_URL = \"https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/data/trademarks/euipo_tm_data.csv\"\n",
    "\n",
    "try:\n",
    "    tm_df = pd.read_csv(\"../data/trademarks/euipo_tm_data.csv\", index_col=0)\n",
    "except FileNotFoundError:\n",
    "    tm_df = pd.read_csv(DATA_URL, index_col=0)\n",
    "\n",
    "print(f\"Total trademarks: {len(tm_df):,}\")\n",
    "print(f\"Columns: {list(tm_df.columns)}\")\n",
    "print(f\"\\nAverage description length: {tm_df['full_description'].str.len().mean():.0f} chars\")\n",
    "print(f\"\\nSample descriptions:\")\n",
    "for i in [0, 100, 500]:\n",
    "    desc = tm_df.iloc[i]['full_description'][:150]\n",
    "    print(f\"  [{tm_df.iloc[i]['owner_name'][:30]}]: {desc}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify AI-adjacent trademarks using keyword matching\n",
    "AI_KEYWORDS = [\n",
    "    'artificial intelligence', 'machine learning', 'deep learning', 'neural network',\n",
    "    'biometric', 'facial recognition', 'voice recognition', 'speech recognition',\n",
    "    'image recognition', 'computer vision', 'natural language', 'autonomous',\n",
    "    'predictive', 'chatbot', 'virtual assistant', 'robot', 'data mining'\n",
    "]\n",
    "\n",
    "desc_lower = tm_df['full_description'].str.lower()\n",
    "tm_df['has_ai_keyword'] = desc_lower.apply(\n",
    "    lambda x: any(kw in str(x) for kw in AI_KEYWORDS)\n",
    ")\n",
    "\n",
    "ai_subset = tm_df[tm_df['has_ai_keyword']]\n",
    "print(f\"AI-adjacent trademarks: {len(ai_subset):,} / {len(tm_df):,} ({len(ai_subset)/len(tm_df):.1%})\")\n",
    "\n",
    "# Keyword frequencies\n",
    "print(f\"\\nKeyword frequencies:\")\n",
    "for kw in sorted(AI_KEYWORDS, key=lambda k: desc_lower.str.contains(k).sum(), reverse=True)[:10]:\n",
    "    count = desc_lower.str.contains(kw).sum()\n",
    "    print(f\"  {kw}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structured Output Schema\n",
    "\n",
    "Here's where we go beyond simple classification. Instead of just a label, the teacher model produces a **structured assessment** with multiple fields:\n",
    "\n",
    "| Field | Type | Purpose |\n",
    "|-------|------|--------|\n",
    "| `is_ai_related` | `bool` | Binary filter — is this even an AI product? |\n",
    "| `risk_tier` | `Literal` | The classification label (5 classes) |\n",
    "| `confidence` | `float` | Self-reported confidence 0–1 |\n",
    "| `ai_capabilities` | `list[str]` | Extracted AI capabilities from the description |\n",
    "| `target_sectors` | `list[str]` | Inferred application domains |\n",
    "| `risk_rationale` | `str` | Free-text explanation of the classification |\n",
    "\n",
    "This is what makes distillation to NB09 interesting: a sklearn classifier can learn `risk_tier`, but **only a language model** can learn to produce the full structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RISK_TIERS = [\"unacceptable\", \"high\", \"limited\", \"minimal\", \"not_ai\"]\n",
    "\n",
    "class AIActAssessment(BaseModel):\n",
    "    \"\"\"EU AI Act regulatory assessment of a trademark filing.\"\"\"\n",
    "    is_ai_related: bool = Field(description=\"Whether the trademark covers AI-related goods/services\")\n",
    "    risk_tier: Literal[\"unacceptable\", \"high\", \"limited\", \"minimal\", \"not_ai\"] = Field(\n",
    "        description=\"EU AI Act risk classification\"\n",
    "    )\n",
    "    confidence: float = Field(ge=0, le=1, description=\"Confidence in the risk tier assignment\")\n",
    "    ai_capabilities: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Specific AI capabilities mentioned, e.g. ['facial recognition', 'predictive analytics']\"\n",
    "    )\n",
    "    target_sectors: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Application domains, e.g. ['healthcare', 'law enforcement', 'finance']\"\n",
    "    )\n",
    "    risk_rationale: str = Field(description=\"1-2 sentence explanation of why this tier was assigned\")\n",
    "\n",
    "# Show the schema\n",
    "print(json.dumps(AIActAssessment.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an EU AI Act compliance analyst. Given a EUIPO trademark goods/services description, produce a structured regulatory assessment as JSON.\n",
    "\n",
    "The EU AI Act defines these risk tiers:\n",
    "- unacceptable: Social scoring, real-time biometric mass surveillance, subliminal manipulation\n",
    "- high: Biometric identification, hiring/recruitment tools, credit scoring, law enforcement, medical devices, critical infrastructure, education assessment\n",
    "- limited: Chatbots, emotion detection, deepfake generation (transparency obligations only)\n",
    "- minimal: Spam filters, game AI, recommendation engines, search tools (no obligations)\n",
    "- not_ai: Products with no AI component\n",
    "\n",
    "Return a JSON object with these fields:\n",
    "- is_ai_related (bool)\n",
    "- risk_tier (one of: unacceptable, high, limited, minimal, not_ai)\n",
    "- confidence (float 0-1)\n",
    "- ai_capabilities (list of strings — specific AI capabilities found in the description)\n",
    "- target_sectors (list of strings — application domains)\n",
    "- risk_rationale (string — 1-2 sentence explanation)\"\"\"\n",
    "\n",
    "\n",
    "def assess_trademark(description: str, owner: str = \"\", max_retries: int = 3) -> Optional[AIActAssessment]:\n",
    "    \"\"\"Assess a trademark using the teacher model with retry logic.\"\"\"\n",
    "    user_msg = f\"Assess this EUIPO trademark under the EU AI Act:\\n\\nOwner: {owner}\\nGoods/Services: {description[:800]}\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=TEACHER_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_msg}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.0,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            return AIActAssessment.model_validate_json(\n",
    "                response.choices[0].message.content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "# Quick test on a known AI trademark\n",
    "test_desc = \"facial recognition software; biometric identification systems; software for law enforcement agencies\"\n",
    "result = assess_trademark(test_desc, \"TEST CORP\")\n",
    "if result:\n",
    "    print(json.dumps(result.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Labeling Real Trademarks (~200 examples)\n",
    "\n",
    "We sample 200 trademarks with a stratified selection:\n",
    "- **~100 from the AI-adjacent subset** (keyword-filtered) — these are likely to span the AI risk tiers\n",
    "- **~100 random from the full dataset** — mostly \"not_ai\", but with some surprises\n",
    "\n",
    "This gives the teacher model a realistic mix. With ~0.3s per call on Groq, labeling takes about 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Sample 100 from AI-adjacent + 100 random\n",
    "ai_sample = ai_subset.sample(min(100, len(ai_subset)), random_state=42)\n",
    "non_ai_pool = tm_df[~tm_df.index.isin(ai_sample.index)]\n",
    "random_sample = non_ai_pool.sample(100, random_state=42)\n",
    "\n",
    "label_pool = pd.concat([ai_sample, random_sample]).reset_index(drop=True)\n",
    "print(f\"Labeling pool: {len(label_pool)} trademarks\")\n",
    "print(f\"  AI-adjacent: {len(ai_sample)}\")\n",
    "print(f\"  Random: {len(random_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_real = []\n",
    "errors = 0\n",
    "\n",
    "for idx, row in tqdm(label_pool.iterrows(), total=len(label_pool), desc=\"Labeling real trademarks\"):\n",
    "    result = assess_trademark(row['full_description'], row.get('owner_name', ''))\n",
    "    if result:\n",
    "        labeled_real.append({\n",
    "            'description': row['full_description'],\n",
    "            'owner_name': row.get('owner_name', ''),\n",
    "            'application_number': row.get('ApplicationNumber', ''),\n",
    "            'source': 'real',\n",
    "            **result.model_dump()\n",
    "        })\n",
    "    else:\n",
    "        errors += 1\n",
    "    time.sleep(0.3)  # Rate limiting\n",
    "\n",
    "real_df = pd.DataFrame(labeled_real)\n",
    "print(f\"\\nLabeled: {len(real_df)}/{len(label_pool)} ({errors} errors)\")\n",
    "print(f\"\\nRisk tier distribution:\")\n",
    "print(real_df['risk_tier'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Check\n",
    "\n",
    "Let's inspect the teacher's outputs before proceeding. We check:\n",
    "- Are the risk tiers plausible? (most should be \"not_ai\" or \"minimal\")\n",
    "- Are the extracted `ai_capabilities` and `target_sectors` reasonable?\n",
    "- What does the confidence distribution look like?\n",
    "\n",
    "> **A note on LLM self-reported confidence:** You may notice that the model reports high confidence (0.85+) on nearly every example. This is typical — LLMs with `temperature=0.0` are poorly calibrated and tend to be overconfident. The filter still helps catch the model's most uncertain outputs, but don't treat these numbers as true probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution\n",
    "print(f\"Confidence stats:\")\n",
    "print(real_df['confidence'].describe())\n",
    "\n",
    "# Inspect a few examples from each tier\n",
    "for tier in RISK_TIERS:\n",
    "    subset = real_df[real_df['risk_tier'] == tier]\n",
    "    if len(subset) == 0:\n",
    "        print(f\"\\n--- {tier.upper()} (0 examples) ---\")\n",
    "        continue\n",
    "    row = subset.iloc[0]\n",
    "    print(f\"\\n--- {tier.upper()} ({len(subset)} examples) ---\")\n",
    "    print(f\"  Description: {row['description'][:120]}...\")\n",
    "    print(f\"  AI capabilities: {row['ai_capabilities']}\")\n",
    "    print(f\"  Target sectors: {row['target_sectors']}\")\n",
    "    print(f\"  Rationale: {row['risk_rationale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating Synthetic Examples (~200)\n",
    "\n",
    "The real trademark data from 2015–2020 is heavily skewed toward \"not_ai\" and \"minimal\". The **unacceptable** and **high-risk** tiers are underrepresented — these AI products either didn't exist yet or weren't being trademarked.\n",
    "\n",
    "We use the teacher model to **generate realistic synthetic trademark descriptions** for underrepresented tiers, then label them with the full schema. This is a standard technique for handling class imbalance in NLP.\n",
    "\n",
    "Target distribution for synthetic data:\n",
    "- ~40 unacceptable (social scoring, mass surveillance)\n",
    "- ~60 high-risk (hiring tools, medical AI, credit scoring, law enforcement)\n",
    "- ~40 limited (chatbots, emotion detection)\n",
    "- ~40 minimal (game AI, recommendations)\n",
    "- ~20 not_ai (edge cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subcategories for synthetic generation\n",
    "SYNTHETIC_TARGETS = {\n",
    "    \"unacceptable\": [\n",
    "        (\"social credit scoring system\", 20),\n",
    "        (\"real-time biometric mass surveillance system for public spaces\", 20),\n",
    "    ],\n",
    "    \"high\": [\n",
    "        (\"AI-powered hiring and recruitment screening tool\", 15),\n",
    "        (\"automated creditworthiness assessment system\", 15),\n",
    "        (\"AI medical diagnostic device\", 15),\n",
    "        (\"AI system for law enforcement and predictive policing\", 15),\n",
    "    ],\n",
    "    \"limited\": [\n",
    "        (\"AI chatbot and conversational agent\", 20),\n",
    "        (\"emotion detection and recognition system\", 20),\n",
    "    ],\n",
    "    \"minimal\": [\n",
    "        (\"AI-powered video game and interactive entertainment\", 20),\n",
    "        (\"recommendation engine and personalization system\", 20),\n",
    "    ],\n",
    "    \"not_ai\": [\n",
    "        (\"standard consumer electronics and accessories\", 10),\n",
    "        (\"conventional software without AI components\", 10),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Get a few real descriptions as style examples\n",
    "style_examples = tm_df.sample(5, random_state=42)['full_description'].tolist()\n",
    "style_block = \"\\n\".join(f\"- {d[:200]}\" for d in style_examples)\n",
    "\n",
    "total_synthetic = sum(count for subcats in SYNTHETIC_TARGETS.values() for _, count in subcats)\n",
    "print(f\"Planned synthetic examples: {total_synthetic}\")\n",
    "for tier, subcats in SYNTHETIC_TARGETS.items():\n",
    "    tier_total = sum(c for _, c in subcats)\n",
    "    print(f\"  {tier}: {tier_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_PROMPT = \"\"\"Generate {n} realistic EUIPO trademark goods/services descriptions for: {category}\n",
    "\n",
    "Rules:\n",
    "- Use the exact style of EUIPO standardized terms\n",
    "- Semicolon-separated goods/services, formal register, no marketing language\n",
    "- Each description should be 50-150 words\n",
    "- Make each one distinct (different product focus)\n",
    "\n",
    "Real EUIPO descriptions for reference style:\n",
    "{style_examples}\n",
    "\n",
    "Return a JSON object with a \"descriptions\" field containing a list of {n} strings.\"\"\"\n",
    "\n",
    "\n",
    "def generate_synthetic_batch(category: str, n: int, max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"Generate a batch of synthetic trademark descriptions.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=TEACHER_MODEL,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": GEN_PROMPT.format(\n",
    "                        n=n, category=category, style_examples=style_block\n",
    "                    )\n",
    "                }],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.8,\n",
    "                max_tokens=2000\n",
    "            )\n",
    "            data = json.loads(response.choices[0].message.content)\n",
    "            return data.get(\"descriptions\", [])\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic descriptions in batches of 10\n",
    "synthetic_descriptions = []  # (description, intended_tier, subcategory)\n",
    "\n",
    "for tier, subcats in SYNTHETIC_TARGETS.items():\n",
    "    for category, count in subcats:\n",
    "        # Generate in batches of 10\n",
    "        remaining = count\n",
    "        while remaining > 0:\n",
    "            batch_size = min(10, remaining)\n",
    "            descs = generate_synthetic_batch(category, batch_size)\n",
    "            for d in descs:\n",
    "                synthetic_descriptions.append((d, tier, category))\n",
    "            remaining -= len(descs)\n",
    "            time.sleep(0.5)\n",
    "        print(f\"  {tier}/{category}: generated {count}\")\n",
    "\n",
    "print(f\"\\nTotal synthetic descriptions generated: {len(synthetic_descriptions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now label each synthetic description with the full schema\n",
    "labeled_synthetic = []\n",
    "synth_errors = 0\n",
    "\n",
    "for desc, intended_tier, subcat in tqdm(synthetic_descriptions, desc=\"Labeling synthetic\"):\n",
    "    result = assess_trademark(desc, \"SYNTHETIC\")\n",
    "    if result:\n",
    "        labeled_synthetic.append({\n",
    "            'description': desc,\n",
    "            'owner_name': 'SYNTHETIC',\n",
    "            'application_number': '',\n",
    "            'source': 'synthetic',\n",
    "            'intended_tier': intended_tier,\n",
    "            **result.model_dump()\n",
    "        })\n",
    "    else:\n",
    "        synth_errors += 1\n",
    "    time.sleep(0.3)\n",
    "\n",
    "synth_df = pd.DataFrame(labeled_synthetic)\n",
    "print(f\"\\nLabeled: {len(synth_df)}/{len(synthetic_descriptions)} ({synth_errors} errors)\")\n",
    "print(f\"\\nRisk tier distribution (teacher-assigned):\")\n",
    "print(synth_df['risk_tier'].value_counts())\n",
    "\n",
    "# Check alignment between intended and assigned tiers\n",
    "if 'intended_tier' in synth_df.columns:\n",
    "    agreement = (synth_df['risk_tier'] == synth_df['intended_tier']).mean()\n",
    "    print(f\"\\nTeacher agreed with intended tier: {agreement:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge + Quality Filter\n",
    "\n",
    "We combine real and synthetic labeled data, then apply quality controls:\n",
    "1. **Confidence filter** — drop examples where the teacher was uncertain\n",
    "2. **Deduplication** — remove near-duplicate descriptions\n",
    "3. **Validation** — drop examples with missing fields\n",
    "4. **Class balance check** — ensure minimum representation per tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge real + synthetic\n",
    "all_labeled = pd.concat([real_df, synth_df], ignore_index=True)\n",
    "print(f\"Total before filtering: {len(all_labeled)}\")\n",
    "print(f\"  Real: {len(real_df)}, Synthetic: {len(synth_df)}\")\n",
    "\n",
    "# 1. Confidence filter\n",
    "filtered = all_labeled[all_labeled['confidence'] >= 0.7].copy()\n",
    "print(f\"\\nAfter confidence >= 0.7: {len(filtered)}\")\n",
    "\n",
    "# 2. Deduplication by description hash\n",
    "filtered['desc_hash'] = filtered['description'].apply(\n",
    "    lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    ")\n",
    "filtered = filtered.drop_duplicates(subset='desc_hash')\n",
    "print(f\"After dedup: {len(filtered)}\")\n",
    "\n",
    "# 3. Validate: drop rows with empty rationale\n",
    "filtered = filtered[filtered['risk_rationale'].str.len() > 10]\n",
    "print(f\"After validation: {len(filtered)}\")\n",
    "\n",
    "# 4. Class balance check\n",
    "print(f\"\\nFinal class distribution:\")\n",
    "print(filtered['risk_tier'].value_counts())\n",
    "print(f\"\\nSource breakdown:\")\n",
    "print(filtered.groupby(['risk_tier', 'source']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Distill to sklearn (Classification Only)\n",
    "\n",
    "Now we train a fast classifier on just the `risk_tier` labels. This is the standard distillation step: encode descriptions with a sentence transformer, train logistic regression on the embeddings.\n",
    "\n",
    "We hold out ~50 examples for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_dist, test_dist = train_test_split(\n",
    "    filtered, test_size=50, random_state=42, stratify=filtered['risk_tier']\n",
    ")\n",
    "print(f\"Train: {len(train_dist)}, Test: {len(test_dist)}\")\n",
    "\n",
    "# Encode with sentence transformer\n",
    "EMBED_MODEL = \"intfloat/e5-small\"\n",
    "e5_model = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "train_texts = [f\"query: {t.strip()}\" for t in train_dist['description'].tolist()]\n",
    "test_texts = [f\"query: {t.strip()}\" for t in test_dist['description'].tolist()]\n",
    "\n",
    "train_emb = e5_model.encode(train_texts, show_progress_bar=True, normalize_embeddings=True)\n",
    "test_emb = e5_model.encode(test_texts, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "# Train logistic regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(train_emb, train_dist['risk_tier'])\n",
    "lr_preds = lr.predict(test_emb)\n",
    "\n",
    "print(f\"\\nsklearn (E5 + LR) accuracy: {accuracy_score(test_dist['risk_tier'], lr_preds):.1%}\")\n",
    "print(classification_report(test_dist['risk_tier'], lr_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. The Key Insight: Classification vs Extraction\n",
    "\n",
    "The sklearn model learned the `risk_tier` label — and it's fast (sub-millisecond inference, no API calls).\n",
    "\n",
    "But look at what it **cannot** do: given a new trademark description, it can predict \"high\" but it **cannot** tell you *which* AI capabilities were detected, *which* sectors are affected, or *why* it made that classification.\n",
    "\n",
    "That's the gap NB09 fills — fine-tuning a small language model (Qwen3-4B) to produce the **full structured output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate the gap\nexample = test_dist.iloc[0]\n\nprint(\"=\" * 60)\nprint(\"SAME INPUT:\")\nprint(f\"  {example['description'][:200]}...\")\nprint()\n\n# sklearn output\nexample_emb = e5_model.encode([f\"query: {example['description']}\"], normalize_embeddings=True)\nsklearn_pred = lr.predict(example_emb)[0]\nprint(f\"sklearn output: {sklearn_pred}\")\nprint(f\"  → Just a label. No explanation, no extracted capabilities.\")\nprint()\n\n# Teacher output (what we want the fine-tuned model to learn)\nprint(f\"Teacher output (what NB09's model will learn to produce):\")\nteacher_output = {\n    'is_ai_related': bool(example['is_ai_related']),\n    'risk_tier': str(example['risk_tier']),\n    'confidence': float(example['confidence']),\n    'ai_capabilities': list(example['ai_capabilities']),\n    'target_sectors': list(example['target_sectors']),\n    'risk_rationale': str(example['risk_rationale'])\n}\nprint(json.dumps(teacher_output, indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save for NB09 (Fine-tuning)\n",
    "\n",
    "We save the labeled data in **conversation format** — ready for supervised fine-tuning with Qwen3-4B in NB09.\n",
    "\n",
    "Each example becomes a user/assistant conversation pair where the assistant response is the full JSON assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def to_conversation(row):\n    \"\"\"Format a labeled example as a conversation pair for fine-tuning.\"\"\"\n    user_msg = f\"Assess this EUIPO trademark under the EU AI Act:\\n\\nOwner: {row.get('owner_name', '')}\\nGoods/Services: {row['description'][:800]}\"\n\n    assistant_msg = json.dumps({\n        'is_ai_related': bool(row['is_ai_related']),\n        'risk_tier': str(row['risk_tier']),\n        'confidence': float(row['confidence']),\n        'ai_capabilities': list(row['ai_capabilities']),\n        'target_sectors': list(row['target_sectors']),\n        'risk_rationale': str(row['risk_rationale'])\n    }, ensure_ascii=False)\n\n    return {\n        'conversations': [\n            {'role': 'user', 'content': user_msg},\n            {'role': 'assistant', 'content': assistant_msg}\n        ]\n    }\n\n\n# Convert all filtered data to conversation format\nconversations = [to_conversation(row) for _, row in filtered.iterrows()]\n\n# Save\noutput_file = \"trademark_ai_act_conversations.json\"\nwith open(output_file, 'w') as f:\n    json.dump(conversations, f, indent=2, ensure_ascii=False)\n\nprint(f\"Saved {len(conversations)} conversation pairs to {output_file}\")\nprint(f\"\\nExample conversation:\")\nprint(json.dumps(conversations[0], indent=2, ensure_ascii=False)[:500])\n\n# Also save the test set separately for NB09 evaluation\ntest_conversations = [to_conversation(row) for _, row in test_dist.iterrows()]\nwith open(\"trademark_ai_act_test.json\", 'w') as f:\n    json.dump(test_conversations, f, indent=2, ensure_ascii=False)\n\nprint(f\"Saved {len(test_conversations)} test examples to trademark_ai_act_test.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_real = len(real_df)\n",
    "n_synthetic_gen = len(synthetic_descriptions) // 10  # batch calls\n",
    "n_synthetic_label = len(synth_df)\n",
    "total_calls = n_real + n_synthetic_gen + n_synthetic_label\n",
    "\n",
    "print(\"Distillation Cost Analysis:\")\n",
    "print(f\"  Real trademarks labeled: {n_real}\")\n",
    "print(f\"  Synthetic generation calls: ~{n_synthetic_gen}\")\n",
    "print(f\"  Synthetic trademarks labeled: {n_synthetic_label}\")\n",
    "print(f\"  Total API calls: ~{total_calls}\")\n",
    "print(f\"  Cost on Groq free tier: $0.00\")\n",
    "print(f\"\\n  sklearn inference: <1ms per trademark (no API needed)\")\n",
    "print(f\"  Teacher inference: ~300ms per trademark (API required)\")\n",
    "print(f\"  Fine-tuned Qwen3-4B (NB09): ~50ms per trademark (local, free)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exercise\n",
    "\n",
    "Try the following experiments:\n",
    "\n",
    "1. **Confidence thresholds** — try `0.5`, `0.8`, `0.9` and see how the student accuracy changes\n",
    "2. **Real-only vs synthetic-augmented** — train the sklearn model on only the real examples. Does adding synthetic data help?\n",
    "3. **Different embedding models** — try `all-MiniLM-L6-v2` instead of `e5-small`. Which embeds trademark language better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Compare real-only vs augmented\n",
    "# -----------------------------------------\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Filter `filtered` to source == 'real' only\n",
    "# 2. Train sklearn on real-only data\n",
    "# 3. Compare accuracy to the augmented model above\n",
    "\n",
    "# real_only = filtered[filtered['source'] == 'real']\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Takeaways\n",
    "\n",
    "**What we built:**\n",
    "- A pipeline that uses a large teacher model (`kimi-k2`, 1T params) to produce **structured annotations** for EUIPO trademarks\n",
    "- Synthetic data generation to handle class imbalance in rare EU AI Act risk tiers\n",
    "- Quality filtering (confidence, dedup, validation)\n",
    "- A fast sklearn student for **classification only** (risk tier prediction)\n",
    "\n",
    "**The distillation tradeoff:**\n",
    "\n",
    "| Approach | Classification | Structured Extraction | Speed | Cost |\n",
    "|----------|:-------------:|:--------------------:|:-----:|:----:|\n",
    "| Teacher (kimi-k2) | Yes | Yes | Slow | API |\n",
    "| sklearn student | Yes | **No** | Fast | Free |\n",
    "| Fine-tuned Qwen3-4B (NB09) | Yes | **Yes** | Medium | Free |\n",
    "\n",
    "**Key lesson:** Classification-only distillation is useful but limited. To preserve the teacher's ability to produce structured output — lists of capabilities, sector inference, rationale — you need to fine-tune a language model. That's NB09.\n",
    "\n",
    "**Next:** In NB09, we load the conversation data saved above and fine-tune Qwen3-4B with LoRA to produce the full structured assessment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}