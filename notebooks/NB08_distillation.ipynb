{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB08: Distillation — LLM Label Synthesis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB08_distillation.ipynb)\n",
    "\n",
    "**Time:** ~65 minutes\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Generate training data using LLMs** — use a large language model as an automatic labeler for unlabeled text corpora\n",
    "2. **Implement confidence filtering and deduplication** — apply quality controls to noisy LLM-generated labels\n",
    "3. **Train a fast classifier on synthetic labels** — distill the LLM's knowledge into a lightweight, deployable model\n",
    "4. **Compare to human-labeled baselines** — evaluate whether distilled students can match or approach teacher performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pydantic pandas scikit-learn sentence-transformers tqdm datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. The Idea: LLMs as Label Generators\n\nLarge language models (LLMs) are remarkably good at understanding and classifying text — but they are **expensive and slow at inference time**. Every prediction requires an API call, network latency, and per-token costs. This makes them impractical for high-throughput production systems.\n\nThe solution is **knowledge distillation**:\n\n1. **Teacher:** Use a large LLM to **label** a training set (one-time cost)\n2. **Student:** Train a small, fast classifier (TF-IDF + LR, SBERT + LR, etc.) on those labels\n3. **Deploy:** Serve the student model — no API calls, sub-millisecond inference\n\nThis transfers the *knowledge* of the large model into a small one. The student won't be as flexible as the teacher, but for a **fixed classification task** it can be surprisingly competitive — at a fraction of the cost.\n\n![Distillation Concept](https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/notebooks/figures/distillation_concept.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n\n# If not set above, try Colab secrets → then environment variable\nif not GROQ_API_KEY:\n    try:\n        from google.colab import userdata\n        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n    except (ImportError, Exception):\n        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n\nclient = OpenAI(\n    api_key=GROQ_API_KEY,\n    base_url=\"https://api.groq.com/openai/v1\"\n)\nMODEL = \"llama-3.1-8b-instant\"\n\n# Test the connection\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[{\"role\": \"user\", \"content\": \"Say 'ready'\"}],\n    max_tokens=5\n)\nprint(resp.choices[0].message.content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Unlabeled Dataset\n",
    "\n",
    "We will use the **dair-ai/emotion** dataset — a collection of ~416K English tweets labeled with 6 emotions (sadness, joy, love, anger, fear, surprise). This is a rich, real-world dataset for social media text classification.\n",
    "\n",
    "To simulate a realistic distillation scenario, we **pretend we don't have labels** for a subset of the training data and use the LLM to label them. We keep a separate held-out set with real (human) labels so we can evaluate how well the distillation pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the emotion dataset from Hugging Face\n",
    "emotion_ds = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "# Label mapping\n",
    "EMOTION_LABELS = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_full = pd.DataFrame(emotion_ds[\"train\"])\n",
    "train_full[\"label_name\"] = train_full[\"label\"].map(lambda x: EMOTION_LABELS[x])\n",
    "\n",
    "# Subsample: 1000 for LLM labeling (pretend unlabeled), 200 as gold eval set\n",
    "np.random.seed(42)\n",
    "pool_idx = np.random.choice(len(train_full), size=1200, replace=False)\n",
    "pool_df = train_full.iloc[pool_idx].reset_index(drop=True)\n",
    "\n",
    "train_pool = pool_df.iloc[:1000].copy()\n",
    "test_df = pool_df.iloc[1000:].copy()\n",
    "\n",
    "print(f\"Unlabeled pool: {len(train_pool)} tweets\")\n",
    "print(f\"Test set (real labels): {len(test_df)} tweets\")\n",
    "print(f\"\\nEmotion distribution in pool:\")\n",
    "print(train_pool['label_name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Labeling with Structured Output\n",
    "\n",
    "We ask the LLM to classify each text into one of our categories and return **structured JSON** with:\n",
    "- `label` — the predicted category\n",
    "- `confidence` — a self-reported confidence score (0-1)\n",
    "- `reasoning` — a brief explanation\n",
    "\n",
    "We use **Pydantic** to validate every response, ensuring type safety and catching malformed outputs. The retry logic handles transient API errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "class LabelPrediction(BaseModel):\n",
    "    label: Literal[\n",
    "        \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"\n",
    "    ] = Field(description=\"Best-fit emotion category\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence 0-1\")\n",
    "    reasoning: str = Field(description=\"Brief reasoning for the classification\")\n",
    "\n",
    "\n",
    "def label_with_retry(text: str, max_retries: int = 3) -> Optional[LabelPrediction]:\n",
    "    \"\"\"Label a text using LLM with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": f\"Classify the emotion expressed in the following text into one of these categories: {CATEGORIES}. Return JSON with 'label', 'confidence' (0-1), and 'reasoning'.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": text[:500]  # Truncate for speed\n",
    "                    }\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.0,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            return LabelPrediction.model_validate_json(\n",
    "                response.choices[0].message.content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Labeling\n",
    "\n",
    "Now we send every tweet in our unlabeled pool through the LLM. This is the **most expensive step** — but it only happens once. We keep the true labels alongside for evaluation purposes (in a real scenario, you would not have these).\n",
    "\n",
    "Note: With 1000 tweets, this takes about 2-3 minutes using Groq's free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = []\n",
    "errors = 0\n",
    "\n",
    "for idx, row in tqdm(train_pool.iterrows(), total=len(train_pool), desc=\"LLM Labeling\"):\n",
    "    result = label_with_retry(row['text'])\n",
    "    if result:\n",
    "        labeled_data.append({\n",
    "            'text': row['text'],\n",
    "            'llm_label': result.label,\n",
    "            'confidence': result.confidence,\n",
    "            'reasoning': result.reasoning,\n",
    "            'true_label': row['label_name']  # We keep this for evaluation only!\n",
    "        })\n",
    "    else:\n",
    "        errors += 1\n",
    "    time.sleep(0.1)  # Rate limiting\n",
    "\n",
    "labeled_df = pd.DataFrame(labeled_data)\n",
    "print(f\"\\nLabeled: {len(labeled_df)}/{len(train_pool)} ({errors} errors)\")\n",
    "print(f\"LLM accuracy vs true labels: {accuracy_score(labeled_df['true_label'], labeled_df['llm_label']):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Filtering\n",
    "\n",
    "Not all LLM labels are created equal. The model itself reports a confidence score — we can use this to **filter out uncertain predictions** and keep only high-quality labels for training.\n",
    "\n",
    "We also **deduplicate** by text hash to avoid training on repeated examples, which could bias the student model.\n",
    "\n",
    "Key insight: a smaller set of *high-quality* labels often outperforms a larger set of *noisy* labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Confidence filtering\n",
    "high_conf = labeled_df[labeled_df['confidence'] >= 0.7].copy()\n",
    "print(f\"After confidence filter (>=0.7): {len(high_conf)}/{len(labeled_df)} ({len(high_conf)/len(labeled_df):.0%})\")\n",
    "\n",
    "# Check if filtering improves accuracy\n",
    "if len(high_conf) > 0:\n",
    "    print(f\"High-confidence accuracy: {accuracy_score(high_conf['true_label'], high_conf['llm_label']):.1%}\")\n",
    "    print(f\"All labels accuracy:     {accuracy_score(labeled_df['true_label'], labeled_df['llm_label']):.1%}\")\n",
    "\n",
    "# 2. Deduplication (by text hash)\n",
    "high_conf['text_hash'] = high_conf['text'].apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "deduped = high_conf.drop_duplicates(subset='text_hash')\n",
    "print(f\"\\nAfter dedup: {len(deduped)} unique tweets\")\n",
    "\n",
    "# 3. Class balance check\n",
    "print(f\"\\nEmotion distribution in filtered set:\")\n",
    "print(deduped['llm_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a Student Classifier\n",
    "\n",
    "Now we train **fast, lightweight classifiers** on the LLM-generated labels. These \"student\" models learn to mimic the teacher's decisions — but at a fraction of the inference cost.\n",
    "\n",
    "We train two students:\n",
    "1. **TF-IDF + Logistic Regression** — the classic baseline, extremely fast\n",
    "2. **SBERT + Logistic Regression** — uses pre-trained sentence embeddings for richer representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student 1: TF-IDF + Logistic Regression\n",
    "tfidf_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=10000)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "tfidf_pipe.fit(deduped['text'], deduped['llm_label'])\n",
    "tfidf_preds = tfidf_pipe.predict(test_df['text'])\n",
    "tfidf_acc = accuracy_score(test_df['label_name'], tfidf_preds)\n",
    "\n",
    "# Student 2: SBERT + Logistic Regression\n",
    "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train_emb = sbert.encode(deduped['text'].tolist(), show_progress_bar=True)\n",
    "test_emb = sbert.encode(test_df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(train_emb, deduped['llm_label'])\n",
    "sbert_preds = lr.predict(test_emb)\n",
    "sbert_acc = accuracy_score(test_df['label_name'], sbert_preds)\n",
    "\n",
    "print(f\"\\n{'Model':<30} {'Accuracy':>10}\")\n",
    "print(\"-\" * 42)\n",
    "print(f\"{'LLM (teacher, zero-shot)':<30} {accuracy_score(labeled_df['true_label'], labeled_df['llm_label']):>10.1%}\")\n",
    "print(f\"{'TF-IDF + LR (student)':<30} {tfidf_acc:>10.1%}\")\n",
    "print(f\"{'SBERT + LR (student)':<30} {sbert_acc:>10.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Student Model (SBERT + LR) — Classification Report:\")\n",
    "print(classification_report(test_df['label_name'], sbert_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. The Distillation Pipeline\n\nHere is the full pipeline we just built, summarized as a diagram:\n\n![Distillation Pipeline](https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/notebooks/figures/distillation_pipeline.png)\n\n**Key properties of this pipeline:**\n- The LLM is used **once** during training — not at inference time\n- The student model is **self-contained** — no network calls, no API keys needed\n- Confidence filtering acts as a **quality gate** — noisy labels are discarded\n- The student can be retrained as new unlabeled data arrives\n- This pattern scales to **millions of texts** at minimal cost"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost Analysis\n",
    "\n",
    "One of the main advantages of distillation is the **dramatic cost reduction** at inference time. Let's quantify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled = len(train_pool)\n",
    "tokens_per_request = 200  # tweets are short\n",
    "total_tokens = n_labeled * tokens_per_request\n",
    "\n",
    "print(\"Distillation Cost Analysis:\")\n",
    "print(f\"  Tweets labeled: {n_labeled}\")\n",
    "print(f\"  Tokens used: ~{total_tokens:,}\")\n",
    "print(f\"  Groq free tier: $0.00\")\n",
    "print(f\"  At scale (10K tweets): ~{10000 * tokens_per_request:,} tokens = still free tier\")\n",
    "print(f\"  At scale (100K tweets): ~{100000 * tokens_per_request:,} tokens\")\n",
    "print(f\"\\n  Student model inference: <1ms per tweet (no API needed!)\")\n",
    "print(f\"  LLM inference: ~200ms per tweet (API required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercise\n",
    "\n",
    "Try the following experiments to deepen your understanding:\n",
    "\n",
    "1. **Different confidence thresholds** — try `0.5`, `0.8`, `0.9` and see how the student accuracy changes. Is there a sweet spot between label quality and training set size?\n",
    "\n",
    "2. **Class balancing** — undersample the majority emotion in `deduped` so all emotions have equal representation. Does this help or hurt student performance?\n",
    "\n",
    "3. **Larger pool** — increase the subsample from 1000 to 5000 tweets. Does the student improve with more (noisy) training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Experiment with confidence thresholds\n",
    "# ------------------------------------------------\n",
    "# Try different thresholds and compare student accuracy\n",
    "\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    subset = labeled_df[labeled_df['confidence'] >= thresh]\n",
    "    if len(subset) < 10:\n",
    "        print(f\"Threshold {thresh}: too few samples ({len(subset)})\")\n",
    "        continue\n",
    "\n",
    "    # TODO: Train a TF-IDF student on `subset` and evaluate on test_df\n",
    "    # pipe = Pipeline([...])\n",
    "    # pipe.fit(subset['text'], subset['llm_label'])\n",
    "    # preds = pipe.predict(test_df['text'])\n",
    "    # acc = accuracy_score(test_df['label_name'], preds)\n",
    "    # print(f\"Threshold {thresh}: {len(subset)} samples, accuracy = {acc:.1%}\")\n",
    "    \n",
    "    print(f\"Threshold {thresh}: {len(subset)} samples available — implement training above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Save Distilled Labels for NB09\n\nIf you plan to continue with **NB09 (Fine-tuning)**, save the filtered labels so you can load them directly instead of re-running the LLM labeling pipeline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save the high-quality distilled labels for use in NB09\noutput_file = \"emotion_distilled_labels.csv\"\ndeduped[['text', 'llm_label', 'confidence']].to_csv(output_file, index=False)\nprint(f\"Saved {len(deduped)} distilled labels to {output_file}\")\nprint(f\"Columns: {list(deduped[['text', 'llm_label', 'confidence']].columns)}\")\nprint(f\"\\nTo use in NB09, upload this file to the Colab runtime or keep it in the same directory.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Takeaways\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "- **Distillation** is the process of transferring knowledge from a large, expensive model (teacher) to a small, fast model (student) via synthetic labels\n",
    "- **Confidence filtering matters** — LLM self-reported confidence can be used to discard noisy labels and improve student quality\n",
    "- **Students can match or exceed the teacher** on structured, fixed classification tasks — especially when combined with good feature representations (SBERT)\n",
    "- **The production pattern is: label once, serve forever** — the one-time cost of LLM labeling is amortized over millions of fast student inferences\n",
    "\n",
    "**When to use distillation:**\n",
    "- You have a **fixed classification task** with a known label set\n",
    "- You have **lots of unlabeled data** but limited annotation budget\n",
    "- You need **fast inference** (real-time, batch processing, edge deployment)\n",
    "- The teacher LLM performs well enough on your task in zero-shot mode\n",
    "\n",
    "**When NOT to use distillation:**\n",
    "- The task requires **open-ended generation** (not classification)\n",
    "- The label space **changes frequently**\n",
    "- The teacher LLM performs **poorly** on your specific domain\n",
    "\n",
    "**Next:** In NB09, we will take the distilled emotion labels and use them to **fine-tune** a small language model (Qwen3-4B), creating an even more capable student."
   ]
  }
 ]
}