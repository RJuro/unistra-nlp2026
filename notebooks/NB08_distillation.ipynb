{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB08: Distillation — LLM Label Synthesis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB08_distillation.ipynb)\n",
    "\n",
    "**Time:** ~65 minutes\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Generate training data using LLMs** — use a large language model as an automatic labeler for unlabeled text corpora\n",
    "2. **Implement confidence filtering and deduplication** — apply quality controls to noisy LLM-generated labels\n",
    "3. **Train a fast classifier on synthetic labels** — distill the LLM's knowledge into a lightweight, deployable model\n",
    "4. **Compare to human-labeled baselines** — evaluate whether distilled students can match or approach teacher performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pydantic pandas scikit-learn sentence-transformers tqdm datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. The Idea: LLMs as Label Generators\n\nLarge language models (LLMs) are remarkably good at understanding and classifying text — but they are **expensive and slow at inference time**. Every prediction requires an API call, network latency, and per-token costs. This makes them impractical for high-throughput production systems.\n\nThe solution is **knowledge distillation**:\n\n1. **Teacher:** Use a large LLM to **label** a training set (one-time cost)\n2. **Student:** Train a small, fast classifier (TF-IDF + LR, SBERT + LR, etc.) on those labels\n3. **Deploy:** Serve the student model — no API calls, sub-millisecond inference\n\nThis transfers the *knowledge* of the large model into a small one. The student won't be as flexible as the teacher, but for a **fixed classification task** it can be surprisingly competitive — at a fraction of the cost.\n\n![Distillation Concept](https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/notebooks/figures/distillation_concept.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n\n# If not set above, try Colab secrets → then environment variable\nif not GROQ_API_KEY:\n    try:\n        from google.colab import userdata\n        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n    except (ImportError, Exception):\n        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n\n# === Choose your provider ===\n# Option A: Groq API (recommended — good quality, free tier)\nclient = OpenAI(\n    api_key=GROQ_API_KEY,\n    base_url=\"https://api.groq.com/openai/v1\"\n)\nMODEL_FAST = \"moonshotai/kimi-k2-instruct\"  # Good quality for classification\n\n# Option B: Local Ollama (unlimited rate, no API key needed)\n# If you hit Groq rate limits with 1000 tweets, switch to local inference.\n# See NB03b for the full Ollama setup on Colab (install + start_ollama()).\n# Quick setup:\n#   !sudo apt-get install -y zstd pciutils -qq\n#   !curl -fsSL https://ollama.com/install.sh | sh\n#   import subprocess, requests, time\n#   subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n#   time.sleep(5)  # wait for server\n#   !ollama pull ministral:latest\n#   client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n#   MODEL_FAST = \"ministral:latest\"\n\n# Test the connection\nresp = client.chat.completions.create(\n    model=MODEL_FAST,\n    messages=[{\"role\": \"user\", \"content\": \"Say 'ready'\"}],\n    max_tokens=5\n)\nprint(resp.choices[0].message.content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Unlabeled Dataset\n",
    "\n",
    "We will use the **dair-ai/emotion** dataset — a collection of ~416K English tweets labeled with 6 emotions (sadness, joy, love, anger, fear, surprise). This is a rich, real-world dataset for social media text classification.\n",
    "\n",
    "To simulate a realistic distillation scenario, we **pretend we don't have labels** for a subset of the training data and use the LLM to label them. We keep a separate held-out set with real (human) labels so we can evaluate how well the distillation pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the emotion dataset from Hugging Face\n",
    "emotion_ds = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "# Label mapping\n",
    "EMOTION_LABELS = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_full = pd.DataFrame(emotion_ds[\"train\"])\n",
    "train_full[\"label_name\"] = train_full[\"label\"].map(lambda x: EMOTION_LABELS[x])\n",
    "\n",
    "# Subsample: 1000 for LLM labeling (pretend unlabeled), 200 as gold eval set\n",
    "np.random.seed(42)\n",
    "pool_idx = np.random.choice(len(train_full), size=1200, replace=False)\n",
    "pool_df = train_full.iloc[pool_idx].reset_index(drop=True)\n",
    "\n",
    "train_pool = pool_df.iloc[:1000].copy()\n",
    "test_df = pool_df.iloc[1000:].copy()\n",
    "\n",
    "print(f\"Unlabeled pool: {len(train_pool)} tweets\")\n",
    "print(f\"Test set (real labels): {len(test_df)} tweets\")\n",
    "print(f\"\\nEmotion distribution in pool:\")\n",
    "print(train_pool['label_name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Labeling with Structured Output\n",
    "\n",
    "We ask the LLM to classify each text into one of our categories and return **structured JSON** with:\n",
    "- `label` — the predicted category\n",
    "- `confidence` — a self-reported confidence score (0-1)\n",
    "- `reasoning` — a brief explanation\n",
    "\n",
    "We use **Pydantic** to validate every response, ensuring type safety and catching malformed outputs. The retry logic handles transient API errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CATEGORIES = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n\nclass LabelPrediction(BaseModel):\n    label: Literal[\n        \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"\n    ] = Field(description=\"Best-fit emotion category\")\n    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence 0-1\")\n    reasoning: str = Field(description=\"Brief reasoning for the classification\")\n\n\ndef label_with_retry(text: str, max_retries: int = 3) -> Optional[LabelPrediction]:\n    \"\"\"Label a text using LLM with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = client.chat.completions.create(\n                model=MODEL_FAST,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": f\"Classify the emotion expressed in the following text into one of these categories: {CATEGORIES}. Return JSON with 'label', 'confidence' (0-1), and 'reasoning'.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": text[:500]  # Truncate for speed\n                    }\n                ],\n                response_format={\"type\": \"json_object\"},\n                temperature=0.0,\n                max_tokens=150\n            )\n            return LabelPrediction.model_validate_json(\n                response.choices[0].message.content\n            )\n        except Exception as e:\n            if attempt < max_retries - 1:\n                time.sleep(2 ** attempt)\n            else:\n                return None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Labeling\n",
    "\n",
    "Now we send every tweet in our unlabeled pool through the LLM. This is the **most expensive step** — but it only happens once. We keep the true labels alongside for evaluation purposes (in a real scenario, you would not have these).\n",
    "\n",
    "Note: With 1000 tweets, this takes about 2-3 minutes using Groq's free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "labeled_data = []\nerrors = 0\n\nfor idx, row in tqdm(train_pool.iterrows(), total=len(train_pool), desc=\"LLM Labeling\"):\n    result = label_with_retry(row['text'])\n    if result:\n        labeled_data.append({\n            'text': row['text'],\n            'llm_label': result.label,\n            'confidence': result.confidence,\n            'reasoning': result.reasoning,\n            'true_label': row['label_name']  # We keep this for evaluation only!\n        })\n    else:\n        errors += 1\n    time.sleep(0.2)  # Rate limiting (increase to 0.5+ if you hit limits)\n\nlabeled_df = pd.DataFrame(labeled_data)\nprint(f\"\\nLabeled: {len(labeled_df)}/{len(train_pool)} ({errors} errors)\")\nprint(f\"LLM accuracy vs true labels: {accuracy_score(labeled_df['true_label'], labeled_df['llm_label']):.1%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Quality Filtering\n\nNot all LLM labels are created equal. The model itself reports a confidence score — we can use this to **filter out uncertain predictions** and keep only high-quality labels for training.\n\nWe also **deduplicate** by text hash to avoid training on repeated examples, which could bias the student model.\n\nKey insight: a smaller set of *high-quality* labels often outperforms a larger set of *noisy* labels.\n\n> **A note on LLM self-reported confidence:** You may notice that the LLM reports high confidence (0.80+) on nearly every example — even when it turns out to be wrong. This is a known limitation: LLMs with `temperature=0.0` produce poorly calibrated confidence scores and tend to be overconfident. If the filter keeps almost everything, that's expected behavior, not a bug.\n>\n> In practice, a more reliable approach is **consistency-based confidence**: label each text 3 times with `temperature=0.7` and use the agreement rate as your confidence score. If 3/3 runs agree, confidence is high; if they split 2/1, it's lower. This captures genuine model uncertainty better than a single self-reported number."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Confidence filtering\nhigh_conf = labeled_df[labeled_df['confidence'] >= 0.7].copy()\nprint(f\"After confidence filter (>=0.7): {len(high_conf)}/{len(labeled_df)} ({len(high_conf)/len(labeled_df):.0%})\")\n\n# Check if filtering improves accuracy\nif len(high_conf) > 0:\n    print(f\"High-confidence accuracy: {accuracy_score(high_conf['true_label'], high_conf['llm_label']):.1%}\")\n    print(f\"All labels accuracy:     {accuracy_score(labeled_df['true_label'], labeled_df['llm_label']):.1%}\")\n\n# 2. Deduplication (by text hash)\nhigh_conf['text_hash'] = high_conf['text'].apply(lambda x: hashlib.md5(x.encode()).hexdigest())\ndeduped = high_conf.drop_duplicates(subset='text_hash')\nprint(f\"\\nAfter dedup: {len(deduped)} unique tweets\")\n\n# 3. Class balance check — compare LLM labels to true labels\nprint(f\"\\nEmotion distribution (LLM labels vs true labels):\")\nllm_dist = deduped['llm_label'].value_counts()\ntrue_dist = deduped['true_label'].value_counts()\ncompare = pd.DataFrame({'llm_labels': llm_dist, 'true_labels': true_dist}).fillna(0).astype(int)\ncompare['difference'] = compare['llm_labels'] - compare['true_labels']\nprint(compare.to_string())\nprint(f\"\\nNote: If the LLM over-predicts one class (e.g., 'sadness'), the student\")\nprint(f\"will inherit that bias. Class balancing (see Exercise) can help.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Training a Student Classifier\n\nNow we train **fast, lightweight classifiers** on the LLM-generated labels. These \"student\" models learn to mimic the teacher's decisions — but at a fraction of the inference cost.\n\nWe train two students:\n1. **TF-IDF + Logistic Regression** — the classic baseline, extremely fast\n2. **E5 Embeddings + Logistic Regression** — uses the `intfloat/e5-small` model (consistent with NB04–07) for richer representations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Student 1: TF-IDF + Logistic Regression\ntfidf_pipe = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=10000)),\n    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n])\ntfidf_pipe.fit(deduped['text'], deduped['llm_label'])\ntfidf_preds = tfidf_pipe.predict(test_df['text'])\ntfidf_acc = accuracy_score(test_df['label_name'], tfidf_preds)\n\n# Student 2: E5 Embeddings + Logistic Regression\nEMBED_MODEL = \"intfloat/e5-small\"  # Consistent with NB04-07\ne5_model = SentenceTransformer(EMBED_MODEL)\n\n# E5 models require \"query: \" prefix for all inputs\ntrain_texts = [f\"query: {t.strip()}\" for t in deduped['text'].tolist()]\ntest_texts = [f\"query: {t.strip()}\" for t in test_df['text'].tolist()]\n\ntrain_emb = e5_model.encode(train_texts, show_progress_bar=True, normalize_embeddings=True)\ntest_emb = e5_model.encode(test_texts, show_progress_bar=True, normalize_embeddings=True)\n\nlr = LogisticRegression(max_iter=1000, random_state=42)\nlr.fit(train_emb, deduped['llm_label'])\ne5_preds = lr.predict(test_emb)\ne5_acc = accuracy_score(test_df['label_name'], e5_preds)\n\nprint(f\"\\n{'Model':<30} {'Accuracy':>10}\")\nprint(\"-\" * 42)\nprint(f\"{'LLM (teacher, zero-shot)':<30} {accuracy_score(labeled_df['true_label'], labeled_df['llm_label']):>10.1%}\")\nprint(f\"{'TF-IDF + LR (student)':<30} {tfidf_acc:>10.1%}\")\nprint(f\"{'E5 + LR (student)':<30} {e5_acc:>10.1%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Student Model (E5 + LR) — Classification Report:\")\nprint(classification_report(test_df['label_name'], e5_preds, zero_division=0))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. The Distillation Pipeline\n\nHere is the full pipeline we just built, summarized as a diagram:\n\n![Distillation Pipeline](https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/notebooks/figures/distillation_pipeline.png)\n\n**Key properties of this pipeline:**\n- The LLM is used **once** during training — not at inference time\n- The student model is **self-contained** — no network calls, no API keys needed\n- Confidence filtering acts as a **quality gate** — noisy labels are discarded\n- The student can be retrained as new unlabeled data arrives\n- This pattern scales to **millions of texts** at minimal cost"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost Analysis\n",
    "\n",
    "One of the main advantages of distillation is the **dramatic cost reduction** at inference time. Let's quantify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled = len(train_pool)\n",
    "tokens_per_request = 200  # tweets are short\n",
    "total_tokens = n_labeled * tokens_per_request\n",
    "\n",
    "print(\"Distillation Cost Analysis:\")\n",
    "print(f\"  Tweets labeled: {n_labeled}\")\n",
    "print(f\"  Tokens used: ~{total_tokens:,}\")\n",
    "print(f\"  Groq free tier: $0.00\")\n",
    "print(f\"  At scale (10K tweets): ~{10000 * tokens_per_request:,} tokens = still free tier\")\n",
    "print(f\"  At scale (100K tweets): ~{100000 * tokens_per_request:,} tokens\")\n",
    "print(f\"\\n  Student model inference: <1ms per tweet (no API needed!)\")\n",
    "print(f\"  LLM inference: ~200ms per tweet (API required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercise\n",
    "\n",
    "Try the following experiments to deepen your understanding:\n",
    "\n",
    "1. **Different confidence thresholds** — try `0.5`, `0.8`, `0.9` and see how the student accuracy changes. Is there a sweet spot between label quality and training set size?\n",
    "\n",
    "2. **Class balancing** — undersample the majority emotion in `deduped` so all emotions have equal representation. Does this help or hurt student performance?\n",
    "\n",
    "3. **Larger pool** — increase the subsample from 1000 to 5000 tweets. Does the student improve with more (noisy) training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise: Experiment with confidence thresholds\n# ------------------------------------------------\n# Try different thresholds and compare student accuracy\n\nthresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n\nresults = []\nfor thresh in thresholds:\n    subset = labeled_df[labeled_df['confidence'] >= thresh]\n    if len(subset) < 10:\n        print(f\"Threshold {thresh}: too few samples ({len(subset)}), skipping\")\n        continue\n\n    # Train a TF-IDF student on the filtered subset\n    pipe = Pipeline([\n        ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=10000)),\n        ('clf', LogisticRegression(max_iter=1000, random_state=42))\n    ])\n    pipe.fit(subset['text'], subset['llm_label'])\n    preds = pipe.predict(test_df['text'])\n    acc = accuracy_score(test_df['label_name'], preds)\n    results.append({'threshold': thresh, 'n_samples': len(subset), 'accuracy': acc})\n    print(f\"Threshold {thresh}: {len(subset)} samples, accuracy = {acc:.1%}\")\n\nif results:\n    results_df = pd.DataFrame(results)\n    print(f\"\\n{'Threshold':<12} {'Samples':>8} {'Accuracy':>10}\")\n    print(\"-\" * 32)\n    for _, r in results_df.iterrows():\n        print(f\"  {r['threshold']:<10} {int(r['n_samples']):>8} {r['accuracy']:>10.1%}\")\n    print(f\"\\nBest threshold: {results_df.loc[results_df['accuracy'].idxmax(), 'threshold']}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Save Distilled Labels for NB09\n\nIf you plan to continue with **NB09 (Fine-tuning)**, save the filtered labels so you can load them directly instead of re-running the LLM labeling pipeline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save the high-quality distilled labels for use in NB09\noutput_file = \"emotion_distilled_labels.csv\"\ndeduped[['text', 'llm_label', 'confidence']].to_csv(output_file, index=False)\nprint(f\"Saved {len(deduped)} distilled labels to {output_file}\")\nprint(f\"Columns: {list(deduped[['text', 'llm_label', 'confidence']].columns)}\")\nprint(f\"\\nTo use in NB09, upload this file to the Colab runtime or keep it in the same directory.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Summary & Takeaways\n\n**What we learned:**\n\n- **Distillation** is the process of transferring knowledge from a large, expensive model (teacher) to a small, fast model (student) via synthetic labels\n- **Confidence filtering** is conceptually important — but LLM self-reported confidence is often poorly calibrated (see Section 5 note). In practice, consistency-based approaches (multiple runs) or active learning may be more effective quality gates\n- **Class imbalance cascades** — if the teacher LLM over-predicts certain classes, the student inherits that bias. Always check the label distribution before training\n- **Students can approach teacher performance** on structured, fixed classification tasks — especially when combined with good feature representations (E5 embeddings)\n- **The production pattern is: label once, serve forever** — the one-time cost of LLM labeling is amortized over millions of fast student inferences\n\n**When to use distillation:**\n- You have a **fixed classification task** with a known label set\n- You have **lots of unlabeled data** but limited annotation budget\n- You need **fast inference** (real-time, batch processing, edge deployment)\n- The teacher LLM performs well enough on your task in zero-shot mode\n\n**When NOT to use distillation:**\n- The task requires **open-ended generation** (not classification)\n- The label space **changes frequently**\n- The teacher LLM performs **poorly** on your specific domain\n\n**Next:** In NB09, we will take the distilled emotion labels and use them to **fine-tune** a small language model (Qwen3-4B), creating an even more capable student."
  }
 ]
}