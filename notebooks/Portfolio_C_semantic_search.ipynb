{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio C: Semantic Search\n",
    "**Build a research paper search engine**\n",
    "\n",
    "Build a semantic search system that lets researchers find relevant scientific papers using natural language queries — including across languages. Start with a bi-encoder for fast retrieval, then add a cross-encoder for precision.\n",
    "\n",
    "**Dataset**: SciFact (scientific abstracts + claims)\n",
    "**Your goal**: Build a search pipeline, evaluate retrieval quality, and experiment with multilingual search.\n",
    "\n",
    "### Deliverables\n",
    "- Working search pipeline (bi-encoder retrieval + optional reranking)\n",
    "- Evaluation: precision@5 on at least 5 queries\n",
    "- At least one experiment (multilingual, different models, reranking comparison)\n",
    "- Brief model card\n",
    "\n",
    "**Estimated time**: Sprint 1 (55 min) + Sprint 2 (90 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets sentence-transformers faiss-cpu matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mteb/scifact\", split=\"corpus\")\n",
    "corpus_df = dataset.to_pandas()\n",
    "corpus_df.columns = ['doc_id', 'title', 'text']\n",
    "corpus_df['full_text'] = corpus_df['title'] + \". \" + corpus_df['text']\n",
    "print(f\"Corpus: {len(corpus_df)} documents\")\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encode & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "start = time.time()\n",
    "corpus_embeddings = model.encode(\n",
    "    corpus_df['full_text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "print(f\"Encoded {len(corpus_embeddings)} docs in {time.time()-start:.1f}s\")\n",
    "\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(corpus_embeddings.astype('float32'))\n",
    "print(f\"FAISS index: {index.ntotal} vectors, {dimension}d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    q_emb = model.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    scores, indices = index.search(q_emb, top_k)\n",
    "    results = []\n",
    "    for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
    "        results.append({\n",
    "            'rank': rank,\n",
    "            'score': float(score),\n",
    "            'title': corpus_df.iloc[idx]['title'],\n",
    "            'text': corpus_df.iloc[idx]['full_text'][:200] + '...',\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test it\n",
    "results = search(\"effects of vaccination on immune response\")\n",
    "print(results[['rank', 'score', 'title']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate: Precision@5\n",
    "Define queries with expected relevant terms, then measure how many of the top-5 results are actually relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries = [\n",
    "    {\"query\": \"vaccine effectiveness against viral infections\",\n",
    "     \"relevant_terms\": [\"vaccine\", \"immunization\", \"viral\", \"antibod\"]},\n",
    "    {\"query\": \"genetic mutations and cancer risk\",\n",
    "     \"relevant_terms\": [\"mutat\", \"cancer\", \"oncogen\", \"tumor\", \"genetic\"]},\n",
    "    {\"query\": \"effects of exercise on mental health\",\n",
    "     \"relevant_terms\": [\"exercis\", \"physical\", \"mental\", \"depress\", \"anxiety\"]},\n",
    "    {\"query\": \"antibiotic resistance mechanisms in bacteria\",\n",
    "     \"relevant_terms\": [\"antibiotic\", \"resistan\", \"bacteri\", \"antimicrob\"]},\n",
    "    {\"query\": \"neural networks in medical imaging\",\n",
    "     \"relevant_terms\": [\"neural\", \"deep learning\", \"imaging\", \"diagnos\", \"radiol\"]},\n",
    "]\n",
    "\n",
    "def is_relevant(doc_text, terms):\n",
    "    doc_lower = doc_text.lower()\n",
    "    return any(t in doc_lower for t in terms)\n",
    "\n",
    "for eq in eval_queries:\n",
    "    results = search(eq['query'], top_k=5)\n",
    "    relevant = sum(is_relevant(row['text'], eq['relevant_terms']) for _, row in results.iterrows())\n",
    "    print(f\"P@5={relevant/5:.0%} | {eq['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your Turn: Improve the Pipeline\n",
    "\n",
    "Choose one or more experiments:\n",
    "- **Cross-encoder reranking** (from NB07): Retrieve top-20 with bi-encoder, rerank with `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "- **Multilingual search**: Switch to `BAAI/bge-m3` and try queries in French/German\n",
    "- **Different embedding model**: Try `all-mpnet-base-v2` or another model\n",
    "- **Better evaluation**: Write more queries, use tighter relevance criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — Experiment 2 (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — compare baseline vs improved pipeline\n",
    "# e.g., average P@5 before and after reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Card\n",
    "\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| **Task** | Scientific paper retrieval |\n",
    "| **Corpus** | SciFact (_N_ documents) |\n",
    "| **Embedding model** | _model name_ |\n",
    "| **Avg P@5 (baseline)** | _score_ |\n",
    "| **Avg P@5 (improved)** | _score_ |\n",
    "| **Best query** | _which query works best?_ |\n",
    "| **Worst query** | _which query fails?_ |\n",
    "| **Improvement idea** | _what you'd try next_ |"
   ]
  }
 ]
}