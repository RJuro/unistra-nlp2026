{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB06: FAISS Retrieval + Semantic Search\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB06_faiss_retrieval.ipynb)\n",
    "\n",
    "**Duration:** ~70 minutes\n",
    "\n",
    "> **GPU optional** — CPU is fine for the 300-doc demo. A GPU simply speeds up embedding in Sections 2 and 6.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Build a semantic search system** from scratch using dense embeddings\n",
    "2. **Use FAISS for fast similarity search** over document collections\n",
    "3. **Understand the bi-encoder retrieval paradigm** — encode once, search many times\n",
    "4. **Use compact E5 models** (`intfloat/e5-small`, `intfloat/multilingual-e5-small`) with the correct `query:` / `passage:` format\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** NB02 (sentence embeddings). Familiarity with cosine similarity and vector representations of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu sentence-transformers datasets pandas numpy -q\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "print(f\"FAISS version: {faiss.__version__}\")\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── GPU Check ─────────────────────────────────────────────────────────────\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected — running on CPU.\")\n",
    "    print(\"This notebook still runs fine on CPU for 300 documents.\")\n",
    "    print(\"To enable GPU: Runtime -> Change runtime type -> T4 GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Use Case: Searching Policy Documents\n",
    "\n",
    "Social scientists, policy analysts, and researchers frequently need to find relevant documents in large collections — policy briefs, legal texts, academic papers, parliamentary debates, and more.\n",
    "\n",
    "**The problem with keyword search:** Traditional keyword search (TF-IDF, BM25) only matches documents that share *exact words* with the query.\n",
    "\n",
    "**The solution: Semantic search.** We encode documents and queries into dense vectors that capture meaning.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Load a corpus of scientific abstracts\n",
    "2. Encode all documents into embeddings\n",
    "3. Build a FAISS index for fast retrieval\n",
    "4. Search with natural-language queries\n",
    "5. Evaluate retrieval quality\n",
    "6. Enable cross-lingual search with a compact multilingual model\n",
    "\n",
    "### Why E5 small models here?\n",
    "\n",
    "You asked for a smaller IntFloat model, and that is a good fit for this notebook:\n",
    "- `intfloat/e5-small` is compact and strong for retrieval tasks\n",
    "- `intfloat/multilingual-e5-small` gives cross-lingual retrieval without the heavy bge-m3 footprint\n",
    "- Both require the prompt format:\n",
    "  - documents: `\"passage: ...\"`\n",
    "  - queries: `\"query: ...\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust SciFact loader (handles schema/split differences)\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def load_scifact_corpus(max_docs=300):\n",
    "    ds = load_dataset(\"mteb/scifact\")\n",
    "\n",
    "    if isinstance(ds, dict) or hasattr(ds, \"keys\"):\n",
    "        if \"corpus\" in ds:\n",
    "            split = ds[\"corpus\"]\n",
    "            split_name = \"corpus\"\n",
    "        elif \"train\" in ds:\n",
    "            split = ds[\"train\"]\n",
    "            split_name = \"train\"\n",
    "        else:\n",
    "            first_key = list(ds.keys())[0]\n",
    "            split = ds[first_key]\n",
    "            split_name = first_key\n",
    "    else:\n",
    "        split = ds\n",
    "        split_name = \"(single split)\"\n",
    "\n",
    "    raw_df = split.to_pandas()\n",
    "    print(f\"Loaded split: {split_name}\")\n",
    "    print(f\"Available columns: {list(raw_df.columns)}\")\n",
    "\n",
    "    df = raw_df.copy()\n",
    "\n",
    "    # Ensure doc_id exists\n",
    "    if \"doc_id\" not in df.columns:\n",
    "        if \"_id\" in df.columns:\n",
    "            df[\"doc_id\"] = df[\"_id\"].astype(str)\n",
    "        else:\n",
    "            df[\"doc_id\"] = df.index.astype(str)\n",
    "\n",
    "    # Resolve title/text columns robustly\n",
    "    title_candidates = [\"title\", \"document_title\", \"paper_title\"]\n",
    "    text_candidates = [\"text\", \"abstract\", \"contents\", \"content\", \"document\"]\n",
    "\n",
    "    title_col = next((c for c in title_candidates if c in df.columns), None)\n",
    "    text_col = next((c for c in text_candidates if c in df.columns), None)\n",
    "\n",
    "    if text_col is None:\n",
    "        # Fallback: first object column that is not known metadata\n",
    "        object_cols = [c for c in df.columns if df[c].dtype == object and c not in {\"doc_id\", title_col}]\n",
    "        if object_cols:\n",
    "            text_col = object_cols[0]\n",
    "\n",
    "    if title_col is None:\n",
    "        df[\"title\"] = \"\"\n",
    "    else:\n",
    "        df[\"title\"] = df[title_col].fillna(\"\").astype(str)\n",
    "\n",
    "    if text_col is None:\n",
    "        raise ValueError(f\"Could not identify a text column from: {list(df.columns)}\")\n",
    "    df[\"text\"] = df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "    df = df[[\"doc_id\", \"title\", \"text\"]].copy()\n",
    "    df[\"full_text\"] = (df[\"title\"].str.strip() + \". \" + df[\"text\"].str.strip()).str.strip(\" .\")\n",
    "    df = df[df[\"full_text\"].str.len() > 20].reset_index(drop=True)\n",
    "\n",
    "    if len(df) > max_docs:\n",
    "        df = df.head(max_docs).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "corpus_df = load_scifact_corpus(max_docs=300)\n",
    "\n",
    "print(f\"\\nCorpus size: {len(corpus_df)} documents\")\n",
    "print(\"\\nExample document:\")\n",
    "print(corpus_df.iloc[0][\"full_text\"][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding the Corpus\n",
    "\n",
    "We use a **bi-encoder** approach:\n",
    "\n",
    "1. Encode corpus documents once (offline)\n",
    "2. Encode each query at runtime (online)\n",
    "3. Compare vectors in FAISS\n",
    "\n",
    "For retrieval, we use `intfloat/e5-small`.\n",
    "\n",
    "Important: E5 models expect explicit prefixes:\n",
    "- corpus docs: `passage: ...`\n",
    "- queries: `query: ...`\n",
    "\n",
    "Without these prefixes, retrieval quality drops noticeably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English retrieval model (compact, strong)\n",
    "MODEL_NAME = \"intfloat/e5-small\"\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "\n",
    "def format_passage(text: str) -> str:\n",
    "    return f\"passage: {text.strip()}\"\n",
    "\n",
    "\n",
    "def format_query(text: str) -> str:\n",
    "    return f\"query: {text.strip()}\"\n",
    "\n",
    "\n",
    "corpus_inputs = [format_passage(t) for t in corpus_df[\"full_text\"].tolist()]\n",
    "\n",
    "print(f\"Encoding corpus with {MODEL_NAME}...\")\n",
    "start = time.time()\n",
    "corpus_embeddings = model.encode(\n",
    "    corpus_inputs,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "print(f\"Encoded {len(corpus_embeddings)} documents in {time.time()-start:.1f}s\")\n",
    "print(f\"Embedding shape: {corpus_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a FAISS Index\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) is an open-source library developed by Meta AI for efficient similarity search over dense vectors. It is the standard tool for this task and scales to billions of vectors.\n",
    "\n",
    "FAISS offers many index types. We use **`IndexFlatIP`** (Flat Index with Inner Product):\n",
    "\n",
    "| Index type | Description | Speed | Accuracy |\n",
    "|---|---|---|---|\n",
    "| `IndexFlatIP` | Exact inner product search (brute force) | Slower for huge corpora | 100% exact |\n",
    "| `IndexFlatL2` | Exact L2 distance search | Slower for huge corpora | 100% exact |\n",
    "| `IndexIVFFlat` | Approximate search with inverted file | Fast | Very good |\n",
    "| `IndexHNSW` | Approximate search with graph structure | Very fast | Very good |\n",
    "\n",
    "For our 300-document corpus, `IndexFlatIP` is perfect — it gives exact results and is fast enough. For millions of documents, you would switch to an approximate index like `IndexIVFFlat` or `IndexHNSW`.\n",
    "\n",
    "**Why Inner Product?** Because we normalized our embeddings to unit length, the inner product between two vectors equals their cosine similarity. Higher score = more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Create a FAISS index\n",
    "# e5-small produces 384-dimensional vectors\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product == cosine similarity for normalized vectors\n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(corpus_embeddings.astype(\"float32\"))\n",
    "\n",
    "print(f\"FAISS index built: {index.ntotal} vectors, {dimension} dimensions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Searching!\n",
    "\n",
    "Now for the fun part. We define a `search()` function that:\n",
    "\n",
    "1. Takes a natural-language query string\n",
    "2. Encodes it into an embedding with the same model\n",
    "3. Searches the FAISS index for the `top_k` most similar documents\n",
    "4. Returns a clean DataFrame with ranks, scores, titles, and text previews\n",
    "\n",
    "Let's test it with several queries and see what comes back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Search the corpus for documents matching the query.\"\"\"\n",
    "    query_embedding = model.encode(\n",
    "        [format_query(query)],\n",
    "        normalize_embeddings=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            \"rank\": len(results) + 1,\n",
    "            \"score\": float(score),\n",
    "            \"title\": corpus_df.iloc[idx][\"title\"],\n",
    "            \"text\": corpus_df.iloc[idx][\"full_text\"][:200] + \"...\",\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"effects of climate change on biodiversity\",\n",
    "    \"how do vaccines work\",\n",
    "    \"machine learning for medical diagnosis\",\n",
    "    \"genetic factors in cancer risk\",\n",
    "    \"air pollution and respiratory disease\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {q}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results = search(q, top_k=3)\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"  [{row['rank']}] (score: {row['score']:.3f}) {row['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Retrieval Quality\n",
    "\n",
    "How do we know if our search engine is any good? We need **retrieval evaluation metrics**.\n",
    "\n",
    "The most intuitive metric is **Precision@k**: of the top *k* results returned, how many are actually relevant?\n",
    "\n",
    "$$\\text{Precision@k} = \\frac{\\text{Number of relevant documents in top } k}{k}$$\n",
    "\n",
    "For example, if we retrieve 5 documents and 3 are relevant, Precision@5 = 3/5 = 60%.\n",
    "\n",
    "**The hard part:** Determining relevance usually requires human judgment. For this demo, we will use a simple proxy — checking whether retrieved documents contain keywords related to the query topic. This is imperfect, but gives a rough signal.\n",
    "\n",
    "In a real evaluation, you would use a benchmark dataset with human-annotated relevance judgments (like BEIR, MTEB, or TREC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define queries with expected relevant terms\n",
    "eval_queries = [\n",
    "    {\"query\": \"vaccine effectiveness against viral infections\", \n",
    "     \"relevant_terms\": [\"vaccine\", \"immunization\", \"viral\", \"antibod\"]},\n",
    "    {\"query\": \"genetic mutations and cancer development\",\n",
    "     \"relevant_terms\": [\"genetic\", \"mutation\", \"cancer\", \"tumor\", \"oncog\"]},\n",
    "    {\"query\": \"impact of air pollution on health\",\n",
    "     \"relevant_terms\": [\"pollution\", \"air\", \"respiratory\", \"particulate\"]},\n",
    "]\n",
    "\n",
    "# Simple keyword-based relevance proxy\n",
    "def is_relevant(doc_text, relevant_terms):\n",
    "    doc_lower = doc_text.lower()\n",
    "    return any(term in doc_lower for term in relevant_terms)\n",
    "\n",
    "for eq in eval_queries:\n",
    "    results = search(eq['query'], top_k=5)\n",
    "    relevant = sum(is_relevant(row['text'], eq['relevant_terms']) for _, row in results.iterrows())\n",
    "    precision = relevant / len(results)\n",
    "    print(f\"Query: '{eq['query'][:50]}...'\")\n",
    "    print(f\"  Precision@5: {precision:.0%} ({relevant}/5 relevant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multilingual Search with `intfloat/multilingual-e5-small`\n",
    "\n",
    "Now we switch to a compact multilingual retrieval model.\n",
    "\n",
    "Compared to `bge-m3`, this model is much lighter while still enabling cross-lingual retrieval for teaching-scale demos.\n",
    "\n",
    "As with E5-small, we must keep prefixes:\n",
    "- documents: `passage: ...`\n",
    "- queries: `query: ...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact multilingual retrieval model\n",
    "ML_MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
    "print(f\"Loading {ML_MODEL_NAME} ...\")\n",
    "ml_model = SentenceTransformer(ML_MODEL_NAME)\n",
    "\n",
    "\n",
    "def format_passage_ml(text: str) -> str:\n",
    "    return f\"passage: {text.strip()}\"\n",
    "\n",
    "\n",
    "def format_query_ml(text: str) -> str:\n",
    "    return f\"query: {text.strip()}\"\n",
    "\n",
    "\n",
    "ml_inputs = [format_passage_ml(t) for t in corpus_df[\"full_text\"].tolist()]\n",
    "ml_embeddings = ml_model.encode(\n",
    "    ml_inputs,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "ml_dimension = ml_embeddings.shape[1]\n",
    "ml_index = faiss.IndexFlatIP(ml_dimension)\n",
    "ml_index.add(ml_embeddings.astype(\"float32\"))\n",
    "\n",
    "\n",
    "def search_multilingual(query: str, top_k: int = 5):\n",
    "    \"\"\"Search using multilingual E5 embeddings.\"\"\"\n",
    "    q_emb = ml_model.encode([format_query_ml(query)], normalize_embeddings=True).astype(\"float32\")\n",
    "    scores, indices = ml_index.search(q_emb, top_k)\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            \"rank\": len(results) + 1,\n",
    "            \"score\": float(score),\n",
    "            \"title\": corpus_df.iloc[idx][\"title\"],\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(f\"\\nMultilingual FAISS index built: {ml_index.ntotal} vectors, {ml_dimension} dimensions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same meaning, different languages\n",
    "cross_lingual_queries = [\n",
    "    (\"English\", \"effects of vaccination on immune response\"),\n",
    "    (\"French\", \"effets de la vaccination sur la r\\u00e9ponse immunitaire\"),\n",
    "    (\"German\", \"Auswirkungen der Impfung auf die Immunantwort\"),\n",
    "    (\"Spanish\", \"efectos de la vacunaci\\u00f3n en la respuesta inmune\"),\n",
    "]\n",
    "\n",
    "print(\"Cross-lingual retrieval test:\")\n",
    "print(\"=\"*60)\n",
    "for lang, query in cross_lingual_queries:\n",
    "    results = search_multilingual(query, top_k=3)\n",
    "    print(f\"\\n[{lang}] '{query[:50]}...'\")\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"  [{row['rank']}] ({row['score']:.3f}) {row['title'][:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From FAISS to a Vector Database: ChromaDB\n",
    "\n",
    "FAISS is excellent for understanding how vector search works under the hood — but in production, you often want a **vector database** that handles persistence, metadata filtering, and API convenience for you.\n",
    "\n",
    "**ChromaDB** is a lightweight, open-source vector database that is perfect for prototyping and small-to-medium scale applications. Here is how it compares to raw FAISS:\n",
    "\n",
    "| Feature | FAISS | ChromaDB |\n",
    "|---------|-------|----------|\n",
    "| **Persistence** | Manual (save/load index files) | Built-in (auto-saves to disk) |\n",
    "| **Metadata** | Not supported (vectors only) | Filter by any metadata field |\n",
    "| **Embedding** | BYO (encode externally) | Built-in sentence-transformers |\n",
    "| **API** | Low-level NumPy arrays | High-level Python API |\n",
    "| **Scale** | Billions of vectors | Millions of vectors |\n",
    "| **Best for** | Research, max performance | Prototyping, applications, RAG |\n",
    "\n",
    "Let's build the same search engine using ChromaDB — notice how much simpler the code is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb -q\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# Create an in-memory ChromaDB client (use PersistentClient for disk storage)\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a collection — ChromaDB handles embedding automatically!\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"scifact_abstracts\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    ")\n",
    "\n",
    "# Add documents (ChromaDB embeds them using its default model)\n",
    "collection.add(\n",
    "    documents=corpus_df['full_text'].tolist(),\n",
    "    ids=[str(i) for i in range(len(corpus_df))],\n",
    "    metadatas=[{\"title\": t} for t in corpus_df['title'].tolist()]\n",
    ")\n",
    "\n",
    "# Query — just pass a string, ChromaDB handles the rest\n",
    "results = collection.query(\n",
    "    query_texts=[\"effects of vaccination on immune response\"],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(\"ChromaDB search results:\")\n",
    "for i, (doc, meta, dist) in enumerate(zip(\n",
    "    results['documents'][0], results['metadatas'][0], results['distances'][0]\n",
    ")):\n",
    "    print(f\"  [{i+1}] (distance: {dist:.3f}) {meta['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Which?\n",
    "\n",
    "- **Use FAISS** when you need to understand vector search internals, need maximum performance at scale (billions of vectors), or want full control over the index type and parameters.\n",
    "- **Use ChromaDB** when you are building applications, prototyping RAG pipelines, or need metadata filtering and persistence without managing index files manually.\n",
    "- **Use a managed service** (Pinecone, Weaviate, Qdrant) when you need production-grade infrastructure with replication, auth, and monitoring.\n",
    "\n",
    "In practice, many teams **start with ChromaDB** for rapid prototyping, then move to a managed service as their needs grow. The concepts you learned with FAISS (embeddings, similarity search, index types) transfer directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise: Build Your Own Search Engine\n",
    "\n",
    "Now it's your turn. Build a semantic search engine over a different corpus.\n",
    "\n",
    "**Suggestions:**\n",
    "- Load a different dataset (Wikipedia snippets, news, or your own documents)\n",
    "- Define at least 3 meaningful search queries\n",
    "- Evaluate Precision@5 for each query\n",
    "- Compare:\n",
    "  - `intfloat/e5-small` (English-focused)\n",
    "  - `intfloat/multilingual-e5-small` (cross-lingual)\n",
    "\n",
    "**Bonus:** Try `IndexIVFFlat` instead of `IndexFlatIP` and compare speed/accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 1: Load a corpus\n",
    "# e.g., dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:500]\")\n",
    "\n",
    "\n",
    "# Step 2: Encode the corpus with a sentence-transformer model\n",
    "\n",
    "\n",
    "# Step 3: Build a FAISS index\n",
    "\n",
    "\n",
    "# Step 4: Define search queries and test them\n",
    "\n",
    "\n",
    "# Step 5: Evaluate Precision@5 with relevance terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Takeaways\n",
    "\n",
    "In this notebook, we built a complete semantic search system with FAISS and compact E5 models.\n",
    "\n",
    "| Concept | What we learned |\n",
    "|---|---|\n",
    "| **Semantic search** | Dense embeddings capture meaning, not just keywords. |\n",
    "| **FAISS** | `IndexFlatIP` gives exact nearest-neighbor retrieval over normalized vectors. |\n",
    "| **Bi-encoder paradigm** | Encode corpus once, then encode each query and search fast. |\n",
    "| **E5 prompt format** | For E5 models, `passage:` for docs and `query:` for queries is critical for quality. |\n",
    "| **Multilingual retrieval** | `intfloat/multilingual-e5-small` enables cross-lingual search without a heavy model. |\n",
    "\n",
    "### Notes for practice\n",
    "\n",
    "- If your corpus is English-only and you need speed, `intfloat/e5-small` is a strong default.\n",
    "- If users query in multiple languages, switch to `intfloat/multilingual-e5-small`.\n",
    "- For large production corpora, move from `IndexFlatIP` to approximate indices (`IVF`, `HNSW`) and add reranking.\n",
    "\n",
    "In **NB07**, we add reranking on top of retrieval for better precision.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
