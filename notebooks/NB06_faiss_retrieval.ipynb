{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NB06: FAISS Retrieval + Semantic Search\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB06_faiss_retrieval.ipynb)\n\n**Duration:** ~70 minutes\n\n## Learning Goals\n\nBy the end of this notebook, you will be able to:\n\n1. **Build a semantic search system** from scratch using dense embeddings\n2. **Use FAISS for fast similarity search** over large document collections\n3. **Understand the bi-encoder retrieval paradigm** — encode once, search many times\n4. **Try multilingual search with bge-m3** — query in one language, retrieve in another\n\n---\n\n**Prerequisites:** NB02 (sentence embeddings). Familiarity with cosine similarity and vector representations of text."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu sentence-transformers datasets pandas numpy -q\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "print(f\"FAISS version: {faiss.__version__}\")\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Use Case: Searching Policy Documents\n",
    "\n",
    "Social scientists, policy analysts, and researchers frequently need to find relevant documents in large collections — policy briefs, legal texts, academic papers, parliamentary debates, and more.\n",
    "\n",
    "**The problem with keyword search:** Traditional keyword search (TF-IDF, BM25) only matches documents that share *exact words* with the query. This means:\n",
    "\n",
    "- Searching for \"climate change effects on wildlife\" will **miss** a document titled \"global warming impacts on animal populations\" — even though they address the same topic.\n",
    "- Searching for \"vaccination efficacy\" will **miss** documents about \"how well immunization works.\"\n",
    "\n",
    "**The solution: Semantic search.** We encode documents and queries into dense vector representations (embeddings) that capture *meaning*, not just surface words. Similar meanings produce similar vectors, regardless of the exact words used.\n",
    "\n",
    "In this notebook, we will build a complete semantic search engine:\n",
    "\n",
    "1. Load a corpus of scientific abstracts\n",
    "2. Encode all documents into embeddings\n",
    "3. Build a FAISS index for fast retrieval\n",
    "4. Search with natural-language queries\n",
    "5. Evaluate retrieval quality\n",
    "6. Enable cross-lingual search with a multilingual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a subset of scientific abstracts as our corpus\n",
    "# In practice, this could be policy documents, legal texts, academic papers\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small corpus of abstracts\n",
    "dataset = load_dataset(\"mteb/scifact\", split=\"corpus\")\n",
    "corpus_df = dataset.to_pandas().head(300)\n",
    "corpus_df.columns = ['doc_id', 'title', 'text']\n",
    "corpus_df['full_text'] = corpus_df['title'] + \". \" + corpus_df['text']\n",
    "\n",
    "print(f\"Corpus size: {len(corpus_df)} documents\")\n",
    "print(f\"\\nExample document:\")\n",
    "print(corpus_df.iloc[0]['full_text'][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding the Corpus\n",
    "\n",
    "We use a **bi-encoder** approach to semantic search:\n",
    "\n",
    "1. **Offline step (done once):** Encode all documents in the corpus into fixed-size embedding vectors and store them.\n",
    "2. **Online step (done per query):** Encode the user's query into the same embedding space, then find the nearest document vectors.\n",
    "\n",
    "This is extremely efficient because:\n",
    "- The expensive corpus encoding happens **once** and can be cached.\n",
    "- At query time, we only need to encode **one short query** and perform a vector lookup.\n",
    "\n",
    "We start with `all-MiniLM-L6-v2`, a lightweight model (80 MB) that produces 384-dimensional embeddings. It is fast, effective for English, and a good baseline.\n",
    "\n",
    "**Key detail:** We set `normalize_embeddings=True` so that all vectors have unit length. This means the inner product (dot product) between any two vectors equals their cosine similarity — which is exactly what we want for measuring semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a lightweight but effective model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Encoding corpus...\")\n",
    "start = time.time()\n",
    "corpus_embeddings = model.encode(\n",
    "    corpus_df['full_text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64,\n",
    "    normalize_embeddings=True  # Important for cosine similarity with FAISS\n",
    ")\n",
    "print(f\"Encoded {len(corpus_embeddings)} documents in {time.time()-start:.1f}s\")\n",
    "print(f\"Embedding shape: {corpus_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a FAISS Index\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) is an open-source library developed by Meta AI for efficient similarity search over dense vectors. It is the standard tool for this task and scales to billions of vectors.\n",
    "\n",
    "FAISS offers many index types. We use **`IndexFlatIP`** (Flat Index with Inner Product):\n",
    "\n",
    "| Index type | Description | Speed | Accuracy |\n",
    "|---|---|---|---|\n",
    "| `IndexFlatIP` | Exact inner product search (brute force) | Slower for huge corpora | 100% exact |\n",
    "| `IndexFlatL2` | Exact L2 distance search | Slower for huge corpora | 100% exact |\n",
    "| `IndexIVFFlat` | Approximate search with inverted file | Fast | Very good |\n",
    "| `IndexHNSW` | Approximate search with graph structure | Very fast | Very good |\n",
    "\n",
    "For our 300-document corpus, `IndexFlatIP` is perfect — it gives exact results and is fast enough. For millions of documents, you would switch to an approximate index like `IndexIVFFlat` or `IndexHNSW`.\n",
    "\n",
    "**Why Inner Product?** Because we normalized our embeddings to unit length, the inner product between two vectors equals their cosine similarity. Higher score = more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = corpus_embeddings.shape[1]  # 384 for MiniLM\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product = cosine sim (normalized)\n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(corpus_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index built: {index.ntotal} vectors, {dimension} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Searching!\n",
    "\n",
    "Now for the fun part. We define a `search()` function that:\n",
    "\n",
    "1. Takes a natural-language query string\n",
    "2. Encodes it into an embedding with the same model\n",
    "3. Searches the FAISS index for the `top_k` most similar documents\n",
    "4. Returns a clean DataFrame with ranks, scores, titles, and text previews\n",
    "\n",
    "Let's test it with several queries and see what comes back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Search the corpus for documents matching the query.\"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    \n",
    "    # Search FAISS index\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            'rank': len(results) + 1,\n",
    "            'score': float(score),\n",
    "            'title': corpus_df.iloc[idx]['title'],\n",
    "            'text': corpus_df.iloc[idx]['full_text'][:200] + '...'\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"effects of climate change on biodiversity\",\n",
    "    \"how do vaccines work\",\n",
    "    \"machine learning for medical diagnosis\",\n",
    "    \"genetic factors in cancer risk\",\n",
    "    \"air pollution and respiratory disease\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {q}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results = search(q, top_k=3)\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"  [{row['rank']}] (score: {row['score']:.3f}) {row['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Retrieval Quality\n",
    "\n",
    "How do we know if our search engine is any good? We need **retrieval evaluation metrics**.\n",
    "\n",
    "The most intuitive metric is **Precision@k**: of the top *k* results returned, how many are actually relevant?\n",
    "\n",
    "$$\\text{Precision@k} = \\frac{\\text{Number of relevant documents in top } k}{k}$$\n",
    "\n",
    "For example, if we retrieve 5 documents and 3 are relevant, Precision@5 = 3/5 = 60%.\n",
    "\n",
    "**The hard part:** Determining relevance usually requires human judgment. For this demo, we will use a simple proxy — checking whether retrieved documents contain keywords related to the query topic. This is imperfect, but gives a rough signal.\n",
    "\n",
    "In a real evaluation, you would use a benchmark dataset with human-annotated relevance judgments (like BEIR, MTEB, or TREC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define queries with expected relevant terms\n",
    "eval_queries = [\n",
    "    {\"query\": \"vaccine effectiveness against viral infections\", \n",
    "     \"relevant_terms\": [\"vaccine\", \"immunization\", \"viral\", \"antibod\"]},\n",
    "    {\"query\": \"genetic mutations and cancer development\",\n",
    "     \"relevant_terms\": [\"genetic\", \"mutation\", \"cancer\", \"tumor\", \"oncog\"]},\n",
    "    {\"query\": \"impact of air pollution on health\",\n",
    "     \"relevant_terms\": [\"pollution\", \"air\", \"respiratory\", \"particulate\"]},\n",
    "]\n",
    "\n",
    "# Simple keyword-based relevance proxy\n",
    "def is_relevant(doc_text, relevant_terms):\n",
    "    doc_lower = doc_text.lower()\n",
    "    return any(term in doc_lower for term in relevant_terms)\n",
    "\n",
    "for eq in eval_queries:\n",
    "    results = search(eq['query'], top_k=5)\n",
    "    relevant = sum(is_relevant(row['text'], eq['relevant_terms']) for _, row in results.iterrows())\n",
    "    precision = relevant / len(results)\n",
    "    print(f\"Query: '{eq['query'][:50]}...'\")\n",
    "    print(f\"  Precision@5: {precision:.0%} ({relevant}/5 relevant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multilingual Search with bge-m3\n",
    "\n",
    "So far, we have used an English-only model. But what if your corpus is in English and your users search in French, German, or Spanish?\n",
    "\n",
    "**bge-m3** (BAAI General Embedding — Multi-lingual, Multi-granularity, Multi-functionality) is a state-of-the-art multilingual embedding model that:\n",
    "\n",
    "- Supports **100+ languages**\n",
    "- Produces **1024-dimensional** embeddings\n",
    "- Enables **cross-lingual retrieval**: query in one language, retrieve in another\n",
    "\n",
    "This is incredibly useful for:\n",
    "- Multilingual policy analysis (e.g., EU documents in 24 languages)\n",
    "- Comparative political science across countries\n",
    "- Searching English academic literature with non-English queries\n",
    "\n",
    "The key insight: bge-m3 maps semantically equivalent sentences from different languages to **nearby points** in the same embedding space. So \"vaccination\" (EN), \"vaccination\" (FR), \"Impfung\" (DE), and \"vacunaci\\u00f3n\" (ES) all end up close together.\n",
    "\n",
    "**Note:** bge-m3 is larger (~2 GB) and slower than MiniLM. Loading and encoding will take a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multilingual model (larger but much more powerful)\n",
    "print(\"Loading multilingual model (this may take a minute)...\")\n",
    "ml_model = SentenceTransformer('BAAI/bge-m3')\n",
    "\n",
    "# Re-encode corpus with multilingual model\n",
    "ml_embeddings = ml_model.encode(\n",
    "    corpus_df['full_text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Build new FAISS index\n",
    "ml_dimension = ml_embeddings.shape[1]\n",
    "ml_index = faiss.IndexFlatIP(ml_dimension)\n",
    "ml_index.add(ml_embeddings.astype('float32'))\n",
    "\n",
    "def search_multilingual(query: str, top_k: int = 5):\n",
    "    \"\"\"Search using multilingual embeddings.\"\"\"\n",
    "    q_emb = ml_model.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    scores, indices = ml_index.search(q_emb, top_k)\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            'rank': len(results) + 1,\n",
    "            'score': float(score),\n",
    "            'title': corpus_df.iloc[idx]['title'],\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nMultilingual FAISS index built: {ml_index.ntotal} vectors, {ml_dimension} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same meaning, different languages\n",
    "cross_lingual_queries = [\n",
    "    (\"English\", \"effects of vaccination on immune response\"),\n",
    "    (\"French\", \"effets de la vaccination sur la r\\u00e9ponse immunitaire\"),\n",
    "    (\"German\", \"Auswirkungen der Impfung auf die Immunantwort\"),\n",
    "    (\"Spanish\", \"efectos de la vacunaci\\u00f3n en la respuesta inmune\"),\n",
    "]\n",
    "\n",
    "print(\"Cross-lingual retrieval test:\")\n",
    "print(\"=\"*60)\n",
    "for lang, query in cross_lingual_queries:\n",
    "    results = search_multilingual(query, top_k=3)\n",
    "    print(f\"\\n[{lang}] '{query[:50]}...'\")\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"  [{row['rank']}] ({row['score']:.3f}) {row['title'][:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## From FAISS to a Vector Database: ChromaDB\n\nFAISS is excellent for understanding how vector search works under the hood — but in production, you often want a **vector database** that handles persistence, metadata filtering, and API convenience for you.\n\n**ChromaDB** is a lightweight, open-source vector database that is perfect for prototyping and small-to-medium scale applications. Here is how it compares to raw FAISS:\n\n| Feature | FAISS | ChromaDB |\n|---------|-------|----------|\n| **Persistence** | Manual (save/load index files) | Built-in (auto-saves to disk) |\n| **Metadata** | Not supported (vectors only) | Filter by any metadata field |\n| **Embedding** | BYO (encode externally) | Built-in sentence-transformers |\n| **API** | Low-level NumPy arrays | High-level Python API |\n| **Scale** | Billions of vectors | Millions of vectors |\n| **Best for** | Research, max performance | Prototyping, applications, RAG |\n\nLet's build the same search engine using ChromaDB — notice how much simpler the code is.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install chromadb -q\n\nimport chromadb\n\n# Create an in-memory ChromaDB client (use PersistentClient for disk storage)\nchroma_client = chromadb.Client()\n\n# Create a collection — ChromaDB handles embedding automatically!\ncollection = chroma_client.create_collection(\n    name=\"scifact_abstracts\",\n    metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n)\n\n# Add documents (ChromaDB embeds them using its default model)\ncollection.add(\n    documents=corpus_df['full_text'].tolist(),\n    ids=[str(i) for i in range(len(corpus_df))],\n    metadatas=[{\"title\": t} for t in corpus_df['title'].tolist()]\n)\n\n# Query — just pass a string, ChromaDB handles the rest\nresults = collection.query(\n    query_texts=[\"effects of vaccination on immune response\"],\n    n_results=5\n)\n\nprint(\"ChromaDB search results:\")\nfor i, (doc, meta, dist) in enumerate(zip(\n    results['documents'][0], results['metadatas'][0], results['distances'][0]\n)):\n    print(f\"  [{i+1}] (distance: {dist:.3f}) {meta['title']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### When to Use Which?\n\n- **Use FAISS** when you need to understand vector search internals, need maximum performance at scale (billions of vectors), or want full control over the index type and parameters.\n- **Use ChromaDB** when you are building applications, prototyping RAG pipelines, or need metadata filtering and persistence without managing index files manually.\n- **Use a managed service** (Pinecone, Weaviate, Qdrant) when you need production-grade infrastructure with replication, auth, and monitoring.\n\nIn practice, many teams **start with ChromaDB** for rapid prototyping, then move to a managed service as their needs grow. The concepts you learned with FAISS (embeddings, similarity search, index types) transfer directly.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise: Build Your Own Search Engine\n",
    "\n",
    "Now it's your turn! Build a semantic search engine over a different corpus.\n",
    "\n",
    "**Suggestions:**\n",
    "- Load a different dataset (e.g., Wikipedia snippets, news articles, or your own research papers)\n",
    "- Define at least 3 meaningful search queries relevant to the corpus\n",
    "- Evaluate Precision@5 for each query using keyword-based or manual relevance judgments\n",
    "- Compare results between the English model (`all-MiniLM-L6-v2`) and the multilingual model (`BAAI/bge-m3`)\n",
    "\n",
    "**Bonus:** Try indexing with `IndexIVFFlat` instead of `IndexFlatIP` and compare speed/accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 1: Load a corpus\n",
    "# e.g., dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:500]\")\n",
    "\n",
    "\n",
    "# Step 2: Encode the corpus with a sentence-transformer model\n",
    "\n",
    "\n",
    "# Step 3: Build a FAISS index\n",
    "\n",
    "\n",
    "# Step 4: Define search queries and test them\n",
    "\n",
    "\n",
    "# Step 5: Evaluate Precision@5 with relevance terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Takeaways\n",
    "\n",
    "In this notebook, we built a complete semantic search system. Here are the key takeaways:\n",
    "\n",
    "| Concept | What we learned |\n",
    "|---|---|\n",
    "| **Semantic search** | Dense embeddings capture meaning, not just keywords — enabling retrieval of semantically similar documents even when they use different words. |\n",
    "| **FAISS** | Facebook AI Similarity Search provides fast, scalable nearest-neighbor search over dense vectors. `IndexFlatIP` gives exact results; approximate indices scale to billions of vectors. |\n",
    "| **Bi-encoder paradigm** | Encode the corpus once (offline), then encode queries at search time (online). This separation makes retrieval extremely fast. |\n",
    "| **Multilingual retrieval** | Models like bge-m3 map text from 100+ languages into a shared embedding space, enabling cross-lingual search — query in French, retrieve English documents. |\n",
    "| **Evaluation** | Precision@k measures how many of the top-k retrieved documents are relevant. Real evaluation requires human-annotated relevance judgments. |\n",
    "\n",
    "### Limitations of bi-encoder retrieval\n",
    "\n",
    "Bi-encoders are fast but imperfect. Because query and document are encoded **independently**, the model cannot attend to fine-grained interactions between them. This means:\n",
    "\n",
    "- Subtle semantic distinctions may be missed\n",
    "- The top-1 result is not always the best — but the correct answer is usually somewhere in the top 10-20\n",
    "\n",
    "### What's next?\n",
    "\n",
    "In **NB07**, we will address these limitations by adding a **cross-encoder reranker** on top of the bi-encoder retriever. The cross-encoder processes each (query, document) pair jointly, enabling much more precise relevance scoring. The typical pipeline:\n",
    "\n",
    "1. **Retrieve** the top 50-100 candidates with a bi-encoder (fast but approximate)\n",
    "2. **Rerank** those candidates with a cross-encoder (slow but precise)\n",
    "\n",
    "This two-stage approach gives you both speed and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}