{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB01: TF-IDF + Linear Models -- Baselines that Win\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB01_tfidf_baselines.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- **Build a complete text classification pipeline** from raw text to predictions using scikit-learn\n",
    "- **Understand TF-IDF** -- how term frequency and inverse document frequency turn words into numbers\n",
    "- **Evaluate models** with confusion matrices and classification reports (precision, recall, F1)\n",
    "- **Analyze errors** -- the single most important skill for improving any NLP system\n",
    "\n",
    "**Estimated time:** ~90 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful.\n"
     ]
    }
   ],
   "source": [
    "# ── Setup ────────────────────────────────────────────────────────────\n",
    "# Install dependencies (Colab-friendly)\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn -q\n",
    "\n",
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Dataset\n",
    "\n",
    "We will work with **dk_posts** -- a collection of **457 synthetic Danish Reddit advice posts** that have been translated into English and labelled into **8 categories**:\n",
    "\n",
    "| Category | Description |\n",
    "|---|---|\n",
    "| Love & Dating | Romantic relationships and dating |\n",
    "| Family Dynamics | Issues with parents, siblings, relatives |\n",
    "| Friendship & Social Life | Making/keeping friends, social anxiety |\n",
    "| Work, Study & Career | Jobs, education, career decisions |\n",
    "| Health & Wellness (Physical and Mental) | Physical and mental health concerns |\n",
    "| Personal Finance & Housing | Money, rent, budgeting |\n",
    "| Everyday Observations & Rants | General complaints and observations |\n",
    "| Practical Questions & Everyday Life | How-to questions, daily life tips |\n",
    "\n",
    "This dataset is our **running example for NB01 through NB03**. It is small enough to iterate quickly, yet diverse enough to show real classification challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /Users/roman/Desktop/UNISTRA-NLP-2026/data/dk_posts_synth_en_processed.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2972701441.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLocal_DATA_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/roman/Desktop/UNISTRA-NLP-2026/data/dk_posts_synth_en_processed.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLocal_DATA_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mconvert_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ujson\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         ):\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {filepath_or_buffer} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             warnings.warn(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /Users/roman/Desktop/UNISTRA-NLP-2026/data/dk_posts_synth_en_processed.json does not exist"
     ]
    }
   ],
   "source": [
    "# ── Load data from GitHub ────────────────────────────────────────────\n",
    "DATA_URL = \"https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/data/dk_posts_synth_en_processed.json\"\n",
    "\n",
    "df = pd.read_json(DATA_URL)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print()\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n── Label distribution ──\")\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Before we can classify posts, we need to prepare the text:\n",
    "\n",
    "1. **Combine title + body** -- many posts carry important signal in both the title and the selftext. We join them with a period separator so the vectorizer sees one coherent document.\n",
    "2. **Basic cleaning** -- lowercase everything and collapse extra whitespace. We intentionally keep it minimal: TF-IDF with stop-word removal already handles most noise, and aggressive cleaning (stemming, lemmatization) often hurts more than it helps for short social-media text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Combine title + selftext ─────────────────────────────────────────\n",
    "df[\"text\"] = df[\"title\"] + \" . \" + df[\"selftext\"]\n",
    "\n",
    "\n",
    "# ── Simple cleaning function ─────────────────────────────────────────\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, strip, and collapse whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# ── Before / after example ───────────────────────────────────────────\n",
    "idx = 0\n",
    "print(\"BEFORE cleaning:\")\n",
    "print(df.loc[idx, \"text\"][:200])\n",
    "print()\n",
    "print(\"AFTER cleaning:\")\n",
    "print(df.loc[idx, \"text_clean\"][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split\n",
    "\n",
    "We hold out **25 %** of the data for evaluation. The `stratify` parameter ensures that every class keeps roughly the same proportion in both the training and test sets -- essential when some categories are smaller than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stratified 75/25 split ───────────────────────────────────────────\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text_clean\"],\n",
    "    df[\"label\"],\n",
    "    test_size=0.25,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples:     {X_test.shape[0]}\")\n",
    "print(f\"Number of classes: {y_train.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF + Logistic Regression\n",
    "\n",
    "### What is TF-IDF?\n",
    "\n",
    "**TF-IDF** (Term Frequency -- Inverse Document Frequency) turns each document into a sparse vector:\n",
    "\n",
    "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\log\\!\\left(\\frac{N}{\\text{DF}(t)}\\right)$$\n",
    "\n",
    "- **TF (Term Frequency):** how often a word appears *in this document* -- frequent words get higher scores.\n",
    "- **IDF (Inverse Document Frequency):** down-weights words that appear in *many* documents (e.g., \"the\", \"is\").\n",
    "- Together they surface **distinctive** words: terms that appear often in one document but rarely elsewhere.\n",
    "\n",
    "### Why ngrams?\n",
    "\n",
    "With `ngram_range=(1, 2)` we look at both single words (*unigrams*) and pairs of consecutive words (*bigrams*). Bigrams capture phrases like \"mental health\" or \"student job\" that carry more meaning than the individual words.\n",
    "\n",
    "### Why Logistic Regression?\n",
    "\n",
    "Logistic Regression is a fast, interpretable linear model. It learns a weight for each TF-IDF feature per class. Despite its simplicity, it is one of the strongest baselines for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Pipeline: TF-IDF + Logistic Regression ──────────────────────────\n",
    "pipe_lr = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=10_000,\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred_lr = pipe_lr.predict(X_test)\n",
    "\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression accuracy: {acc_lr:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TF-IDF + Linear SVC\n",
    "\n",
    "**Support Vector Machines (SVM)** find the hyperplane that maximally separates classes. `LinearSVC` is an efficient implementation that works directly in the high-dimensional TF-IDF space. SVMs have been the go-to classifier for text since the early 2000s because:\n",
    "\n",
    "- High-dimensional sparse features (like TF-IDF) are exactly what SVMs handle well.\n",
    "- The max-margin objective acts as a strong regularizer.\n",
    "\n",
    "Let's see whether it beats Logistic Regression on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Pipeline: TF-IDF + Linear SVC ───────────────────────────────────\n",
    "pipe_svc = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=10_000,\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(\n",
    "        max_iter=2000,\n",
    "    )),\n",
    "])\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "y_pred_svc = pipe_svc.predict(X_test)\n",
    "\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "print(f\"Linear SVC accuracy: {acc_svc:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix\n",
    "\n",
    "A **confusion matrix** shows, for each true class, how many samples were predicted as each class. The diagonal cells are correct predictions; off-diagonal cells are errors. This visualization immediately reveals:\n",
    "\n",
    "- Which classes are **easy** (strong diagonal)\n",
    "- Which classes **get confused** with each other (bright off-diagonal cells)\n",
    "\n",
    "We plot the confusion matrix for the better-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Pick the better model for visualization ─────────────────────────\n",
    "if acc_svc >= acc_lr:\n",
    "    best_name = \"Linear SVC\"\n",
    "    best_pred = y_pred_svc\n",
    "else:\n",
    "    best_name = \"Logistic Regression\"\n",
    "    best_pred = y_pred_lr\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    best_pred,\n",
    "    ax=ax,\n",
    "    cmap=\"Blues\",\n",
    "    xticks_rotation=45,\n",
    ")\n",
    "ax.set_title(f\"Confusion Matrix -- TF-IDF + {best_name}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Top Features per Class\n",
    "\n",
    "One of the biggest advantages of linear models is **interpretability**. Both Logistic Regression and LinearSVC learn a coefficient vector for each class. The features with the **highest positive coefficients** are the ones most associated with predicting that class.\n",
    "\n",
    "Let's extract the top-10 TF-IDF features for each of the 8 categories from the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Top 10 TF-IDF features per class (Logistic Regression) ──────────\n",
    "vectorizer = pipe_lr.named_steps[\"tfidf\"]\n",
    "clf_lr = pipe_lr.named_steps[\"clf\"]\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "class_labels = clf_lr.classes_\n",
    "\n",
    "top_n = 10\n",
    "top_features = {}\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    coef = clf_lr.coef_[i]\n",
    "    top_idx = np.argsort(coef)[-top_n:][::-1]\n",
    "    top_features[label] = [feature_names[j] for j in top_idx]\n",
    "\n",
    "top_df = pd.DataFrame(top_features)\n",
    "top_df.index = [f\"#{r+1}\" for r in range(top_n)]\n",
    "top_df.index.name = \"Rank\"\n",
    "\n",
    "# Display with transposed view for readability\n",
    "print(\"Top 10 TF-IDF features per class (Logistic Regression)\\n\")\n",
    "display(top_df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis\n",
    "\n",
    "Looking at aggregate metrics is not enough. **The most important skill in applied NLP is reading the errors.** By examining specific misclassified examples -- especially the ones the model was *most confident* about -- we can discover:\n",
    "\n",
    "- Ambiguous or mislabelled data\n",
    "- Overlapping categories that might need to be merged\n",
    "- Systematic biases (e.g., certain keywords triggering the wrong class)\n",
    "\n",
    "We use the Logistic Regression pipeline because it supports `predict_proba`, which gives us calibrated confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Error analysis with confidence scores ───────────────────────────\n",
    "proba = pipe_lr.predict_proba(X_test)\n",
    "pred_lr = pipe_lr.predict(X_test)\n",
    "\n",
    "# Build a DataFrame of all test predictions\n",
    "error_df = pd.DataFrame({\n",
    "    \"text\": X_test.values,\n",
    "    \"true_label\": y_test.values,\n",
    "    \"predicted_label\": pred_lr,\n",
    "    \"confidence\": proba.max(axis=1),\n",
    "})\n",
    "\n",
    "# Keep only the misclassified examples\n",
    "errors = error_df[error_df[\"true_label\"] != error_df[\"predicted_label\"]].copy()\n",
    "errors[\"text\"] = errors[\"text\"].str[:100] + \"...\"\n",
    "\n",
    "# Sort by confidence descending -- the worst mistakes\n",
    "errors = errors.sort_values(\"confidence\", ascending=False)\n",
    "\n",
    "print(f\"Total test samples: {len(error_df)}\")\n",
    "print(f\"Misclassified:      {len(errors)} ({len(errors)/len(error_df)*100:.1f}%)\")\n",
    "print()\n",
    "print(\"── Top 5 most-confident wrong predictions ──\\n\")\n",
    "display(errors.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to look for in the errors above:**\n",
    "\n",
    "- **High-confidence mistakes** often indicate label noise or genuinely ambiguous posts (e.g., a post about stress at work could be \"Work, Study & Career\" *or* \"Health & Wellness\").\n",
    "- **Recurring confusions** between two categories suggest those categories may overlap in the dataset or that the model needs more discriminative features.\n",
    "- If many errors share a keyword pattern, a custom feature or different preprocessing might help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Takeaways\n",
    "\n",
    "### Results\n",
    "\n",
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| TF-IDF + Logistic Regression | see above |\n",
    "| TF-IDF + Linear SVC | see above |\n",
    "\n",
    "### When should you use TF-IDF + linear models?\n",
    "\n",
    "- **Fast to train and predict** -- no GPU required, fits in seconds\n",
    "- **Highly interpretable** -- you can inspect exactly which words drive each prediction\n",
    "- **Excellent baseline** -- always start here before reaching for deep learning\n",
    "- **Works well with small data** -- 457 samples is plenty for a linear model; a Transformer would likely overfit\n",
    "- **Production-friendly** -- tiny model size, no external dependencies beyond scikit-learn\n",
    "\n",
    "### What's next?\n",
    "\n",
    "In **NB02** we will replace TF-IDF with **sentence embeddings** (SBERT) and see whether capturing semantic meaning improves classification. We will carry forward the same dataset, split, and evaluation setup so the comparison is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Results comparison ───────────────────────────────────────────────\n",
    "results = {\n",
    "    \"TF-IDF + Logistic Regression\": acc_lr,\n",
    "    \"TF-IDF + Linear SVC\": acc_svc,\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    list(results.items()),\n",
    "    columns=[\"Model\", \"Accuracy\"],\n",
    ").sort_values(\"Accuracy\", ascending=False)\n",
    "\n",
    "print(\"── Model Comparison ──\\n\")\n",
    "display(results_df.reset_index(drop=True))\n",
    "\n",
    "# Store for re-use in NB02\n",
    "print(\"\\nResults dict (copy into NB02 for comparison):\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
