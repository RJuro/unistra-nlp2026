{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NB05: SetFit Few-shot Classification\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB05_setfit_fewshot.ipynb)\n\n**Duration:** 70 minutes\n\n> **GPU recommended** — go to **Runtime → Change runtime type → T4 GPU**. SetFit fine-tunes a sentence transformer via contrastive learning — GPU speeds this up significantly, especially for the label efficiency experiments in Section 5.\n\n## Learning Goals\n\nBy the end of this notebook, you will be able to:\n\n1. **Train classifiers with only 8--32 labeled examples** using SetFit, a few-shot learning framework built on sentence transformers.\n2. **Understand contrastive learning** -- how SetFit constructs positive and negative sentence pairs to fine-tune an embedding model with minimal supervision.\n3. **Bootstrap training pairs with LLMs** -- use a large language model to paraphrase and augment your tiny labeled set, creating additional contrastive pairs.\n4. **Measure label efficiency** -- compare SetFit against traditional baselines (TF-IDF + Logistic Regression) across different training-set sizes, and reason about when few-shot methods pay off.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setfit datasets sentence-transformers openai pandas scikit-learn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ── GPU Check ─────────────────────────────────────────────────────────────\nimport torch\n\nif torch.cuda.is_available():\n    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\nelse:\n    print(\"No GPU detected — running on CPU.\")\n    print(\"SetFit training will be slower but still works (~2-5 min per experiment).\")\n    print(\"To enable GPU: Runtime → Change runtime type → T4 GPU\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Task: Detecting Environmental Claims\n",
    "\n",
    "Companies frequently make environmental or \"green\" claims in their public communications -- annual reports, press releases, sustainability disclosures. Some of these are substantive; many are vague or misleading (\"greenwashing\").\n",
    "\n",
    "**Can we build a classifier to detect environmental claims with minimal labeled data?**\n",
    "\n",
    "We will use the [climatebert/environmental_claims](https://huggingface.co/datasets/climatebert/environmental_claims) dataset, which contains sentences labeled as:\n",
    "\n",
    "| Label | Meaning |\n",
    "|-------|---------|\n",
    "| **0** | Not an environmental claim |\n",
    "| **1** | Environmental claim |\n",
    "\n",
    "In a real-world scenario, you might be a regulator or researcher who has time to label only a handful of examples. This is exactly where few-shot learning shines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"climatebert/environmental_claims\")\n",
    "print(dataset)\n",
    "print(f\"\\nTrain size: {len(dataset['train'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")\n",
    "\n",
    "# Look at examples\n",
    "train_df = dataset['train'].to_pandas()\n",
    "print(f\"\\nLabel distribution:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"\\nExample claim:\")\n",
    "print(train_df[train_df.label == 1].iloc[0]['text'][:200])\n",
    "print(f\"\\nExample non-claim:\")\n",
    "print(train_df[train_df.label == 0].iloc[0]['text'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Few-shot Challenge\n",
    "\n",
    "What happens when you only have **8 labeled examples**? Traditional machine-learning classifiers struggle badly:\n",
    "\n",
    "- **TF-IDF + Logistic Regression** needs hundreds or thousands of examples to learn reliable word-level patterns.\n",
    "- **Fine-tuning a BERT-style model** on 8 examples will massively overfit -- the model has ~110M parameters but only 8 gradient signals.\n",
    "- **Zero-shot prompting** (e.g., with GPT-4) can work, but it is slow, expensive, and hard to deploy at scale.\n",
    "\n",
    "**SetFit** fills this gap. It leverages **contrastive learning** to squeeze maximum signal from every labeled example, achieving results competitive with full fine-tuning while needing orders of magnitude fewer labels.\n",
    "\n",
    "| Method | Labels needed | Accuracy (typical) |\n",
    "|--------|:------------:|:-------------------:|\n",
    "| TF-IDF + LR | 500+ | Moderate |\n",
    "| Fine-tuned BERT | 200+ | High |\n",
    "| SetFit | **8--32** | **High** |\n",
    "| Zero-shot LLM | 0 | Variable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. SetFit: How It Works\n\nSetFit (**S**entence-**T**ransformer **F**ine-tuning) is a two-phase framework designed for few-shot text classification:\n\n### Phase 1: Contrastive Fine-tuning of the Sentence Transformer\n\nGiven your few labeled examples, SetFit generates **sentence pairs**:\n\n- **Positive pairs**: two sentences with the **same** label. The model learns to pull their embeddings **closer** together.\n- **Negative pairs**: two sentences with **different** labels. The model learns to push their embeddings **further apart**.\n\nFrom just 8 examples (4 per class), SetFit can generate many contrastive pairs:\n- Positive pairs from class 0: C(4,2) = 6 pairs\n- Positive pairs from class 1: C(4,2) = 6 pairs\n- Negative pairs: 4 x 4 = 16 pairs\n\nWith the `num_iterations` parameter, even more pairs are sampled with replacement. This is the key insight: **contrastive learning multiplies your labeled data**.\n\n### Phase 2: Train a Classification Head\n\nOnce the sentence transformer is fine-tuned, SetFit:\n1. Encodes all few-shot examples using the fine-tuned model.\n2. Trains a simple **logistic regression** classifier on the resulting embeddings.\n\nThis two-phase approach is fast (no large LM fine-tuning needed), data-efficient, and produces a compact, deployable model.\n\n![SetFit Pipeline](https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/notebooks/figures/setfit_pipeline.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_few_shot(dataset, n_per_class, seed=42):\n",
    "    \"\"\"Sample n examples per class for few-shot training.\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "\n",
    "    train_data = dataset['train'].to_pandas()\n",
    "    samples = []\n",
    "    for label in sorted(train_data['label'].unique()):\n",
    "        class_data = train_data[train_data.label == label]\n",
    "        selected = class_data.sample(n=min(n_per_class, len(class_data)), random_state=seed)\n",
    "        samples.append(selected)\n",
    "\n",
    "    return pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Create few-shot sets\n",
    "few_shot_8 = sample_few_shot(dataset, n_per_class=4)    # 8 total (4 per class)\n",
    "few_shot_16 = sample_few_shot(dataset, n_per_class=8)   # 16 total\n",
    "few_shot_32 = sample_few_shot(dataset, n_per_class=16)  # 32 total\n",
    "\n",
    "print(f\"8-shot:  {len(few_shot_8)} examples\")\n",
    "print(f\"16-shot: {len(few_shot_16)} examples\")\n",
    "print(f\"32-shot: {len(few_shot_32)} examples\")\n",
    "\n",
    "# Preview the 8-shot set\n",
    "print(\"\\n--- 8-shot training set ---\")\n",
    "for i, row in few_shot_8.iterrows():\n",
    "    label_name = \"CLAIM\" if row['label'] == 1 else \"NO CLAIM\"\n",
    "    print(f\"  [{label_name}] {row['text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training SetFit with 8 Examples\n",
    "\n",
    "Let us train a SetFit model using only our 8-shot training set. We use `paraphrase-mpnet-base-v2` as the base sentence transformer -- it is a strong general-purpose model that maps sentences to 768-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to HF dataset format\n",
    "train_ds = Dataset.from_pandas(few_shot_8[['text', 'label']])\n",
    "test_ds = dataset['test']\n",
    "\n",
    "# Load pre-trained sentence transformer\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    metric=\"accuracy\",\n",
    "    num_iterations=20,   # Number of text pairs to generate per example\n",
    "    num_epochs=1,        # Epochs for contrastive fine-tuning\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(\"Training SetFit on 8 examples...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"\\n8-shot SetFit Accuracy: {metrics['accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = dataset['test'].to_pandas()\n",
    "predictions = model.predict(test_df['text'].tolist())\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SetFit (8-shot) -- Full Evaluation on Test Set\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(test_df['label'], predictions,\n",
    "                            target_names=['No claim', 'Environmental claim']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Label Efficiency: How Many Examples Do We Need?\n",
    "\n",
    "A key question in practical NLP: **how many labeled examples are enough?**\n",
    "\n",
    "We will run experiments with 4, 8, 16, and 32 examples per class and compare:\n",
    "- **SetFit** (contrastive fine-tuning + logistic head)\n",
    "- **TF-IDF + Logistic Regression** (a strong traditional baseline)\n",
    "\n",
    "This gives us a **label efficiency curve** -- one of the most useful plots for deciding how much annotation effort to invest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_and_evaluate_setfit(few_shot_df, test_dataset, n_per_class):\n    \"\"\"Train a SetFit model on few-shot data and return accuracy.\"\"\"\n    train_ds = Dataset.from_pandas(few_shot_df[['text', 'label']])\n    model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n    trainer = SetFitTrainer(\n        model=model,\n        train_dataset=train_ds,\n        eval_dataset=test_dataset,\n        metric=\"accuracy\",\n        num_iterations=20,\n        num_epochs=1,\n    )\n    trainer.train()\n    return trainer.evaluate()['accuracy']\n\ndef train_and_evaluate_tfidf(few_shot_df, test_texts, test_labels):\n    \"\"\"Train a TF-IDF + LR baseline and return accuracy.\"\"\"\n    tfidf_pipe = Pipeline([\n        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n        ('clf', LogisticRegression(max_iter=1000))\n    ])\n    tfidf_pipe.fit(few_shot_df['text'], few_shot_df['label'])\n    return tfidf_pipe.score(test_texts, test_labels)"
  },
  {
   "cell_type": "code",
   "source": "results = []\n\nfor n_per_class in [4, 8, 16, 32]:\n    print(f\"\\n{'='*50}\")\n    print(f\"Training with {n_per_class} examples per class ({n_per_class*2} total)\")\n    print(f\"{'='*50}\")\n\n    few_shot = sample_few_shot(dataset, n_per_class=n_per_class)\n\n    setfit_acc = train_and_evaluate_setfit(few_shot, dataset['test'], n_per_class)\n    tfidf_acc = train_and_evaluate_tfidf(few_shot, test_df['text'], test_df['label'])\n\n    results.append({\n        'n_examples': n_per_class * 2,\n        'setfit_accuracy': setfit_acc,\n        'tfidf_accuracy': tfidf_acc\n    })\n    print(f\"{n_per_class*2} examples -- SetFit: {setfit_acc:.1%}, TF-IDF: {tfidf_acc:.1%}\")\n\nresults_df = pd.DataFrame(results)\nprint(\"\\n\" + \"=\"*50)\nprint(\"Summary\")\nprint(\"=\"*50)\nprint(results_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(results_df['n_examples'], results_df['setfit_accuracy'],\n",
    "        'o-', label='SetFit', color='#E07850', linewidth=2, markersize=8)\n",
    "ax.plot(results_df['n_examples'], results_df['tfidf_accuracy'],\n",
    "        's--', label='TF-IDF + LR', color='#6B5D55', linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Number of training examples', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Label Efficiency: SetFit vs TF-IDF', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0.4, 1.0)\n",
    "ax.set_xticks(results_df['n_examples'])\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bonus: LLM-Bootstrapped Training Pairs\n",
    "\n",
    "One powerful idea: **use an LLM to augment your few-shot training set**.\n",
    "\n",
    "The recipe:\n",
    "1. Take each of your 8 labeled examples.\n",
    "2. Ask an LLM to **paraphrase** each one, preserving the meaning (and therefore the label).\n",
    "3. Train SetFit on the augmented set (now 16 examples instead of 8).\n",
    "\n",
    "This is a form of **data augmentation** that goes beyond simple synonym replacement -- the LLM can restructure sentences, change phrasing, and introduce natural variation.\n",
    "\n",
    "Why does this help SetFit specifically?\n",
    "- More examples means **more contrastive pairs** can be generated.\n",
    "- Paraphrases create diverse positive pairs that teach the model what variation within a class looks like.\n",
    "\n",
    "> **Note:** This cell requires a Groq API key. If you do not have one, you can skip it -- the rest of the notebook works without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n\n# If not set above, try Colab secrets → then environment variable\nif not GROQ_API_KEY:\n    try:\n        from google.colab import userdata\n        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n    except (ImportError, Exception):\n        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n\nif GROQ_API_KEY:\n    client = OpenAI(api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n\n    def generate_paraphrase(text, label_name):\n        \"\"\"Generate a paraphrase of an example for data augmentation.\"\"\"\n        response = client.chat.completions.create(\n            model=\"llama-3.1-8b-instant\",\n            messages=[{\n                \"role\": \"user\",\n                \"content\": (\n                    f\"Paraphrase this {label_name} text. \"\n                    f\"Keep the same meaning but change the wording. \"\n                    f\"Return ONLY the paraphrase.\\n\\nOriginal: {text}\"\n                )\n            }],\n            temperature=0.7,\n            max_tokens=200\n        )\n        return response.choices[0].message.content.strip()\n\n    print(\"API connected. generate_paraphrase() ready.\")\nelse:\n    print(\"Set GROQ_API_KEY to try LLM bootstrapping.\")\n    print(\"You can get a free key at https://console.groq.com/\")"
  },
  {
   "cell_type": "code",
   "source": "if GROQ_API_KEY:\n    augmented = []\n    label_names = {0: \"non-environmental-claim\", 1: \"environmental claim\"}\n\n    print(\"Generating LLM paraphrases...\")\n    for _, row in tqdm(few_shot_8.iterrows(), total=len(few_shot_8)):\n        # Keep original\n        augmented.append({'text': row['text'], 'label': row['label']})\n        # Generate paraphrase\n        para = generate_paraphrase(row['text'], label_names[row['label']])\n        augmented.append({'text': para, 'label': row['label']})\n        print(f\"  Original:    {row['text'][:70]}...\")\n        print(f\"  Paraphrase:  {para[:70]}...\")\n        print()\n\n    augmented_df = pd.DataFrame(augmented)\n    print(f\"\\nAugmented from {len(few_shot_8)} to {len(augmented_df)} examples\")\n    print(f\"Label distribution: {augmented_df['label'].value_counts().to_dict()}\")\nelse:\n    print(\"Skipping -- no GROQ_API_KEY set.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "if GROQ_API_KEY:\n    print(\"Training SetFit on augmented data...\")\n    aug_train_ds = Dataset.from_pandas(augmented_df[['text', 'label']])\n    aug_model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n    aug_trainer = SetFitTrainer(\n        model=aug_model,\n        train_dataset=aug_train_ds,\n        eval_dataset=dataset['test'],\n        metric=\"accuracy\",\n        num_iterations=20,\n        num_epochs=1,\n    )\n    aug_trainer.train()\n    aug_metrics = aug_trainer.evaluate()\n\n    print(f\"\\n8-shot SetFit (no augmentation): {metrics['accuracy']:.1%}\")\n    print(f\"8-shot SetFit + LLM augmentation: {aug_metrics['accuracy']:.1%}\")\nelse:\n    print(\"Skipping -- no GROQ_API_KEY set.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise\n",
    "\n",
    "Try one (or both) of the following:\n",
    "\n",
    "### Option A: Different Base Model\n",
    "Replace `paraphrase-mpnet-base-v2` with a different sentence transformer and compare results. Suggested alternatives:\n",
    "- `BAAI/bge-small-en-v1.5` (smaller, faster)\n",
    "- `sentence-transformers/all-MiniLM-L6-v2` (good balance of speed and quality)\n",
    "- `BAAI/bge-base-en-v1.5` (strong retrieval model)\n",
    "\n",
    "### Option B: LLM Bootstrapping\n",
    "If you have a Groq API key, try generating **2 paraphrases** per example instead of 1. Does tripling the training set help more than doubling it?\n",
    "\n",
    "### Option C: Number of Iterations\n",
    "The `num_iterations` parameter controls how many contrastive pairs are generated. Try values of 5, 10, 20, and 40. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# -------------------------------------------------------\n",
    "# Option A: Try a different base model\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# model_name = \"BAAI/bge-small-en-v1.5\"  # <-- change this\n",
    "#\n",
    "# train_ds = Dataset.from_pandas(few_shot_8[['text', 'label']])\n",
    "# model_alt = SetFitModel.from_pretrained(model_name)\n",
    "# trainer_alt = SetFitTrainer(\n",
    "#     model=model_alt,\n",
    "#     train_dataset=train_ds,\n",
    "#     eval_dataset=dataset['test'],\n",
    "#     metric=\"accuracy\",\n",
    "#     num_iterations=20,\n",
    "#     num_epochs=1,\n",
    "# )\n",
    "# trainer_alt.train()\n",
    "# alt_metrics = trainer_alt.evaluate()\n",
    "# print(f\"8-shot with {model_name}: {alt_metrics['accuracy']:.1%}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Option C: Try different num_iterations values\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# iter_results = []\n",
    "# for n_iter in [5, 10, 20, 40]:\n",
    "#     model_iter = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "#     trainer_iter = SetFitTrainer(\n",
    "#         model=model_iter,\n",
    "#         train_dataset=train_ds,\n",
    "#         eval_dataset=dataset['test'],\n",
    "#         metric=\"accuracy\",\n",
    "#         num_iterations=n_iter,\n",
    "#         num_epochs=1,\n",
    "#     )\n",
    "#     trainer_iter.train()\n",
    "#     acc = trainer_iter.evaluate()['accuracy']\n",
    "#     iter_results.append({'num_iterations': n_iter, 'accuracy': acc})\n",
    "#     print(f\"num_iterations={n_iter}: {acc:.1%}\")\n",
    "#\n",
    "# print(pd.DataFrame(iter_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **SetFit** | A two-phase framework (contrastive fine-tuning + logistic head) that achieves strong results with 8--32 examples. |\n",
    "| **Contrastive learning** | By generating positive and negative sentence pairs, SetFit multiplies the training signal from each labeled example. |\n",
    "| **Label efficiency** | SetFit dramatically outperforms TF-IDF baselines in the low-data regime (< 64 examples). |\n",
    "| **LLM bootstrapping** | Using an LLM to paraphrase examples creates diverse contrastive pairs, further improving few-shot performance. |\n",
    "\n",
    "### When to Use SetFit vs Full Fine-tuning\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|---------------|\n",
    "| < 50 labeled examples | **SetFit** -- it is designed for this regime. |\n",
    "| 50--500 labeled examples | SetFit or parameter-efficient fine-tuning (LoRA). Compare both. |\n",
    "| 500+ labeled examples | Full fine-tuning of a transformer may yield the best results. |\n",
    "| No labeled data at all | Zero-shot LLM prompting, then iterate with SetFit as labels come in. |\n",
    "\n",
    "### Practical Tips\n",
    "\n",
    "1. **Choose diverse examples.** When selecting your few-shot set, pick examples that cover the variety within each class.\n",
    "2. **num_iterations matters.** More iterations = more contrastive pairs = better embeddings, but with diminishing returns beyond ~20.\n",
    "3. **The base model matters.** Larger sentence transformers tend to perform better, but smaller ones (MiniLM, bge-small) can be surprisingly competitive.\n",
    "4. **LLM augmentation is cheap insurance.** Even a few paraphrases per example can meaningfully improve results.\n",
    "\n",
    "---\n",
    "\n",
    "*Next notebook: We will explore retrieval-augmented generation (RAG), combining embeddings with LLMs for question answering over documents.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}