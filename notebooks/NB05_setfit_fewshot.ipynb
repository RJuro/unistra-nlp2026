{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB05: SetFit Few-shot Classification\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB05_setfit_fewshot.ipynb)\n",
    "\n",
    "**Duration:** 80 minutes\n",
    "\n",
    "> **GPU recommended** — go to **Runtime → Change runtime type → T4 GPU**. SetFit fine-tunes a sentence transformer via contrastive learning; GPU matters once we run multi-seed and augmentation comparisons.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Train robust few-shot classifiers (8--64 labels)** with SetFit and report both accuracy and macro-F1.\n",
    "2. **Diagnose instability in tiny-data settings** using multi-seed evaluation rather than one lucky/unlucky split.\n",
    "3. **Improve LLM bootstrapping quality** with higher-contrast prompts, deduplication, and 2x vs 3x augmentation checks.\n",
    "4. **Test model sensitivity** (base sentence encoder choice) and explain when augmentation helps or hurts.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setfit \"transformers>=4.40,<5\" openai pandas scikit-learn tqdm -q\n",
    "\n",
    "# Colab pre-installs transformers v5, which removed a function SetFit depends on.\n",
    "# After the install above downgrades transformers, we must restart the runtime.\n",
    "import importlib, sys\n",
    "if \"setfit\" in sys.modules:\n",
    "    # Already imported in a previous run — need a full restart\n",
    "    print(\"⚠️  Please restart the runtime: Runtime → Restart runtime, then re-run all cells.\")\n",
    "else:\n",
    "    # First run — try importing to verify\n",
    "    try:\n",
    "        import setfit\n",
    "        print(f\"setfit {setfit.__version__} loaded successfully.\")\n",
    "    except ImportError:\n",
    "        import os\n",
    "        os.kill(os.getpid(), 9)  # auto-restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import set_seed\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All imports successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── GPU Check ─────────────────────────────────────────────────────────────\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected — running on CPU.\")\n",
    "    print(\"SetFit training will be slower but still works (~2-5 min per experiment).\")\n",
    "    print(\"To enable GPU: Runtime → Change runtime type → T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Task: Detecting Environmental Claims\n",
    "\n",
    "Companies make environmental statements in reports, press releases, and ESG disclosures. Some are concrete and verifiable; others are vague or promotional.\n",
    "\n",
    "**Can we detect environmental claims with only a handful of labels?**\n",
    "\n",
    "We use [climatebert/environmental_claims](https://huggingface.co/datasets/climatebert/environmental_claims):\n",
    "\n",
    "| Label | Meaning |\n",
    "|-------|---------|\n",
    "| **0** | Not an environmental claim |\n",
    "| **1** | Environmental claim |\n",
    "\n",
    "Why this is a strong teaching case:\n",
    "- The boundary is subtle: business text can mention sustainability terms without making a claim.\n",
    "- In realistic workflows, annotation budgets are tiny at first (8--32 labels).\n",
    "- We need methods that are strong under low-data uncertainty, not just on large static benchmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"climatebert/environmental_claims\")\n",
    "print(dataset)\n",
    "print(f\"\\nTrain size: {len(dataset['train'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")\n",
    "\n",
    "# Look at examples\n",
    "train_df = dataset['train'].to_pandas()\n",
    "print(f\"\\nLabel distribution:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"\\nExample claim:\")\n",
    "print(train_df[train_df.label == 1].iloc[0]['text'][:200])\n",
    "print(f\"\\nExample non-claim:\")\n",
    "print(train_df[train_df.label == 0].iloc[0]['text'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Few-shot Challenge\n",
    "\n",
    "With only **8 labeled examples**, classical approaches are brittle:\n",
    "\n",
    "- **TF-IDF + Logistic Regression** underfits semantic variation.\n",
    "- **Full BERT fine-tuning** overfits quickly with so few labels.\n",
    "- **Single-run metrics** are misleading because seed variance is high in tiny-data regimes.\n",
    "\n",
    "SetFit addresses this by contrastive training on sentence pairs, but three failure modes remain important:\n",
    "\n",
    "1. **Dataset boundary ambiguity** (claims vs related non-claims can look similar)\n",
    "2. **Base encoder mismatch** (some sentence models separate this boundary better)\n",
    "3. **Weak augmentation prompts** (paraphrases too similar or not label-faithful)\n",
    "\n",
    "This notebook explicitly measures all three.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SetFit: How It Works\n",
    "\n",
    "SetFit (**S**entence-**T**ransformer **F**ine-**T**uning) uses two phases:\n",
    "\n",
    "### Phase 1: Contrastive Fine-tuning\n",
    "\n",
    "From few labeled examples, SetFit builds positive/negative sentence pairs:\n",
    "- Positive = same label\n",
    "- Negative = different labels\n",
    "\n",
    "This multiplies training signal from tiny datasets.\n",
    "\n",
    "### Phase 2: Lightweight Classifier Head\n",
    "\n",
    "After contrastive tuning, SetFit trains a simple classifier (default: logistic regression) on embeddings.\n",
    "\n",
    "### 2026 practical detail (for IntFloat E5 models)\n",
    "\n",
    "We use `intfloat/e5-small` by default. E5 models are retrieval-oriented and expect prefixes.\n",
    "For this notebook, we treat each text as a query-style input and prepend:\n",
    "\n",
    "- `query: ...`\n",
    "\n",
    "This keeps model usage consistent during both training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_few_shot(dataset, n_per_class, seed=42):\n",
    "    # Sample n examples per class for few-shot training.\n",
    "    train_data = dataset[\"train\"].to_pandas()\n",
    "    parts = []\n",
    "    for label in sorted(train_data[\"label\"].unique()):\n",
    "        class_data = train_data[train_data[\"label\"] == label]\n",
    "        parts.append(class_data.sample(n=min(n_per_class, len(class_data)), random_state=seed))\n",
    "    return pd.concat(parts).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "\n",
    "SEEDS = [13, 42, 77]\n",
    "\n",
    "# Canonical few-shot sets (used in later sections)\n",
    "few_shot_8 = sample_few_shot(dataset, n_per_class=4, seed=42)    # 8 total\n",
    "few_shot_16 = sample_few_shot(dataset, n_per_class=8, seed=42)   # 16 total\n",
    "few_shot_32 = sample_few_shot(dataset, n_per_class=16, seed=42)  # 32 total\n",
    "\n",
    "print(f\"8-shot:  {len(few_shot_8)} examples\")\n",
    "print(f\"16-shot: {len(few_shot_16)} examples\")\n",
    "print(f\"32-shot: {len(few_shot_32)} examples\")\n",
    "\n",
    "print(\"\\n--- 8-shot training set (seed=42) ---\")\n",
    "for _, row in few_shot_8.iterrows():\n",
    "    label_name = \"CLAIM\" if row[\"label\"] == 1 else \"NO CLAIM\"\n",
    "    print(f\"[{label_name}] {row['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training SetFit with 8 Examples\n",
    "\n",
    "Default model: `intfloat/e5-small`.\n",
    "\n",
    "Why use it here:\n",
    "- compact and fast\n",
    "- strong semantic encoder for low-data setups\n",
    "- aligned with the retrieval stack we use in NB06\n",
    "\n",
    "Important detail: because this is E5, we add `query:` prefixes to all SetFit inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "BASE_MODEL = \"intfloat/e5-small\"\n",
    "\n",
    "\n",
    "def format_texts_for_model(texts, model_name):\n",
    "    texts = [str(t) for t in texts]\n",
    "    if \"intfloat/e5\" in model_name.lower():\n",
    "        return [f\"query: {t.strip()}\" for t in texts]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def preprocess_df_for_model(df, model_name):\n",
    "    out = df.copy()\n",
    "    out[\"text\"] = format_texts_for_model(out[\"text\"].tolist(), model_name)\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_setfit(\n",
    "    few_shot_df,\n",
    "    model_name=BASE_MODEL,\n",
    "    seed=42,\n",
    "    num_iterations=20,\n",
    "    num_epochs=4,\n",
    "    sampling_strategy=\"oversampling\",\n",
    "):\n",
    "    # Train SetFit and return (model, trainer).\n",
    "    set_seed(seed)\n",
    "\n",
    "    train_df_model = preprocess_df_for_model(few_shot_df[[\"text\", \"label\"]], model_name)\n",
    "    train_ds = Dataset.from_pandas(train_df_model[[\"text\", \"label\"]], preserve_index=False)\n",
    "    model = SetFitModel.from_pretrained(model_name)\n",
    "\n",
    "    arg_kwargs = {\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"batch_size\": 16,\n",
    "        \"sampling_strategy\": sampling_strategy,\n",
    "    }\n",
    "    if num_iterations is not None:\n",
    "        arg_kwargs[\"num_iterations\"] = num_iterations\n",
    "\n",
    "    args = TrainingArguments(**arg_kwargs)\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_ds)\n",
    "    trainer.train()\n",
    "    return model, trainer\n",
    "\n",
    "\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "test_texts_model = format_texts_for_model(test_df[\"text\"].tolist(), BASE_MODEL)\n",
    "\n",
    "print(f\"Training SetFit on 8 examples with {BASE_MODEL}...\")\n",
    "model, trainer = train_setfit(\n",
    "    few_shot_8,\n",
    "    model_name=BASE_MODEL,\n",
    "    seed=42,\n",
    "    num_iterations=20,\n",
    "    num_epochs=4,\n",
    "    sampling_strategy=\"oversampling\",\n",
    ")\n",
    "\n",
    "predictions = model.predict(test_texts_model)\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(test_df[\"label\"], predictions),\n",
    "    \"macro_f1\": f1_score(test_df[\"label\"], predictions, average=\"macro\"),\n",
    "}\n",
    "\n",
    "print(f\"\\n8-shot Accuracy: {metrics['accuracy']:.1%}\")\n",
    "print(f\"8-shot Macro-F1: {metrics['macro_f1']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SetFit (8-shot) -- Full Evaluation on Test Set\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    test_df[\"label\"],\n",
    "    predictions,\n",
    "    target_names=[\"No claim\", \"Environmental claim\"],\n",
    "    digits=3,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Label Efficiency: How Many Examples Do We Need?\n",
    "\n",
    "A practical question: how many labels should we buy from annotators?\n",
    "\n",
    "We run experiments with 4, 8, 16, and 32 examples per class and compare:\n",
    "- **SetFit**\n",
    "- **TF-IDF + Logistic Regression**\n",
    "\n",
    "Important upgrade vs the older version: we run **multiple random seeds** and report mean ± std, so conclusions are not based on a single lucky split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_tfidf(few_shot_df, test_texts, test_labels):\n",
    "    # TF-IDF + Logistic Regression baseline.\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000, stop_words=\"english\", ngram_range=(1, 2))),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\")),\n",
    "    ])\n",
    "    pipe.fit(few_shot_df[\"text\"], few_shot_df[\"label\"])\n",
    "    preds = pipe.predict(test_texts)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(test_labels, preds),\n",
    "        \"macro_f1\": f1_score(test_labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_one_setting(n_per_class, seed, model_name=BASE_MODEL):\n",
    "    # Single run for one data size + seed.\n",
    "    few_shot = sample_few_shot(dataset, n_per_class=n_per_class, seed=seed)\n",
    "\n",
    "    setfit_model, _ = train_setfit(\n",
    "        few_shot,\n",
    "        model_name=model_name,\n",
    "        seed=seed,\n",
    "        num_iterations=20,\n",
    "        num_epochs=4,\n",
    "        sampling_strategy=\"oversampling\",\n",
    "    )\n",
    "    test_texts_model = format_texts_for_model(test_df[\"text\"].tolist(), model_name)\n",
    "    setfit_preds = setfit_model.predict(test_texts_model)\n",
    "    setfit_scores = {\n",
    "        \"accuracy\": accuracy_score(test_df[\"label\"], setfit_preds),\n",
    "        \"macro_f1\": f1_score(test_df[\"label\"], setfit_preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    tfidf_scores = train_and_evaluate_tfidf(\n",
    "        few_shot,\n",
    "        test_df[\"text\"],\n",
    "        test_df[\"label\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"n_examples\": n_per_class * 2,\n",
    "        \"seed\": seed,\n",
    "        \"setfit_accuracy\": setfit_scores[\"accuracy\"],\n",
    "        \"setfit_macro_f1\": setfit_scores[\"macro_f1\"],\n",
    "        \"tfidf_accuracy\": tfidf_scores[\"accuracy\"],\n",
    "        \"tfidf_macro_f1\": tfidf_scores[\"macro_f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PER_CLASS_GRID = [4, 8, 16, 32]\n",
    "records = []\n",
    "\n",
    "for n_per_class in N_PER_CLASS_GRID:\n",
    "    print(f\"\\n{'='*64}\")\n",
    "    print(f\"{n_per_class*2} examples total ({n_per_class}/class)\")\n",
    "    print(f\"{'='*64}\")\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        rec = evaluate_one_setting(n_per_class=n_per_class, seed=seed, model_name=BASE_MODEL)\n",
    "        records.append(rec)\n",
    "        print(\n",
    "            f\"seed={seed} | SetFit acc={rec['setfit_accuracy']:.3f}, f1={rec['setfit_macro_f1']:.3f} | \"\n",
    "            f\"TF-IDF acc={rec['tfidf_accuracy']:.3f}, f1={rec['tfidf_macro_f1']:.3f}\"\n",
    "        )\n",
    "\n",
    "detailed_results_df = pd.DataFrame(records)\n",
    "results_df = (\n",
    "    detailed_results_df\n",
    "    .groupby(\"n_examples\", as_index=False)\n",
    "    .agg(\n",
    "        setfit_accuracy_mean=(\"setfit_accuracy\", \"mean\"),\n",
    "        setfit_accuracy_std=(\"setfit_accuracy\", \"std\"),\n",
    "        tfidf_accuracy_mean=(\"tfidf_accuracy\", \"mean\"),\n",
    "        tfidf_accuracy_std=(\"tfidf_accuracy\", \"std\"),\n",
    "        setfit_f1_mean=(\"setfit_macro_f1\", \"mean\"),\n",
    "        setfit_f1_std=(\"setfit_macro_f1\", \"std\"),\n",
    "        tfidf_f1_mean=(\"tfidf_macro_f1\", \"mean\"),\n",
    "        tfidf_f1_std=(\"tfidf_macro_f1\", \"std\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*64)\n",
    "print(\"Mean ± std over seeds\")\n",
    "print(\"=\"*64)\n",
    "print(results_df.round(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), sharex=True)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].errorbar(\n",
    "    results_df[\"n_examples\"],\n",
    "    results_df[\"setfit_accuracy_mean\"],\n",
    "    yerr=results_df[\"setfit_accuracy_std\"],\n",
    "    fmt=\"o-\",\n",
    "    label=\"SetFit\",\n",
    "    color=\"#E07850\",\n",
    "    linewidth=2,\n",
    "    capsize=4,\n",
    ")\n",
    "axes[0].errorbar(\n",
    "    results_df[\"n_examples\"],\n",
    "    results_df[\"tfidf_accuracy_mean\"],\n",
    "    yerr=results_df[\"tfidf_accuracy_std\"],\n",
    "    fmt=\"s--\",\n",
    "    label=\"TF-IDF + LR\",\n",
    "    color=\"#6B5D55\",\n",
    "    linewidth=2,\n",
    "    capsize=4,\n",
    ")\n",
    "axes[0].set_title(\"Accuracy vs Label Budget\")\n",
    "axes[0].set_xlabel(\"Number of training examples\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Macro-F1\n",
    "axes[1].errorbar(\n",
    "    results_df[\"n_examples\"],\n",
    "    results_df[\"setfit_f1_mean\"],\n",
    "    yerr=results_df[\"setfit_f1_std\"],\n",
    "    fmt=\"o-\",\n",
    "    label=\"SetFit\",\n",
    "    color=\"#E07850\",\n",
    "    linewidth=2,\n",
    "    capsize=4,\n",
    ")\n",
    "axes[1].errorbar(\n",
    "    results_df[\"n_examples\"],\n",
    "    results_df[\"tfidf_f1_mean\"],\n",
    "    yerr=results_df[\"tfidf_f1_std\"],\n",
    "    fmt=\"s--\",\n",
    "    label=\"TF-IDF + LR\",\n",
    "    color=\"#6B5D55\",\n",
    "    linewidth=2,\n",
    "    capsize=4,\n",
    ")\n",
    "axes[1].set_title(\"Macro-F1 vs Label Budget\")\n",
    "axes[1].set_xlabel(\"Number of training examples\")\n",
    "axes[1].set_ylabel(\"Macro-F1\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "fig.suptitle(\"Label Efficiency with Seed Variance\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLM Bootstrapping: Why Results Can Be Bad (and How to Fix It)\n",
    "\n",
    "Naive paraphrase augmentation often disappoints because:\n",
    "1. Paraphrases are too close to originals (low diversity)\n",
    "2. Prompts do not enforce label faithfulness\n",
    "3. Added data does not increase contrastive signal\n",
    "\n",
    "We upgrade the pipeline in three ways:\n",
    "\n",
    "- **Higher-contrast prompt** with strict label definitions\n",
    "- **Quality filter** (deduplicate + remove trivial rewrites)\n",
    "- **2x vs 3x comparison** (1 vs 2 paraphrases per seed example)\n",
    "\n",
    "We also use `sampling_strategy=\"unique\"` for augmented runs to exploit diverse pairs.\n",
    "\n",
    "> **Note:** This section requires a Groq API key. If absent, it is safely skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# If not set above, try Colab secrets -> then environment variable\n",
    "if not GROQ_API_KEY:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\")\n",
    "    except (ImportError, Exception):\n",
    "        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
    "\n",
    "AUGMENT_MODEL = \"openai/gpt-oss-20b\"\n",
    "PARAPHRASES_PER_EXAMPLE = 2  # 2 paraphrases + original = 3x dataset\n",
    "\n",
    "LABEL_GUIDE = {\n",
    "    0: \"No environmental claim. It may mention business activity or sustainability context, but does not assert an environmental action/outcome.\",\n",
    "    1: \"Environmental claim. It explicitly states environmental action, impact, commitment, or performance.\",\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_list_output(raw_text):\n",
    "    raw = raw_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "        if isinstance(data, list):\n",
    "            return [str(x).strip() for x in data if str(x).strip()]\n",
    "    except Exception:\n",
    "        pass\n",
    "    lines = [line.strip(\"-• \").strip() for line in raw.splitlines() if line.strip()]\n",
    "    return [line for line in lines if line]\n",
    "\n",
    "\n",
    "def generate_paraphrases(text, label, n_paraphrases=PARAPHRASES_PER_EXAMPLE):\n",
    "    # Generate label-faithful, high-diversity paraphrases.\n",
    "    if not GROQ_API_KEY:\n",
    "        return []\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are creating synthetic training data for binary classification.\\\\n\\\\n\"\n",
    "        f\"Label definition:\\\\n{LABEL_GUIDE[label]}\\\\n\\\\n\"\n",
    "        f\"Task:\\\\n\"\n",
    "        f\"- Generate exactly {n_paraphrases} paraphrases of the input text.\\\\n\"\n",
    "        f\"- Keep the same label.\\\\n\"\n",
    "        f\"- Use substantially different wording/syntax from the original.\\\\n\"\n",
    "        f\"- Keep each paraphrase to 1 sentence, <= 35 words.\\\\n\"\n",
    "        f\"- Make at least one paraphrase close to the class boundary but still label-correct.\\\\n\\\\n\"\n",
    "        f\"Return ONLY a JSON array of strings.\\\\n\\\\n\"\n",
    "        f\"Original text: {text}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=AUGMENT_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.9,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return _parse_list_output(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    client = OpenAI(api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n",
    "    print(f\"API connected. Using {AUGMENT_MODEL}\")\n",
    "else:\n",
    "    print(\"Set GROQ_API_KEY to run bootstrapping.\")\n",
    "    print(\"You can get a free key at https://console.groq.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    return \" \".join(str(text).lower().split())\n",
    "\n",
    "\n",
    "def dedupe_augmented(df):\n",
    "    tmp = df.copy()\n",
    "    tmp[\"text_norm\"] = tmp[\"text\"].apply(normalize_text)\n",
    "    tmp = tmp.drop_duplicates(subset=[\"text_norm\", \"label\"]).drop(columns=[\"text_norm\"])\n",
    "    return tmp.reset_index(drop=True)\n",
    "\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    rows_2x, rows_3x = [], []\n",
    "\n",
    "    print(\"Generating LLM paraphrases (high-contrast prompt)...\")\n",
    "    for _, row in tqdm(few_shot_8.iterrows(), total=len(few_shot_8)):\n",
    "        original = row[\"text\"]\n",
    "        label = int(row[\"label\"])\n",
    "\n",
    "        rows_2x.append({\"text\": original, \"label\": label})\n",
    "        rows_3x.append({\"text\": original, \"label\": label})\n",
    "\n",
    "        paras = generate_paraphrases(original, label=label, n_paraphrases=PARAPHRASES_PER_EXAMPLE)\n",
    "\n",
    "        cleaned = []\n",
    "        seen = set()\n",
    "        orig_norm = normalize_text(original)\n",
    "        for p in paras:\n",
    "            p_norm = normalize_text(p)\n",
    "            if len(p_norm.split()) < 6:\n",
    "                continue\n",
    "            if p_norm == orig_norm or p_norm in seen:\n",
    "                continue\n",
    "            seen.add(p_norm)\n",
    "            cleaned.append(p.strip())\n",
    "\n",
    "        if cleaned:\n",
    "            rows_2x.append({\"text\": cleaned[0], \"label\": label})\n",
    "        for p in cleaned[:2]:\n",
    "            rows_3x.append({\"text\": p, \"label\": label})\n",
    "\n",
    "    augmented_2x_df = dedupe_augmented(pd.DataFrame(rows_2x))\n",
    "    augmented_3x_df = dedupe_augmented(pd.DataFrame(rows_3x))\n",
    "\n",
    "    print(f\"\\nOriginal size: {len(few_shot_8)}\")\n",
    "    print(f\"2x target size: {len(augmented_2x_df)}\")\n",
    "    print(f\"3x target size: {len(augmented_3x_df)}\")\n",
    "    print(f\"2x label distribution: {augmented_2x_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"3x label distribution: {augmented_3x_df['label'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print(\"Skipping -- no GROQ_API_KEY set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training_df(\n",
    "    train_df,\n",
    "    tag,\n",
    "    model_name=BASE_MODEL,\n",
    "    seeds=(13, 42, 77),\n",
    "    num_iterations=20,\n",
    "    sampling_strategy=\"oversampling\",\n",
    "):\n",
    "    rows = []\n",
    "    for seed in seeds:\n",
    "        model_i, _ = train_setfit(\n",
    "            train_df,\n",
    "            model_name=model_name,\n",
    "            seed=seed,\n",
    "            num_iterations=num_iterations,\n",
    "            num_epochs=4,\n",
    "            sampling_strategy=sampling_strategy,\n",
    "        )\n",
    "        test_texts_model = format_texts_for_model(test_df[\"text\"].tolist(), model_name)\n",
    "        preds_i = model_i.predict(test_texts_model)\n",
    "        rows.append({\n",
    "            \"seed\": seed,\n",
    "            \"accuracy\": accuracy_score(test_df[\"label\"], preds_i),\n",
    "            \"macro_f1\": f1_score(test_df[\"label\"], preds_i, average=\"macro\"),\n",
    "        })\n",
    "\n",
    "    score_df = pd.DataFrame(rows)\n",
    "    return {\n",
    "        \"setting\": tag,\n",
    "        \"model\": model_name,\n",
    "        \"accuracy_mean\": score_df[\"accuracy\"].mean(),\n",
    "        \"accuracy_std\": score_df[\"accuracy\"].std(ddof=0),\n",
    "        \"macro_f1_mean\": score_df[\"macro_f1\"].mean(),\n",
    "        \"macro_f1_std\": score_df[\"macro_f1\"].std(ddof=0),\n",
    "    }\n",
    "\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    comparison_rows = []\n",
    "\n",
    "    comparison_rows.append(evaluate_training_df(\n",
    "        few_shot_8,\n",
    "        tag=\"8-shot baseline\",\n",
    "        model_name=BASE_MODEL,\n",
    "        seeds=SEEDS,\n",
    "        num_iterations=20,\n",
    "        sampling_strategy=\"oversampling\",\n",
    "    ))\n",
    "\n",
    "    comparison_rows.append(evaluate_training_df(\n",
    "        augmented_2x_df,\n",
    "        tag=\"LLM 2x (1 paraphrase/example)\",\n",
    "        model_name=BASE_MODEL,\n",
    "        seeds=SEEDS,\n",
    "        num_iterations=None,\n",
    "        sampling_strategy=\"unique\",\n",
    "    ))\n",
    "\n",
    "    comparison_rows.append(evaluate_training_df(\n",
    "        augmented_3x_df,\n",
    "        tag=\"LLM 3x (2 paraphrases/example)\",\n",
    "        model_name=BASE_MODEL,\n",
    "        seeds=SEEDS,\n",
    "        num_iterations=None,\n",
    "        sampling_strategy=\"unique\",\n",
    "    ))\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_rows).sort_values(\"macro_f1_mean\", ascending=False)\n",
    "    print(\"Mean +- std over seeds\")\n",
    "    print(comparison_df.round(3).to_string(index=False))\n",
    "\n",
    "    # Quick model sensitivity check on 3x data\n",
    "    print(\"\\nModel sensitivity check on 3x augmented set (seed=42):\")\n",
    "    for model_name in [\n",
    "        BASE_MODEL,\n",
    "        \"BAAI/bge-small-en-v1.5\",\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    ]:\n",
    "        m, _ = train_setfit(\n",
    "            augmented_3x_df,\n",
    "            model_name=model_name,\n",
    "            seed=42,\n",
    "            num_iterations=None,\n",
    "            num_epochs=4,\n",
    "            sampling_strategy=\"unique\",\n",
    "        )\n",
    "        preds = m.predict(format_texts_for_model(test_df[\"text\"].tolist(), model_name))\n",
    "        acc = accuracy_score(test_df[\"label\"], preds)\n",
    "        f1 = f1_score(test_df[\"label\"], preds, average=\"macro\")\n",
    "        print(f\"  {model_name:45s} acc={acc:.3f} macro_f1={f1:.3f}\")\n",
    "else:\n",
    "    print(\"Skipping -- no GROQ_API_KEY set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise\n",
    "\n",
    "Pick one:\n",
    "\n",
    "### Option A: Prompt Contrast Ablation\n",
    "Remove the boundary-focused instruction from the augmentation prompt. Compare 3x results before/after. Did macro-F1 drop?\n",
    "\n",
    "### Option B: 2x vs 3x vs 4x\n",
    "Set `PARAPHRASES_PER_EXAMPLE` to 1, 2, and 3. Plot performance and identify where gains plateau.\n",
    "\n",
    "### Option C: Model Swap Under Fixed Data\n",
    "Keep the same 3x augmented dataset and compare at least 3 encoders (`intfloat/e5-small`, `BAAI/bge-small-en-v1.5`, `all-MiniLM-L6-v2`). Which is most stable across seeds?\n",
    "\n",
    "### Option D: Prefix Ablation for E5\n",
    "If using E5, compare runs with vs without `query:` prefixes. How much performance changes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# -------------------------------------------------------\n",
    "# Option A: Try a different base model\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# model_name = \"BAAI/bge-small-en-v1.5\"  # <-- change this\n",
    "#\n",
    "# train_ds = Dataset.from_pandas(few_shot_8[['text', 'label']])\n",
    "# model_alt = SetFitModel.from_pretrained(model_name)\n",
    "# args_alt = TrainingArguments(num_iterations=20, num_epochs=1)\n",
    "# trainer_alt = Trainer(\n",
    "#     model=model_alt,\n",
    "#     args=args_alt,\n",
    "#     train_dataset=train_ds,\n",
    "# )\n",
    "# trainer_alt.train()\n",
    "# alt_metrics = trainer_alt.evaluate(dataset['test'])\n",
    "# print(f\"8-shot with {model_name}: {alt_metrics['accuracy']:.1%}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Option C: Try different num_iterations values\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# iter_results = []\n",
    "# for n_iter in [5, 10, 20, 40]:\n",
    "#     model_iter = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "#     args_iter = TrainingArguments(num_iterations=n_iter, num_epochs=1)\n",
    "#     trainer_iter = Trainer(\n",
    "#         model=model_iter,\n",
    "#         args=args_iter,\n",
    "#         train_dataset=train_ds,\n",
    "#     )\n",
    "#     trainer_iter.train()\n",
    "#     acc = trainer_iter.evaluate(dataset['test'])['accuracy']\n",
    "#     iter_results.append({'num_iterations': n_iter, 'accuracy': acc})\n",
    "#     print(f\"num_iterations={n_iter}: {acc:.1%}\")\n",
    "#\n",
    "# print(pd.DataFrame(iter_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Takeaways\n",
    "\n",
    "### What we learned\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **SetFit in low-data regimes** | Strong few-shot baseline, but one-run results are noisy; use multi-seed reporting. |\n",
    "| **Metric choice** | Macro-F1 is essential when class performance is asymmetric. |\n",
    "| **LLM bootstrapping** | Gains depend on augmentation quality: label-faithful, diverse rewrites help; weak paraphrases can hurt. |\n",
    "| **2x vs 3x** | Tripling data can help, but returns are task- and model-dependent. Validate empirically. |\n",
    "| **E5 usage detail** | If you use IntFloat E5 models, keep input formatting consistent (`query:` prefixes here). |\n",
    "\n",
    "### Practical recommendations (2026)\n",
    "\n",
    "1. Always report **mean ± std across seeds** for few-shot experiments.\n",
    "2. Keep augmentation prompts **label-anchored and diversity-seeking**.\n",
    "3. For larger augmented sets, test `sampling_strategy=\"unique\"`.\n",
    "4. Compare at least 2--3 encoders early.\n",
    "5. Track both **accuracy and macro-F1**.\n",
    "\n",
    "### Maker tutorials and references\n",
    "\n",
    "- SetFit quickstart: https://huggingface.co/docs/setfit/main/en/quickstart\n",
    "- SetFit training arguments: https://huggingface.co/docs/setfit/main/en/reference/trainer\n",
    "- SetFit synthetic data tutorial: https://huggingface.co/docs/setfit/main/en/tutorials/setfit_synthetic\n",
    "- SetFit distillation tutorial: https://huggingface.co/docs/setfit/main/en/how_to/knowledge_distillation\n",
    "\n",
    "---\n",
    "\n",
    "*Next notebook: retrieval and reranking pipelines where these classifiers can be used for filtering and routing.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
