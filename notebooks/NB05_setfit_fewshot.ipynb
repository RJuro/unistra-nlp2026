{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB05: SetFit Few-shot Classification\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB05_setfit_fewshot.ipynb)\n",
    "\n",
    "**Duration:** 80 minutes\n",
    "\n",
    "> **GPU recommended** — go to **Runtime → Change runtime type → T4 GPU**. SetFit fine-tunes a sentence transformer via contrastive learning; GPU matters once we run multi-seed and augmentation comparisons.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Train robust few-shot classifiers (8--64 labels)** with SetFit and report both accuracy and macro-F1.\n",
    "2. **Diagnose instability in tiny-data settings** using multi-seed evaluation rather than one lucky/unlucky split.\n",
    "3. **Improve LLM bootstrapping quality** with higher-contrast prompts, deduplication, and 2x vs 3x augmentation checks.\n",
    "4. **Test model sensitivity** (base sentence encoder choice) and explain when augmentation helps or hurts.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setfit \"transformers>=4.40,<5\" openai pandas scikit-learn tqdm -q\n",
    "\n",
    "# Colab pre-installs transformers v5, which removed a function SetFit depends on.\n",
    "# After the install above downgrades transformers, we must restart the runtime.\n",
    "import importlib, sys\n",
    "if \"setfit\" in sys.modules:\n",
    "    # Already imported in a previous run — need a full restart\n",
    "    print(\"⚠️  Please restart the runtime: Runtime → Restart runtime, then re-run all cells.\")\n",
    "else:\n",
    "    # First run — try importing to verify\n",
    "    try:\n",
    "        import setfit\n",
    "        print(f\"setfit {setfit.__version__} loaded successfully.\")\n",
    "    except ImportError:\n",
    "        import os\n",
    "        os.kill(os.getpid(), 9)  # auto-restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import set_seed\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All imports successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── GPU Check ─────────────────────────────────────────────────────────────\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected — running on CPU.\")\n",
    "    print(\"SetFit training will be slower but still works (~2-5 min per experiment).\")\n",
    "    print(\"To enable GPU: Runtime → Change runtime type → T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Task: Detecting Environmental Claims\n",
    "\n",
    "Companies make environmental statements in reports, press releases, and ESG disclosures. Some are concrete and verifiable; others are vague or promotional.\n",
    "\n",
    "**Can we detect environmental claims with only a handful of labels?**\n",
    "\n",
    "We use [climatebert/environmental_claims](https://huggingface.co/datasets/climatebert/environmental_claims):\n",
    "\n",
    "| Label | Meaning |\n",
    "|-------|---------|\n",
    "| **0** | Not an environmental claim |\n",
    "| **1** | Environmental claim |\n",
    "\n",
    "Why this is a strong teaching case:\n",
    "- The boundary is subtle: business text can mention sustainability terms without making a claim.\n",
    "- In realistic workflows, annotation budgets are tiny at first (8--32 labels).\n",
    "- We need methods that are strong under low-data uncertainty, not just on large static benchmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"climatebert/environmental_claims\")\n",
    "print(dataset)\n",
    "print(f\"\\nTrain size: {len(dataset['train'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")\n",
    "\n",
    "# Look at examples\n",
    "train_df = dataset['train'].to_pandas()\n",
    "print(f\"\\nLabel distribution:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"\\nExample claim:\")\n",
    "print(train_df[train_df.label == 1].iloc[0]['text'][:200])\n",
    "print(f\"\\nExample non-claim:\")\n",
    "print(train_df[train_df.label == 0].iloc[0]['text'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Few-shot Challenge\n",
    "\n",
    "With only **8 labeled examples**, classical approaches are brittle:\n",
    "\n",
    "- **TF-IDF + Logistic Regression** underfits semantic variation.\n",
    "- **Full BERT fine-tuning** overfits quickly with so few labels.\n",
    "- **Single-run metrics** are misleading because seed variance is high in tiny-data regimes.\n",
    "\n",
    "SetFit addresses this by contrastive training on sentence pairs, but three failure modes remain important:\n",
    "\n",
    "1. **Dataset boundary ambiguity** (claims vs related non-claims can look similar)\n",
    "2. **Base encoder mismatch** (some sentence models separate this boundary better)\n",
    "3. **Weak augmentation prompts** (paraphrases too similar or not label-faithful)\n",
    "\n",
    "This notebook explicitly measures all three.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SetFit: How It Works\n",
    "\n",
    "SetFit (**S**entence-**T**ransformer **F**ine-**T**uning) uses two phases:\n",
    "\n",
    "### Phase 1: Contrastive Fine-tuning\n",
    "\n",
    "From few labeled examples, SetFit builds positive/negative sentence pairs:\n",
    "- Positive = same label\n",
    "- Negative = different labels\n",
    "\n",
    "This multiplies training signal from tiny datasets.\n",
    "\n",
    "### Phase 2: Lightweight Classifier Head\n",
    "\n",
    "After contrastive tuning, SetFit trains a simple classifier (default: logistic regression) on embeddings.\n",
    "\n",
    "### 2026 practical detail (for IntFloat E5 models)\n",
    "\n",
    "We use `intfloat/e5-small` by default. E5 models are retrieval-oriented and expect prefixes.\n",
    "For this notebook, we treat each text as a query-style input and prepend:\n",
    "\n",
    "- `query: ...`\n",
    "\n",
    "This keeps model usage consistent during both training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_few_shot(dataset, n_per_class, seed=42):\n",
    "    # Sample n examples per class for few-shot training.\n",
    "    train_data = dataset[\"train\"].to_pandas()\n",
    "    parts = []\n",
    "    for label in sorted(train_data[\"label\"].unique()):\n",
    "        class_data = train_data[train_data[\"label\"] == label]\n",
    "        parts.append(class_data.sample(n=min(n_per_class, len(class_data)), random_state=seed))\n",
    "    return pd.concat(parts).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "\n",
    "SEEDS = [13, 42, 77]\n",
    "\n",
    "# Canonical few-shot sets (used in later sections)\n",
    "few_shot_8 = sample_few_shot(dataset, n_per_class=4, seed=42)    # 8 total\n",
    "few_shot_16 = sample_few_shot(dataset, n_per_class=8, seed=42)   # 16 total\n",
    "few_shot_32 = sample_few_shot(dataset, n_per_class=16, seed=42)  # 32 total\n",
    "\n",
    "print(f\"8-shot:  {len(few_shot_8)} examples\")\n",
    "print(f\"16-shot: {len(few_shot_16)} examples\")\n",
    "print(f\"32-shot: {len(few_shot_32)} examples\")\n",
    "\n",
    "print(\"\\n--- 8-shot training set (seed=42) ---\")\n",
    "for _, row in few_shot_8.iterrows():\n",
    "    label_name = \"CLAIM\" if row[\"label\"] == 1 else \"NO CLAIM\"\n",
    "    print(f\"[{label_name}] {row['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training SetFit with 8 Examples\n",
    "\n",
    "Default model: `intfloat/e5-small`.\n",
    "\n",
    "Why use it here:\n",
    "- compact and fast\n",
    "- strong semantic encoder for low-data setups\n",
    "- aligned with the retrieval stack we use in NB06\n",
    "\n",
    "Important detail: because this is E5, we add `query:` prefixes to all SetFit inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "BASE_MODEL = \"intfloat/e5-small\"\n",
    "\n",
    "\n",
    "def format_texts_for_model(texts, model_name):\n",
    "    texts = [str(t) for t in texts]\n",
    "    if \"intfloat/e5\" in model_name.lower():\n",
    "        return [f\"query: {t.strip()}\" for t in texts]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def preprocess_df_for_model(df, model_name):\n",
    "    out = df.copy()\n",
    "    out[\"text\"] = format_texts_for_model(out[\"text\"].tolist(), model_name)\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_setfit(\n",
    "    few_shot_df,\n",
    "    model_name=BASE_MODEL,\n",
    "    seed=42,\n",
    "    num_iterations=20,\n",
    "    num_epochs=4,\n",
    "    sampling_strategy=\"oversampling\",\n",
    "):\n",
    "    # Train SetFit and return (model, trainer).\n",
    "    set_seed(seed)\n",
    "\n",
    "    train_df_model = preprocess_df_for_model(few_shot_df[[\"text\", \"label\"]], model_name)\n",
    "    train_ds = Dataset.from_pandas(train_df_model[[\"text\", \"label\"]], preserve_index=False)\n",
    "    model = SetFitModel.from_pretrained(model_name)\n",
    "\n",
    "    arg_kwargs = {\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"batch_size\": 16,\n",
    "        \"sampling_strategy\": sampling_strategy,\n",
    "    }\n",
    "    if num_iterations is not None:\n",
    "        arg_kwargs[\"num_iterations\"] = num_iterations\n",
    "\n",
    "    args = TrainingArguments(**arg_kwargs)\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_ds)\n",
    "    trainer.train()\n",
    "    return model, trainer\n",
    "\n",
    "\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "test_texts_model = format_texts_for_model(test_df[\"text\"].tolist(), BASE_MODEL)\n",
    "\n",
    "print(f\"Training SetFit on 8 examples with {BASE_MODEL}...\")\n",
    "model, trainer = train_setfit(\n",
    "    few_shot_8,\n",
    "    model_name=BASE_MODEL,\n",
    "    seed=42,\n",
    "    num_iterations=20,\n",
    "    num_epochs=4,\n",
    "    sampling_strategy=\"oversampling\",\n",
    ")\n",
    "\n",
    "predictions = model.predict(test_texts_model)\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(test_df[\"label\"], predictions),\n",
    "    \"macro_f1\": f1_score(test_df[\"label\"], predictions, average=\"macro\"),\n",
    "}\n",
    "\n",
    "print(f\"\\n8-shot Accuracy: {metrics['accuracy']:.1%}\")\n",
    "print(f\"8-shot Macro-F1: {metrics['macro_f1']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SetFit (8-shot) -- Full Evaluation on Test Set\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    test_df[\"label\"],\n",
    "    predictions,\n",
    "    target_names=[\"No claim\", \"Environmental claim\"],\n",
    "    digits=3,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Label Efficiency: How Many Examples Do We Need?\n",
    "\n",
    "A practical question: how many labels should we buy from annotators?\n",
    "\n",
    "We run experiments with 4, 8, 16, and 32 examples per class and compare:\n",
    "- **SetFit**\n",
    "- **TF-IDF + Logistic Regression**\n",
    "\n",
    "Important upgrade vs the older version: we run **multiple random seeds** and report mean ± std, so conclusions are not based on a single lucky split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_tfidf(few_shot_df, test_texts, test_labels):\n",
    "    # TF-IDF + Logistic Regression baseline.\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000, stop_words=\"english\", ngram_range=(1, 2))),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\")),\n",
    "    ])\n",
    "    pipe.fit(few_shot_df[\"text\"], few_shot_df[\"label\"])\n",
    "    preds = pipe.predict(test_texts)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(test_labels, preds),\n",
    "        \"macro_f1\": f1_score(test_labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_one_setting(n_per_class, seed, model_name=BASE_MODEL):\n",
    "    # Single run for one data size + seed.\n",
    "    few_shot = sample_few_shot(dataset, n_per_class=n_per_class, seed=seed)\n",
    "\n",
    "    setfit_model, _ = train_setfit(\n",
    "        few_shot,\n",
    "        model_name=model_name,\n",
    "        seed=seed,\n",
    "        num_iterations=20,\n",
    "        num_epochs=4,\n",
    "        sampling_strategy=\"oversampling\",\n",
    "    )\n",
    "    test_texts_model = format_texts_for_model(test_df[\"text\"].tolist(), model_name)\n",
    "    setfit_preds = setfit_model.predict(test_texts_model)\n",
    "    setfit_scores = {\n",
    "        \"accuracy\": accuracy_score(test_df[\"label\"], setfit_preds),\n",
    "        \"macro_f1\": f1_score(test_df[\"label\"], setfit_preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    tfidf_scores = train_and_evaluate_tfidf(\n",
    "        few_shot,\n",
    "        test_df[\"text\"],\n",
    "        test_df[\"label\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"n_examples\": n_per_class * 2,\n",
    "        \"seed\": seed,\n",
    "        \"setfit_accuracy\": setfit_scores[\"accuracy\"],\n",
    "        \"setfit_macro_f1\": setfit_scores[\"macro_f1\"],\n",
    "        \"tfidf_accuracy\": tfidf_scores[\"accuracy\"],\n",
    "        \"tfidf_macro_f1\": tfidf_scores[\"macro_f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PER_CLASS_GRID = [4, 8, 16, 32]\n",
    "records = []\n",
    "\n",
    "for n_per_class in N_PER_CLASS_GRID:\n",
    "    print(f\"\\n{'='*64}\")\n",
    "    print(f\"{n_per_class*2} examples total ({n_per_class}/class)\")\n",
    "    print(f\"{'='*64}\")\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        rec = evaluate_one_setting(n_per_class=n_per_class, seed=seed, model_name=BASE_MODEL)\n",
    "        records.append(rec)\n",
    "        print(\n",
    "            f\"seed={seed} | SetFit acc={rec['setfit_accuracy']:.3f}, f1={rec['setfit_macro_f1']:.3f} | \"\n",
    "            f\"TF-IDF acc={rec['tfidf_accuracy']:.3f}, f1={rec['tfidf_macro_f1']:.3f}\"\n",
    "        )\n",
    "\n",
    "detailed_results_df = pd.DataFrame(records)\n",
    "results_df = (\n",
    "    detailed_results_df\n",
    "    .groupby(\"n_examples\", as_index=False)\n",
    "    .agg(\n",
    "        setfit_accuracy_mean=(\"setfit_accuracy\", \"mean\"),\n",
    "        setfit_accuracy_std=(\"setfit_accuracy\", \"std\"),\n",
    "        tfidf_accuracy_mean=(\"tfidf_accuracy\", \"mean\"),\n",
    "        tfidf_accuracy_std=(\"tfidf_accuracy\", \"std\"),\n",
    "        setfit_f1_mean=(\"setfit_macro_f1\", \"mean\"),\n",
    "        setfit_f1_std=(\"setfit_macro_f1\", \"std\"),\n",
    "        tfidf_f1_mean=(\"tfidf_macro_f1\", \"mean\"),\n",
    "        tfidf_f1_std=(\"tfidf_macro_f1\", \"std\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*64)\n",
    "print(\"Mean ± std over seeds\")\n",
    "print(\"=\"*64)\n",
    "print(results_df.round(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), sharex=True)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].errorbar(\n",
    "    results_df[\"n_examples\"],\n",
    "    results_df[\"setfit_accuracy_mean\"],\n",
    "    yerr=results_df[\"setfit_accuracy_std\"],\n",
    "    fmt=\"o-\",\n",
    "    label=\"SetFit\",\n",
    "    color=\"#E07850\",\n",
    "    linewidth=2,\n",
    "    capsize=4,\n",
    ")\n",
    "axes[0].errorbar(\n",
    "    results_df[\"n_examples\"],\n",
    "    results_df[\"tfidf_accuracy_mean\"],\n",
    "    yerr=results_df[\"tfidf_accuracy_std\"],\n",
    "    fmt=\"s--\",\n",
    "    label=\"TF-IDF + LR\",\n",
    "    color=\"#6B5D55\",\n",
    "    linewidth=2,\n",
    "    capsize=4,\n",
    ")\n",
    "axes[0].set_title(\"Accuracy vs Label Budget\")\n",
    "axes[0].set_xlabel(\"Number of training examples\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Macro-F1\n",
    "axes[1].errorbar(\n",
    "    results_df[\"n_examples\"],\n",
    "    results_df[\"setfit_f1_mean\"],\n",
    "    yerr=results_df[\"setfit_f1_std\"],\n",
    "    fmt=\"o-\",\n",
    "    label=\"SetFit\",\n",
    "    color=\"#E07850\",\n",
    "    linewidth=2,\n",
    "    capsize=4,\n",
    ")\n",
    "axes[1].errorbar(\n",
    "    results_df[\"n_examples\"],\n",
    "    results_df[\"tfidf_f1_mean\"],\n",
    "    yerr=results_df[\"tfidf_f1_std\"],\n",
    "    fmt=\"s--\",\n",
    "    label=\"TF-IDF + LR\",\n",
    "    color=\"#6B5D55\",\n",
    "    linewidth=2,\n",
    "    capsize=4,\n",
    ")\n",
    "axes[1].set_title(\"Macro-F1 vs Label Budget\")\n",
    "axes[1].set_xlabel(\"Number of training examples\")\n",
    "axes[1].set_ylabel(\"Macro-F1\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "fig.suptitle(\"Label Efficiency with Seed Variance\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. LLM-Bootstrapped Contrastive Augmentation\n\nStandard paraphrase augmentation often disappoints because generated texts are too similar to originals and don't increase the contrastive signal.\n\n**Our strategy: Rephrase + Hard Negative per example**\n\nFor each seed example in the few-shot set, the LLM generates:\n\n1. **Rephrase** (same label): diverse rewording that preserves the label — this is a contrastive *positive*.\n2. **Hard negative** (opposite label): a superficially similar sentence that crosses the decision boundary — this is a contrastive *hard negative*.\n\nKey design choices:\n- **Full context**: ALL few-shot examples are passed to the LLM, so it understands the class boundary when generating.\n- **Boundary-aware generation**: The hard negative must be plausible enough to confuse a weak classifier — this directly improves the contrastive training signal.\n- **Label balance**: Since each class generates hard negatives for the other, the augmented set stays balanced.\n\nWe compare three settings:\n1. **Baseline**: Original 8-shot examples only\n2. **Rephrase-only**: Original + LLM rephrases (2x data, same-label augmentation)\n3. **Full contrastive**: Original + rephrases + hard negatives (3x data, cross-label augmentation)\n\n> **Note:** This section requires a Groq API key. If absent, it is safely skipped."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n\n# If not set above, try Colab secrets -> then environment variable\nif not GROQ_API_KEY:\n    try:\n        from google.colab import userdata\n        GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\")\n    except (ImportError, Exception):\n        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n\nAUGMENT_MODEL = \"moonshotai/kimi-k2-instruct\"\n\nLABEL_GUIDE = {\n    0: \"No environmental claim. It may mention business activity or sustainability context, but does not assert an environmental action/outcome.\",\n    1: \"Environmental claim. It explicitly states environmental action, impact, commitment, or performance.\",\n}\n\n\ndef generate_rephrase_and_hard_negative(text, label, all_examples_df):\n    \"\"\"For a given example, generate:\n    - 1 rephrase with the SAME label (diverse rewording = contrastive positive)\n    - 1 hard negative with the OPPOSITE label (superficially similar = contrastive hard negative)\n\n    All few-shot examples are provided as context so the LLM understands the boundary.\n    \"\"\"\n    if not GROQ_API_KEY:\n        return None, None\n\n    # Build context block from ALL few-shot examples\n    context_lines = []\n    for _, row in all_examples_df.iterrows():\n        lbl = \"CLAIM\" if row[\"label\"] == 1 else \"NO CLAIM\"\n        context_lines.append(f\"[{lbl}] {row['text'][:150]}\")\n    context_block = \"\\n\".join(context_lines)\n\n    opposite_label = 1 - label\n    current_name = \"CLAIM\" if label == 1 else \"NO CLAIM\"\n    opposite_name = \"CLAIM\" if opposite_label == 1 else \"NO CLAIM\"\n\n    prompt = f\"\"\"You are creating contrastive training data for binary classification of environmental claims.\n\n## Label definitions\n- Label 0 (NO CLAIM): {LABEL_GUIDE[0]}\n- Label 1 (CLAIM): {LABEL_GUIDE[1]}\n\n## All labeled examples for context\n{context_block}\n\n## Task\nGiven the input text (label: {current_name}), produce exactly 2 items:\n\n1. REPHRASE: A paraphrase that keeps the SAME label ({current_name}).\n   - Use substantially different wording and sentence structure from the original.\n   - Keep <= 35 words. Must clearly remain label-correct.\n\n2. HARD_NEGATIVE: A sentence that is superficially similar to the input but belongs to the OPPOSITE label ({opposite_name}).\n   - It should be near the decision boundary — plausible enough to confuse a weak classifier.\n   - Keep <= 35 words. Must clearly belong to the opposite label.\n\nReturn ONLY a JSON object: {{\"rephrase\": \"...\", \"hard_negative\": \"...\"}}\n\nInput text: {text}\"\"\"\n\n    response = client.chat.completions.create(\n        model=AUGMENT_MODEL,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.8,\n        max_tokens=300,\n    )\n\n    raw = response.choices[0].message.content.strip()\n    raw = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n\n    try:\n        data = json.loads(raw)\n        rephrase = str(data.get(\"rephrase\", \"\")).strip()\n        hard_neg = str(data.get(\"hard_negative\", \"\")).strip()\n        rephrase = rephrase if len(rephrase.split()) >= 5 else None\n        hard_neg = hard_neg if len(hard_neg.split()) >= 5 else None\n        return rephrase, hard_neg\n    except Exception:\n        return None, None\n\n\nif GROQ_API_KEY:\n    client = OpenAI(api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n    print(f\"API connected. Using {AUGMENT_MODEL}\")\nelse:\n    print(\"Set GROQ_API_KEY to run bootstrapping.\")\n    print(\"You can get a free key at https://console.groq.com/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def normalize_text(text):\n    return \" \".join(str(text).lower().split())\n\n\ndef dedupe_augmented(df):\n    tmp = df.copy()\n    tmp[\"text_norm\"] = tmp[\"text\"].apply(normalize_text)\n    tmp = tmp.drop_duplicates(subset=[\"text_norm\", \"label\"]).drop(columns=[\"text_norm\"])\n    return tmp.reset_index(drop=True)\n\n\nif GROQ_API_KEY:\n    rows_rephrase_only = []  # originals + rephrases (same-label only)\n    rows_full_contrastive = []  # originals + rephrases + hard negatives\n\n    print(f\"Generating rephrase + hard negative per example (model: {AUGMENT_MODEL})...\")\n    print(f\"All {len(few_shot_8)} examples passed as context to each call.\\n\")\n\n    for _, row in tqdm(few_shot_8.iterrows(), total=len(few_shot_8)):\n        original = row[\"text\"]\n        label = int(row[\"label\"])\n        opposite_label = 1 - label\n\n        # Original goes into both sets\n        rows_rephrase_only.append({\"text\": original, \"label\": label, \"source\": \"original\"})\n        rows_full_contrastive.append({\"text\": original, \"label\": label, \"source\": \"original\"})\n\n        rephrase, hard_neg = generate_rephrase_and_hard_negative(\n            original, label, all_examples_df=few_shot_8\n        )\n\n        if rephrase and normalize_text(rephrase) != normalize_text(original):\n            rows_rephrase_only.append({\"text\": rephrase, \"label\": label, \"source\": \"rephrase\"})\n            rows_full_contrastive.append({\"text\": rephrase, \"label\": label, \"source\": \"rephrase\"})\n            lbl_name = \"CLAIM\" if label == 1 else \"NO CLAIM\"\n            print(f\"  [{lbl_name}] rephrase: {rephrase[:80]}...\")\n\n        if hard_neg and normalize_text(hard_neg) != normalize_text(original):\n            rows_full_contrastive.append({\"text\": hard_neg, \"label\": opposite_label, \"source\": \"hard_negative\"})\n            opp_name = \"CLAIM\" if opposite_label == 1 else \"NO CLAIM\"\n            print(f\"  [{opp_name}] hard neg: {hard_neg[:80]}...\")\n\n    rephrase_only_df = dedupe_augmented(pd.DataFrame(rows_rephrase_only))\n    full_contrastive_df = dedupe_augmented(pd.DataFrame(rows_full_contrastive))\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Original size:          {len(few_shot_8)}\")\n    print(f\"Rephrase-only size:     {len(rephrase_only_df)} (originals + rephrases)\")\n    print(f\"Full contrastive size:  {len(full_contrastive_df)} (+ hard negatives)\")\n    print(f\"\\nRephrase-only labels:     {rephrase_only_df['label'].value_counts().to_dict()}\")\n    print(f\"Full contrastive labels:  {full_contrastive_df['label'].value_counts().to_dict()}\")\n    print(f\"Sources in full set:      {full_contrastive_df['source'].value_counts().to_dict()}\")\nelse:\n    print(\"Skipping -- no GROQ_API_KEY set.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_training_df(\n    train_df,\n    tag,\n    model_name=BASE_MODEL,\n    seeds=(13, 42, 77),\n    num_iterations=20,\n    sampling_strategy=\"oversampling\",\n):\n    rows = []\n    for seed in seeds:\n        model_i, _ = train_setfit(\n            train_df[[\"text\", \"label\"]],\n            model_name=model_name,\n            seed=seed,\n            num_iterations=num_iterations,\n            num_epochs=4,\n            sampling_strategy=sampling_strategy,\n        )\n        test_texts_model = format_texts_for_model(test_df[\"text\"].tolist(), model_name)\n        preds_i = model_i.predict(test_texts_model)\n        rows.append({\n            \"seed\": seed,\n            \"accuracy\": accuracy_score(test_df[\"label\"], preds_i),\n            \"macro_f1\": f1_score(test_df[\"label\"], preds_i, average=\"macro\"),\n        })\n\n    score_df = pd.DataFrame(rows)\n    return {\n        \"setting\": tag,\n        \"model\": model_name,\n        \"accuracy_mean\": score_df[\"accuracy\"].mean(),\n        \"accuracy_std\": score_df[\"accuracy\"].std(ddof=0),\n        \"macro_f1_mean\": score_df[\"macro_f1\"].mean(),\n        \"macro_f1_std\": score_df[\"macro_f1\"].std(ddof=0),\n    }\n\n\nif GROQ_API_KEY:\n    comparison_rows = []\n\n    # 1. Baseline: original 8-shot\n    comparison_rows.append(evaluate_training_df(\n        few_shot_8,\n        tag=\"8-shot baseline\",\n        model_name=BASE_MODEL,\n        seeds=SEEDS,\n        num_iterations=20,\n        sampling_strategy=\"oversampling\",\n    ))\n\n    # 2. Rephrase-only: originals + same-label rephrases (2x data)\n    comparison_rows.append(evaluate_training_df(\n        rephrase_only_df,\n        tag=\"+ rephrases only (2x)\",\n        model_name=BASE_MODEL,\n        seeds=SEEDS,\n        num_iterations=None,\n        sampling_strategy=\"unique\",\n    ))\n\n    # 3. Full contrastive: originals + rephrases + hard negatives (3x data)\n    comparison_rows.append(evaluate_training_df(\n        full_contrastive_df,\n        tag=\"+ rephrases + hard negatives (3x)\",\n        model_name=BASE_MODEL,\n        seeds=SEEDS,\n        num_iterations=None,\n        sampling_strategy=\"unique\",\n    ))\n\n    comparison_df = pd.DataFrame(comparison_rows).sort_values(\"macro_f1_mean\", ascending=False)\n    print(\"Augmentation strategy comparison (mean ± std over seeds)\")\n    print(\"=\" * 80)\n    print(comparison_df.round(3).to_string(index=False))\n\n    # Quick model sensitivity check on full contrastive data\n    print(\"\\nModel sensitivity on full contrastive set (seed=42):\")\n    for model_name in [\n        BASE_MODEL,\n        \"BAAI/bge-small-en-v1.5\",\n        \"sentence-transformers/all-MiniLM-L6-v2\",\n    ]:\n        m, _ = train_setfit(\n            full_contrastive_df[[\"text\", \"label\"]],\n            model_name=model_name,\n            seed=42,\n            num_iterations=None,\n            num_epochs=4,\n            sampling_strategy=\"unique\",\n        )\n        preds = m.predict(format_texts_for_model(test_df[\"text\"].tolist(), model_name))\n        acc = accuracy_score(test_df[\"label\"], preds)\n        f1 = f1_score(test_df[\"label\"], preds, average=\"macro\")\n        print(f\"  {model_name:45s} acc={acc:.3f} macro_f1={f1:.3f}\")\nelse:\n    print(\"Skipping -- no GROQ_API_KEY set.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise\n",
    "\n",
    "Pick one:\n",
    "\n",
    "### Option A: Prompt Contrast Ablation\n",
    "Remove the boundary-focused instruction from the augmentation prompt. Compare 3x results before/after. Did macro-F1 drop?\n",
    "\n",
    "### Option B: 2x vs 3x vs 4x\n",
    "Set `PARAPHRASES_PER_EXAMPLE` to 1, 2, and 3. Plot performance and identify where gains plateau.\n",
    "\n",
    "### Option C: Model Swap Under Fixed Data\n",
    "Keep the same 3x augmented dataset and compare at least 3 encoders (`intfloat/e5-small`, `BAAI/bge-small-en-v1.5`, `all-MiniLM-L6-v2`). Which is most stable across seeds?\n",
    "\n",
    "### Option D: Prefix Ablation for E5\n",
    "If using E5, compare runs with vs without `query:` prefixes. How much performance changes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# -------------------------------------------------------\n",
    "# Option A: Try a different base model\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# model_name = \"BAAI/bge-small-en-v1.5\"  # <-- change this\n",
    "#\n",
    "# train_ds = Dataset.from_pandas(few_shot_8[['text', 'label']])\n",
    "# model_alt = SetFitModel.from_pretrained(model_name)\n",
    "# args_alt = TrainingArguments(num_iterations=20, num_epochs=1)\n",
    "# trainer_alt = Trainer(\n",
    "#     model=model_alt,\n",
    "#     args=args_alt,\n",
    "#     train_dataset=train_ds,\n",
    "# )\n",
    "# trainer_alt.train()\n",
    "# alt_metrics = trainer_alt.evaluate(dataset['test'])\n",
    "# print(f\"8-shot with {model_name}: {alt_metrics['accuracy']:.1%}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Option C: Try different num_iterations values\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# iter_results = []\n",
    "# for n_iter in [5, 10, 20, 40]:\n",
    "#     model_iter = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "#     args_iter = TrainingArguments(num_iterations=n_iter, num_epochs=1)\n",
    "#     trainer_iter = Trainer(\n",
    "#         model=model_iter,\n",
    "#         args=args_iter,\n",
    "#         train_dataset=train_ds,\n",
    "#     )\n",
    "#     trainer_iter.train()\n",
    "#     acc = trainer_iter.evaluate(dataset['test'])['accuracy']\n",
    "#     iter_results.append({'num_iterations': n_iter, 'accuracy': acc})\n",
    "#     print(f\"num_iterations={n_iter}: {acc:.1%}\")\n",
    "#\n",
    "# print(pd.DataFrame(iter_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Takeaways\n",
    "\n",
    "### What we learned\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **SetFit in low-data regimes** | Strong few-shot baseline, but one-run results are noisy; use multi-seed reporting. |\n",
    "| **Metric choice** | Macro-F1 is essential when class performance is asymmetric. |\n",
    "| **LLM bootstrapping** | Gains depend on augmentation quality: label-faithful, diverse rewrites help; weak paraphrases can hurt. |\n",
    "| **2x vs 3x** | Tripling data can help, but returns are task- and model-dependent. Validate empirically. |\n",
    "| **E5 usage detail** | If you use IntFloat E5 models, keep input formatting consistent (`query:` prefixes here). |\n",
    "\n",
    "### Practical recommendations (2026)\n",
    "\n",
    "1. Always report **mean ± std across seeds** for few-shot experiments.\n",
    "2. Keep augmentation prompts **label-anchored and diversity-seeking**.\n",
    "3. For larger augmented sets, test `sampling_strategy=\"unique\"`.\n",
    "4. Compare at least 2--3 encoders early.\n",
    "5. Track both **accuracy and macro-F1**.\n",
    "\n",
    "### Maker tutorials and references\n",
    "\n",
    "- SetFit quickstart: https://huggingface.co/docs/setfit/main/en/quickstart\n",
    "- SetFit training arguments: https://huggingface.co/docs/setfit/main/en/reference/trainer\n",
    "- SetFit synthetic data tutorial: https://huggingface.co/docs/setfit/main/en/tutorials/setfit_synthetic\n",
    "- SetFit distillation tutorial: https://huggingface.co/docs/setfit/main/en/how_to/knowledge_distillation\n",
    "\n",
    "---\n",
    "\n",
    "*Next notebook: retrieval and reranking pipelines where these classifiers can be used for filtering and routing.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}