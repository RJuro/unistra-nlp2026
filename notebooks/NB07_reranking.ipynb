{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NB07: Cross-encoder Reranking\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB07_reranking.ipynb)\n\n**Duration:** 50 minutes\n\n## Learning Goals\n\nBy the end of this notebook you will be able to:\n\n1. **Understand** the bi-encoder vs cross-encoder tradeoff -- why we need both.\n2. **Implement** a two-stage reranking pipeline (bi-encoder retrieval + cross-encoder reranking).\n3. **Measure** the precision improvement that reranking provides over bi-encoder retrieval alone.\n4. **Apply** the pipeline to social science retrieval tasks (policy documents, academic papers, case law)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu sentence-transformers datasets pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: Bi-encoders Are Fast but Imprecise\n",
    "\n",
    "In NB06 we built a semantic search system using a **bi-encoder**. Bi-encoders encode the query and each document **independently** into fixed-size vectors, then compare them with cosine similarity. This is extremely fast -- we can search millions of documents in milliseconds using FAISS.\n",
    "\n",
    "But there is a cost: because the query and document are encoded separately, the model **cannot attend across them**. It misses fine-grained interactions between query terms and document terms. For example, a bi-encoder might struggle to distinguish:\n",
    "\n",
    "- *\"Does smoking cause cancer?\"* vs *\"Does cancer cause smoking?\"*\n",
    "- *\"Python eats mouse\"* vs *\"Mouse clicks in Python\"*\n",
    "\n",
    "**Cross-encoders** solve this by processing the query and document **together** as a single input. The model can attend to both simultaneously, capturing rich interactions. The result is much more accurate relevance scores -- but at the cost of speed, since we cannot pre-compute document embeddings.\n",
    "\n",
    "| Property | Bi-encoder | Cross-encoder |\n",
    "|---|---|---|\n",
    "| Input | Query and document encoded separately | Query and document encoded together |\n",
    "| Speed | Very fast (vector similarity) | Slow (full forward pass per pair) |\n",
    "| Accuracy | Good | Excellent |\n",
    "| Scalability | Millions of documents | Hundreds of documents |\n",
    "| Use case | First-stage retrieval | Reranking a small candidate set |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Two-Stage Pipeline\n\nThe standard approach in modern information retrieval is to combine both models in a **two-stage pipeline**:\n\n![Two-Stage Retrieval Pipeline](https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/notebooks/figures/reranking_pipeline.png)\n\n**Stage 1 (Bi-encoder):** Quickly narrows the full corpus down to ~100 plausible candidates using vector similarity. This is the same approach we used in NB06.\n\n**Stage 2 (Cross-encoder):** Takes each of the ~100 candidates and scores them jointly with the query. Reorders them by the more accurate cross-encoder score. Returns the top 5.\n\nThis gives us the **best of both worlds**: the speed of bi-encoders with the accuracy of cross-encoders."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus and build FAISS index (same setup as NB06)\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mteb/scifact\", split=\"corpus\")\n",
    "corpus_df = dataset.to_pandas().head(300)\n",
    "corpus_df.columns = ['doc_id', 'title', 'text']\n",
    "corpus_df['full_text'] = corpus_df['title'] + \". \" + corpus_df['text']\n",
    "\n",
    "# Stage 1: Bi-encoder\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "corpus_embeddings = bi_encoder.encode(\n",
    "    corpus_df['full_text'].tolist(), show_progress_bar=True,\n",
    "    normalize_embeddings=True, batch_size=64\n",
    ").astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
    "index.add(corpus_embeddings)\n",
    "print(f\"Index built: {index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Cross-encoder\n",
    "\n",
    "We use a cross-encoder trained on MS MARCO, a large-scale passage ranking dataset. The model takes a `(query, document)` pair and outputs a single relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-encoder: processes (query, document) pairs jointly\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"Cross-encoder loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Reranking Pipeline\n",
    "\n",
    "Now we combine both stages into a single function. The bi-encoder retrieves a broad set of candidates, and the cross-encoder rescores and reorders them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rerank(query: str, top_k_retrieve: int = 20, top_k_final: int = 5):\n",
    "    \"\"\"Two-stage retrieval: bi-encoder retrieve -> cross-encoder rerank.\"\"\"\n",
    "    \n",
    "    # Stage 1: Bi-encoder retrieval (fast)\n",
    "    q_emb = bi_encoder.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    bi_scores, bi_indices = index.search(q_emb, top_k_retrieve)\n",
    "    \n",
    "    # Stage 2: Cross-encoder reranking (accurate)\n",
    "    # Create (query, document) pairs for cross-encoder scoring\n",
    "    pairs = [(query, corpus_df.iloc[idx]['full_text']) for idx in bi_indices[0]]\n",
    "    cross_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Sort by cross-encoder score\n",
    "    reranked = sorted(\n",
    "        zip(bi_indices[0], bi_scores[0], cross_scores),\n",
    "        key=lambda x: x[2],  # Sort by cross-encoder score\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for rank, (idx, bi_score, ce_score) in enumerate(reranked[:top_k_final], 1):\n",
    "        results.append({\n",
    "            'rank': rank,\n",
    "            'bi_score': float(bi_score),\n",
    "            'ce_score': float(ce_score),\n",
    "            'title': corpus_df.iloc[idx]['title'],\n",
    "            'text': corpus_df.iloc[idx]['full_text'][:200] + '...'\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_encoder_only(query: str, top_k: int = 5):\n",
    "    \"\"\"Bi-encoder retrieval only (no reranking).\"\"\"\n",
    "    q_emb = bi_encoder.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    scores, indices = index.search(q_emb, top_k)\n",
    "    results = []\n",
    "    for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
    "        results.append({\n",
    "            'rank': rank,\n",
    "            'score': float(score),\n",
    "            'title': corpus_df.iloc[idx]['title'],\n",
    "            'text': corpus_df.iloc[idx]['full_text'][:200] + '...'\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test query\n",
    "query = \"What are the risk factors for developing lung cancer?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"QUERY: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- Bi-encoder only (top 5) ---\")\n",
    "bi_results = bi_encoder_only(query)\n",
    "for _, r in bi_results.iterrows():\n",
    "    print(f\"  [{r['rank']}] ({r['score']:.3f}) {r['title']}\")\n",
    "\n",
    "print(\"\\n--- Bi-encoder + Cross-encoder reranking (top 5) ---\")\n",
    "reranked_results = retrieve_and_rerank(query)\n",
    "for _, r in reranked_results.iterrows():\n",
    "    print(f\"  [{r['rank']}] (bi:{r['bi_score']:.3f} -> ce:{r['ce_score']:.3f}) {r['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Systematic Evaluation\n",
    "\n",
    "Let's compare precision across multiple queries. We use a simple keyword-based proxy for relevance: a result is \"relevant\" if it contains at least one of the expected terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries = [\n",
    "    {\"query\": \"How do vaccines protect against viral infections?\",\n",
    "     \"relevant_terms\": [\"vaccine\", \"immun\", \"viral\", \"antibod\", \"infection\"]},\n",
    "    {\"query\": \"What causes antibiotic resistance in bacteria?\",\n",
    "     \"relevant_terms\": [\"antibiotic\", \"resist\", \"bacteria\", \"antimicrobial\"]},\n",
    "    {\"query\": \"How does smoking affect lung health?\",\n",
    "     \"relevant_terms\": [\"smok\", \"lung\", \"tobacco\", \"cancer\", \"respiratory\"]},\n",
    "    {\"query\": \"What role does genetics play in obesity?\",\n",
    "     \"relevant_terms\": [\"gene\", \"obes\", \"BMI\", \"weight\", \"metabol\"]},\n",
    "    {\"query\": \"How does exercise impact mental health?\",\n",
    "     \"relevant_terms\": [\"exercise\", \"mental\", \"depress\", \"anxiety\", \"physical\"]},\n",
    "]\n",
    "\n",
    "def calc_precision(results_df, relevant_terms, text_col='text'):\n",
    "    \"\"\"Calculate precision based on keyword matching.\"\"\"\n",
    "    relevant = sum(\n",
    "        any(term in row[text_col].lower() for term in relevant_terms)\n",
    "        for _, row in results_df.iterrows()\n",
    "    )\n",
    "    return relevant / len(results_df)\n",
    "\n",
    "print(f\"{'Query':<50} {'Bi-enc P@5':>12} {'Reranked P@5':>14}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "bi_precisions = []\n",
    "reranked_precisions = []\n",
    "\n",
    "for eq in eval_queries:\n",
    "    bi_res = bi_encoder_only(eq['query'])\n",
    "    reranked_res = retrieve_and_rerank(eq['query'])\n",
    "    \n",
    "    bi_p = calc_precision(bi_res, eq['relevant_terms'])\n",
    "    re_p = calc_precision(reranked_res, eq['relevant_terms'])\n",
    "    bi_precisions.append(bi_p)\n",
    "    reranked_precisions.append(re_p)\n",
    "    \n",
    "    print(f\"  {eq['query'][:48]:<50} {bi_p:>10.0%}   {re_p:>12.0%}\")\n",
    "\n",
    "print(f\"\\n{'Average':<50} {np.mean(bi_precisions):>10.0%}   {np.mean(reranked_precisions):>12.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Speed vs Accuracy Tradeoff\n",
    "\n",
    "Cross-encoder reranking adds latency. Let's measure exactly how much, so we can make informed decisions about when the tradeoff is worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"effects of air pollution on respiratory health\"\n",
    "\n",
    "# Time bi-encoder only\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    bi_encoder_only(query)\n",
    "bi_time = (time.time() - start) / 10\n",
    "\n",
    "# Time reranking pipeline\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    retrieve_and_rerank(query)\n",
    "rerank_time = (time.time() - start) / 10\n",
    "\n",
    "print(f\"Bi-encoder only:  {bi_time*1000:.1f} ms/query\")\n",
    "print(f\"With reranking:   {rerank_time*1000:.1f} ms/query\")\n",
    "print(f\"Reranking overhead: {(rerank_time-bi_time)*1000:.1f} ms ({rerank_time/bi_time:.1f}x slower)\")\n",
    "print(f\"\\nFor social science research, this tradeoff is usually worth it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise: Tune the Pipeline\n",
    "\n",
    "Experiment with the pipeline to understand how its components affect performance:\n",
    "\n",
    "1. **Vary `top_k_retrieve`**: Try values of 10, 20, 50, and 100. How does the number of bi-encoder candidates affect final precision and latency?\n",
    "2. **Try a different cross-encoder**: Replace `cross-encoder/ms-marco-MiniLM-L-6-v2` with another model (e.g., `cross-encoder/ms-marco-TinyBERT-L-2-v2` for speed, or `cross-encoder/ms-marco-MiniLM-L-12-v2` for accuracy).\n",
    "3. **Add your own queries**: Think of a social science research question and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Experiment 1: Try different top_k_retrieve values\n",
    "# for k in [10, 20, 50, 100]:\n",
    "#     results = retrieve_and_rerank(\"your query here\", top_k_retrieve=k)\n",
    "#     print(f\"top_k_retrieve={k}: ...\")\n",
    "\n",
    "# Experiment 2: Try a different cross-encoder model\n",
    "# cross_encoder_v2 = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n",
    "# ...\n",
    "\n",
    "# Experiment 3: Add your own social science query\n",
    "# my_query = \"...\"\n",
    "# retrieve_and_rerank(my_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Bonus: Deploy as a Gradio App\n\nLet's turn our two-stage retrieval pipeline into an interactive search interface. Users can type natural-language queries and see reranked results instantly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "try:\n    !pip install gradio -q\n    import gradio as gr\n\n    def search_and_rerank(query, top_k=5):\n        \"\"\"Search and rerank, returning formatted markdown results.\"\"\"\n        if not query.strip():\n            return \"Please enter a search query.\"\n        results = retrieve_and_rerank(query, top_k_retrieve=20, top_k_final=int(top_k))\n        output = f\"## Results for: *{query}*\\n\\n\"\n        for _, row in results.iterrows():\n            output += f\"**[{row['rank']}]** (score: {row['ce_score']:.3f}) **{row['title']}**\\n\\n\"\n            output += f\"> {row['text'][:150]}...\\n\\n---\\n\\n\"\n        return output\n\n    demo = gr.Interface(\n        fn=search_and_rerank,\n        inputs=[\n            gr.Textbox(lines=2, placeholder=\"Enter a search query...\"),\n            gr.Slider(minimum=1, maximum=10, value=5, step=1, label=\"Number of results\"),\n        ],\n        outputs=gr.Markdown(label=\"Search Results\"),\n        title=\"Semantic Search with Reranking\",\n        description=\"Two-stage retrieval: bi-encoder (fast) â†’ cross-encoder (accurate). Search 300 scientific abstracts.\",\n        examples=[[\"How do vaccines protect against infections?\"], [\"genetic factors in cancer risk\"]],\n    )\n    demo.launch(share=True)\n\nexcept ImportError:\n    print(\"Gradio not available. Install with: pip install gradio\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Takeaways\n",
    "\n",
    "**Two-stage retrieval is the standard approach in production search systems.** Nearly every modern search engine -- from Google to academic paper search -- uses some form of this pattern.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- **Bi-encoders** encode queries and documents independently. They are fast and scalable (millions of documents) but miss fine-grained query-document interactions.\n",
    "- **Cross-encoders** process the query and document together. They are much more accurate but too slow to apply to an entire corpus.\n",
    "- **The two-stage pipeline** combines both: bi-encoder for fast candidate retrieval, cross-encoder for precise reranking. This gives us speed *and* accuracy.\n",
    "- **The pipeline is modular**: you can swap out the bi-encoder, the cross-encoder, the vector index, or the candidate pool size independently. This makes it easy to experiment and improve.\n",
    "\n",
    "**Social science applications:**\n",
    "\n",
    "- **Finding relevant policy documents** in large government archives\n",
    "- **Academic paper retrieval** for literature reviews\n",
    "- **Case law search** in legal research\n",
    "- **Survey response matching** for qualitative analysis\n",
    "- **Media analysis** -- finding relevant news articles on specific social issues\n",
    "\n",
    "In the next notebook, we will look at how to fine-tune these models on domain-specific data to further improve retrieval quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}