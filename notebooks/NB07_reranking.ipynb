{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NB07: Cross-encoder Reranking\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB07_reranking.ipynb)\n\n**Duration:** 50 minutes\n\n## Learning Goals\n\nBy the end of this notebook you will be able to:\n\n1. **Understand** the bi-encoder vs cross-encoder tradeoff -- why we need both.\n2. **Implement** a two-stage reranking pipeline (bi-encoder retrieval + cross-encoder reranking).\n3. **Measure** the precision improvement that reranking provides over bi-encoder retrieval alone.\n4. **Apply** the pipeline to social science retrieval tasks (policy documents, academic papers, case law)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install faiss-cpu sentence-transformers datasets pandas numpy -q\n\nimport faiss\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom datasets import load_dataset\n\nprint(f\"FAISS version: {faiss.__version__}\")\nprint(\"All imports successful.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: Bi-encoders Are Fast but Imprecise\n",
    "\n",
    "In NB06 we built a semantic search system using a **bi-encoder**. Bi-encoders encode the query and each document **independently** into fixed-size vectors, then compare them with cosine similarity. This is extremely fast -- we can search millions of documents in milliseconds using FAISS.\n",
    "\n",
    "But there is a cost: because the query and document are encoded separately, the model **cannot attend across them**. It misses fine-grained interactions between query terms and document terms. For example, a bi-encoder might struggle to distinguish:\n",
    "\n",
    "- *\"Does smoking cause cancer?\"* vs *\"Does cancer cause smoking?\"*\n",
    "- *\"Python eats mouse\"* vs *\"Mouse clicks in Python\"*\n",
    "\n",
    "**Cross-encoders** solve this by processing the query and document **together** as a single input. The model can attend to both simultaneously, capturing rich interactions. The result is much more accurate relevance scores -- but at the cost of speed, since we cannot pre-compute document embeddings.\n",
    "\n",
    "| Property | Bi-encoder | Cross-encoder |\n",
    "|---|---|---|\n",
    "| Input | Query and document encoded separately | Query and document encoded together |\n",
    "| Speed | Very fast (vector similarity) | Slow (full forward pass per pair) |\n",
    "| Accuracy | Good | Excellent |\n",
    "| Scalability | Millions of documents | Hundreds of documents |\n",
    "| Use case | First-stage retrieval | Reranking a small candidate set |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Two-Stage Pipeline\n\nThe standard approach in modern information retrieval is to combine both models in a **two-stage pipeline**:\n\n![Two-Stage Retrieval Pipeline](https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/notebooks/figures/reranking_pipeline.png)\n\n**Stage 1 (Bi-encoder):** Quickly narrows the full corpus down to ~100 plausible candidates using vector similarity. This is the same approach we used in NB06.\n\n**Stage 2 (Cross-encoder):** Takes each of the ~100 candidates and scores them jointly with the query. Reorders them by the more accurate cross-encoder score. Returns the top 5.\n\nThis gives us the **best of both worlds**: the speed of bi-encoders with the accuracy of cross-encoders."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Load SciFact (same approach as NB06) ──────────────────────────────────\n\ndef load_scifact(max_docs: int | None = None):\n    corpus = load_dataset(\"mteb/scifact\", \"corpus\", split=\"corpus\").to_pandas()\n    queries = load_dataset(\"mteb/scifact\", \"queries\", split=\"queries\").to_pandas()\n    qrels_train = load_dataset(\"mteb/scifact\", \"default\", split=\"train\").to_pandas()\n    qrels_test = load_dataset(\"mteb/scifact\", \"default\", split=\"test\").to_pandas()\n\n    corpus[\"_id\"] = corpus[\"_id\"].astype(str)\n    queries[\"_id\"] = queries[\"_id\"].astype(str)\n    qrels_train[\"query-id\"] = qrels_train[\"query-id\"].astype(str)\n    qrels_train[\"corpus-id\"] = qrels_train[\"corpus-id\"].astype(str)\n    qrels_test[\"query-id\"] = qrels_test[\"query-id\"].astype(str)\n    qrels_test[\"corpus-id\"] = qrels_test[\"corpus-id\"].astype(str)\n\n    corpus[\"title\"] = corpus[\"title\"].fillna(\"\").astype(str)\n    corpus[\"text\"] = corpus[\"text\"].fillna(\"\").astype(str)\n    corpus[\"full_text\"] = (corpus[\"title\"].str.strip() + \". \" + corpus[\"text\"].str.strip()).str.strip(\" .\")\n\n    corpus_df = corpus.rename(columns={\"_id\": \"doc_id\"})[[\"doc_id\", \"title\", \"text\", \"full_text\"]]\n    queries_df = queries.rename(columns={\"_id\": \"query_id\", \"text\": \"query\"})[[\"query_id\", \"query\"]]\n\n    if max_docs is not None:\n        corpus_df = corpus_df.head(max_docs).reset_index(drop=True)\n\n    return corpus_df.reset_index(drop=True), queries_df, qrels_train, qrels_test\n\n\ncorpus_df, queries_df, qrels_train, qrels_test = load_scifact(max_docs=None)\n\n# Combine train + test qrels for evaluation\nqrels = pd.concat([qrels_train, qrels_test], ignore_index=True)\n\n# Build a lookup: query_id -> set of relevant doc_ids\nrelevant_docs = qrels.groupby(\"query-id\")[\"corpus-id\"].apply(set).to_dict()\n\nprint(f\"Corpus: {len(corpus_df)} docs | Queries: {len(queries_df)} | Qrels: {len(qrels)} judgments\")\nprint(f\"Queries with relevance labels: {len(relevant_docs)}\")\n\n# ── Bi-encoder: intfloat/e5-small (same as NB06) ─────────────────────────\nMODEL_NAME = \"intfloat/e5-small\"\nbi_encoder = SentenceTransformer(MODEL_NAME)\n\n\ndef format_passage(text: str) -> str:\n    return f\"passage: {text.strip()}\"\n\n\ndef format_query(text: str) -> str:\n    return f\"query: {text.strip()}\"\n\n\ncorpus_inputs = [format_passage(t) for t in corpus_df[\"full_text\"].tolist()]\n\nprint(f\"\\nEncoding corpus with {MODEL_NAME}...\")\nstart = time.time()\ncorpus_embeddings = bi_encoder.encode(\n    corpus_inputs,\n    show_progress_bar=True,\n    batch_size=64,\n    normalize_embeddings=True,\n).astype(\"float32\")\n\nindex = faiss.IndexFlatIP(corpus_embeddings.shape[1])\nindex.add(corpus_embeddings)\nprint(f\"Index built in {time.time()-start:.1f}s: {index.ntotal} vectors, {corpus_embeddings.shape[1]} dims\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Cross-encoder\n",
    "\n",
    "We use a cross-encoder trained on MS MARCO, a large-scale passage ranking dataset. The model takes a `(query, document)` pair and outputs a single relevance score."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cross-encoder: processes (query, document) pairs jointly\ncross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\nprint(\"Cross-encoder loaded!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. The Reranking Pipeline\n\nNow we combine both stages into a single function. The bi-encoder retrieves a broad set of candidates (using E5 `query:` prefix), and the cross-encoder rescores and reorders them (using raw query-document pairs — cross-encoders have their own input format).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def retrieve_and_rerank(query: str, top_k_retrieve: int = 20, top_k_final: int = 5):\n    \"\"\"Two-stage retrieval: bi-encoder retrieve -> cross-encoder rerank.\"\"\"\n\n    # Stage 1: Bi-encoder retrieval (fast) — uses E5 query prefix\n    q_emb = bi_encoder.encode(\n        [format_query(query)], normalize_embeddings=True\n    ).astype(\"float32\")\n    bi_scores, bi_indices = index.search(q_emb, top_k_retrieve)\n\n    # Stage 2: Cross-encoder reranking (accurate)\n    # Cross-encoders take raw (query, document) pairs — no prefix needed\n    pairs = [(query, corpus_df.iloc[idx][\"full_text\"]) for idx in bi_indices[0]]\n    cross_scores = cross_encoder.predict(pairs)\n\n    # Sort by cross-encoder score (descending)\n    reranked = sorted(\n        zip(bi_indices[0], bi_scores[0], cross_scores),\n        key=lambda x: x[2],\n        reverse=True,\n    )\n\n    results = []\n    for rank, (idx, bi_score, ce_score) in enumerate(reranked[:top_k_final], 1):\n        results.append({\n            \"rank\": rank,\n            \"doc_id\": corpus_df.iloc[idx][\"doc_id\"],\n            \"bi_score\": float(bi_score),\n            \"ce_score\": float(ce_score),\n            \"title\": corpus_df.iloc[idx][\"title\"],\n            \"text\": corpus_df.iloc[idx][\"full_text\"][:200] + \"...\",\n        })\n    return pd.DataFrame(results)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def bi_encoder_only(query: str, top_k: int = 5):\n    \"\"\"Bi-encoder retrieval only (no reranking).\"\"\"\n    q_emb = bi_encoder.encode(\n        [format_query(query)], normalize_embeddings=True\n    ).astype(\"float32\")\n    scores, indices = index.search(q_emb, top_k)\n    results = []\n    for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n        results.append({\n            \"rank\": rank,\n            \"doc_id\": corpus_df.iloc[idx][\"doc_id\"],\n            \"score\": float(score),\n            \"title\": corpus_df.iloc[idx][\"title\"],\n            \"text\": corpus_df.iloc[idx][\"full_text\"][:200] + \"...\",\n        })\n    return pd.DataFrame(results)\n\n# Test query\nquery = \"What are the risk factors for developing lung cancer?\"\n\nprint(\"=\" * 70)\nprint(f\"QUERY: {query}\")\nprint(\"=\" * 70)\n\nprint(\"\\n--- Bi-encoder only (top 5) ---\")\nbi_results = bi_encoder_only(query)\nfor _, r in bi_results.iterrows():\n    print(f\"  [{r['rank']}] ({r['score']:.3f}) {r['title']}\")\n\nprint(\"\\n--- Bi-encoder + Cross-encoder reranking (top 5) ---\")\nreranked_results = retrieve_and_rerank(query)\nfor _, r in reranked_results.iterrows():\n    print(f\"  [{r['rank']}] (bi:{r['bi_score']:.3f} -> ce:{r['ce_score']:.3f}) {r['title']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>## 4. Systematic Evaluation\n\n### 4a. Keyword-proxy evaluation\n\nWe start with the same keyword-proxy approach from NB06: a result is \"relevant\" if it contains at least one expected term. This is imperfect but gives a quick intuition."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "eval_queries = [\n    {\"query\": \"How do vaccines protect against viral infections?\",\n     \"relevant_terms\": [\"vaccine\", \"immun\", \"viral\", \"antibod\", \"infection\"]},\n    {\"query\": \"What causes antibiotic resistance in bacteria?\",\n     \"relevant_terms\": [\"antibiotic\", \"resist\", \"bacteria\", \"antimicrobial\"]},\n    {\"query\": \"How does smoking affect lung health?\",\n     \"relevant_terms\": [\"smok\", \"lung\", \"tobacco\", \"cancer\", \"respiratory\"]},\n    {\"query\": \"What role does genetics play in obesity?\",\n     \"relevant_terms\": [\"gene\", \"obes\", \"BMI\", \"weight\", \"metabol\"]},\n    {\"query\": \"How does exercise impact mental health?\",\n     \"relevant_terms\": [\"exercise\", \"mental\", \"depress\", \"anxiety\", \"physical\"]},\n]\n\n\ndef calc_precision(results_df, relevant_terms, text_col=\"text\"):\n    \"\"\"Calculate precision based on keyword matching.\"\"\"\n    relevant = sum(\n        any(term in row[text_col].lower() for term in relevant_terms)\n        for _, row in results_df.iterrows()\n    )\n    return relevant / len(results_df)\n\n\nprint(f\"{'Query':<50} {'Bi-enc P@5':>12} {'Reranked P@5':>14}\")\nprint(\"-\" * 78)\n\nbi_precisions, reranked_precisions = [], []\n\nfor eq in eval_queries:\n    bi_res = bi_encoder_only(eq[\"query\"])\n    reranked_res = retrieve_and_rerank(eq[\"query\"])\n\n    bi_p = calc_precision(bi_res, eq[\"relevant_terms\"])\n    re_p = calc_precision(reranked_res, eq[\"relevant_terms\"])\n    bi_precisions.append(bi_p)\n    reranked_precisions.append(re_p)\n\n    print(f\"  {eq['query'][:48]:<50} {bi_p:>10.0%}   {re_p:>12.0%}\")\n\nprint(f\"\\n{'Average':<50} {np.mean(bi_precisions):>10.0%}   {np.mean(reranked_precisions):>12.0%}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 4b. Ground-truth evaluation with qrels\n\nSciFact comes with **human-annotated relevance judgments** (qrels). For each query, experts have labeled which corpus documents are truly relevant. This lets us compute proper Precision@k without guessing keywords.\n\nThis is what professional IR evaluation looks like — and it is exactly why benchmark datasets like BEIR, MTEB, and TREC are so valuable.\n\n**Important context:** SciFact has very sparse relevance — most queries have only 1 relevant document in the entire corpus. This means:\n\n- **Precision@5 is capped at 20%** for most queries (1 hit in 5 slots)\n- The bi-encoder is already quite good at finding that one needle in the haystack\n- Reranking cannot add new documents — it can only **reorder** what was already retrieved\n\nSo we expect **modest P@k improvements**. The real value of reranking shows up in two ways:\n1. **Recall@k at lower k**: pushing the relevant doc from position 15 to position 3 (measured by NDCG)\n2. **Qualitative ordering**: as we saw in the lung cancer example, irrelevant-but-similar documents get pushed down",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def precision_at_k(retrieved_doc_ids: list[str], relevant_set: set[str], k: int) -> float:\n    \"\"\"Precision@k using ground-truth relevance judgments.\"\"\"\n    hits = sum(1 for doc_id in retrieved_doc_ids[:k] if doc_id in relevant_set)\n    return hits / k\n\n\ndef recall_at_k(retrieved_doc_ids: list[str], relevant_set: set[str], k: int) -> float:\n    \"\"\"Recall@k: fraction of relevant docs found in top k.\"\"\"\n    if not relevant_set:\n        return 0.0\n    hits = sum(1 for doc_id in retrieved_doc_ids[:k] if doc_id in relevant_set)\n    return hits / len(relevant_set)\n\n\ndef ndcg_at_k(retrieved_doc_ids: list[str], relevant_set: set[str], k: int) -> float:\n    \"\"\"NDCG@k — the standard metric for ranking quality.\n\n    Unlike Precision@k which only asks 'is the relevant doc in the top k?',\n    NDCG rewards placing it at rank 1 much more than rank 5.\n    This is exactly what reranking is designed to improve.\n    \"\"\"\n    dcg = 0.0\n    for i, doc_id in enumerate(retrieved_doc_ids[:k]):\n        if doc_id in relevant_set:\n            dcg += 1.0 / np.log2(i + 2)  # rank is 1-indexed, so i+2\n    # Ideal DCG: all relevant docs at the top\n    ideal_hits = min(len(relevant_set), k)\n    idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n    return dcg / idcg if idcg > 0 else 0.0\n\n\n# Evaluate on all queries that have relevance judgments\nbi_p5, bi_p10, bi_r20, bi_ndcg10 = [], [], [], []\nre_p5, re_p10, re_r20, re_ndcg10 = [], [], [], []\n\nfor _, row in queries_df.iterrows():\n    qid = row[\"query_id\"]\n    if qid not in relevant_docs:\n        continue\n\n    query_text = row[\"query\"]\n    gold = relevant_docs[qid]\n\n    # Bi-encoder top-20\n    bi_res = bi_encoder_only(query_text, top_k=20)\n    bi_ids = bi_res[\"doc_id\"].tolist()\n\n    # Reranked top-20 (retrieve 50, rerank to 20)\n    re_res = retrieve_and_rerank(query_text, top_k_retrieve=50, top_k_final=20)\n    re_ids = re_res[\"doc_id\"].tolist()\n\n    bi_p5.append(precision_at_k(bi_ids, gold, 5))\n    bi_p10.append(precision_at_k(bi_ids, gold, 10))\n    bi_r20.append(recall_at_k(bi_ids, gold, 20))\n    bi_ndcg10.append(ndcg_at_k(bi_ids, gold, 10))\n\n    re_p5.append(precision_at_k(re_ids, gold, 5))\n    re_p10.append(precision_at_k(re_ids, gold, 10))\n    re_r20.append(recall_at_k(re_ids, gold, 20))\n    re_ndcg10.append(ndcg_at_k(re_ids, gold, 10))\n\nprint(f\"Ground-truth evaluation on {len(bi_p5)} queries with relevance labels\\n\")\nprint(f\"{'Metric':<20} {'Bi-encoder':>12} {'+ Reranking':>12} {'Delta':>10}\")\nprint(\"-\" * 56)\nfor name, bi_vals, re_vals in [\n    (\"Precision@5\", bi_p5, re_p5),\n    (\"Precision@10\", bi_p10, re_p10),\n    (\"NDCG@10\", bi_ndcg10, re_ndcg10),\n    (\"Recall@20\", bi_r20, re_r20),\n]:\n    bm, rm = np.mean(bi_vals), np.mean(re_vals)\n    print(f\"  {name:<18} {bm:>10.1%}   {rm:>10.1%}   {rm - bm:>+9.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Interpreting the results\n\n**Why are the P@k deltas so modest?** This is expected and educational:\n\n1. **Sparsity**: 91% of SciFact queries have exactly 1 relevant document. With 1 relevant doc, P@5 is either 0% or 20% — there's no room for nuance.\n\n2. **The bi-encoder is already good**: E5-small finds the relevant doc in the top 20 for ~85% of queries. For those queries, reranking can help; for the other 15%, the relevant doc was never retrieved, and reranking cannot conjure it.\n\n3. **NDCG@10 tells the real story**: Unlike precision (which is binary — \"is it in the top 5?\"), NDCG rewards pushing a relevant doc from rank 8 to rank 1. This is exactly what cross-encoder reranking does, and is why NDCG is the standard metric in retrieval benchmarks like MTEB and TREC.\n\n**In production systems with richer relevance** (e.g., Google, where many results could be relevant), reranking gains are typically much larger (5–15% NDCG improvement). SciFact is a conservative test case — if reranking helps here, it definitely helps in practice.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Speed vs Accuracy Tradeoff\n",
    "\n",
    "Cross-encoder reranking adds latency. Let's measure exactly how much, so we can make informed decisions about when the tradeoff is worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"effects of air pollution on respiratory health\"\n",
    "\n",
    "# Time bi-encoder only\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    bi_encoder_only(query)\n",
    "bi_time = (time.time() - start) / 10\n",
    "\n",
    "# Time reranking pipeline\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    retrieve_and_rerank(query)\n",
    "rerank_time = (time.time() - start) / 10\n",
    "\n",
    "print(f\"Bi-encoder only:  {bi_time*1000:.1f} ms/query\")\n",
    "print(f\"With reranking:   {rerank_time*1000:.1f} ms/query\")\n",
    "print(f\"Reranking overhead: {(rerank_time-bi_time)*1000:.1f} ms ({rerank_time/bi_time:.1f}x slower)\")\n",
    "print(f\"\\nFor social science research, this tradeoff is usually worth it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise: Tune the Pipeline\n",
    "\n",
    "Experiment with the pipeline to understand how its components affect performance:\n",
    "\n",
    "1. **Vary `top_k_retrieve`**: Try values of 10, 20, 50, and 100. How does the number of bi-encoder candidates affect final precision and latency?\n",
    "2. **Try a different cross-encoder**: Replace `cross-encoder/ms-marco-MiniLM-L-6-v2` with another model (e.g., `cross-encoder/ms-marco-TinyBERT-L-2-v2` for speed, or `cross-encoder/ms-marco-MiniLM-L-12-v2` for accuracy).\n",
    "3. **Add your own queries**: Think of a social science research question and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Experiment 1: Try different top_k_retrieve values\n",
    "# for k in [10, 20, 50, 100]:\n",
    "#     results = retrieve_and_rerank(\"your query here\", top_k_retrieve=k)\n",
    "#     print(f\"top_k_retrieve={k}: ...\")\n",
    "\n",
    "# Experiment 2: Try a different cross-encoder model\n",
    "# cross_encoder_v2 = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n",
    "# ...\n",
    "\n",
    "# Experiment 3: Add your own social science query\n",
    "# my_query = \"...\"\n",
    "# retrieve_and_rerank(my_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Bonus: Deploy as a Gradio App\n\nLet's turn our two-stage retrieval pipeline into an interactive search interface. Users can type natural-language queries and see reranked results instantly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "try:\n    !pip install gradio -q\n    import gradio as gr\n\n    def search_and_rerank(query, top_k=5):\n        \"\"\"Search and rerank, returning formatted markdown results.\"\"\"\n        if not query.strip():\n            return \"Please enter a search query.\"\n        results = retrieve_and_rerank(query, top_k_retrieve=20, top_k_final=int(top_k))\n        output = f\"## Results for: *{query}*\\n\\n\"\n        for _, row in results.iterrows():\n            output += f\"**[{row['rank']}]** (score: {row['ce_score']:.3f}) **{row['title']}**\\n\\n\"\n            output += f\"> {row['text'][:150]}...\\n\\n---\\n\\n\"\n        return output\n\n    demo = gr.Interface(\n        fn=search_and_rerank,\n        inputs=[\n            gr.Textbox(lines=2, placeholder=\"Enter a search query...\"),\n            gr.Slider(minimum=1, maximum=10, value=5, step=1, label=\"Number of results\"),\n        ],\n        outputs=gr.Markdown(label=\"Search Results\"),\n        title=\"Semantic Search with Reranking\",\n        description=\"Two-stage retrieval: E5 bi-encoder (fast) + cross-encoder (accurate). Search SciFact scientific abstracts.\",\n        examples=[[\"How do vaccines protect against infections?\"], [\"genetic factors in cancer risk\"]],\n    )\n    demo.launch(share=True)\n\nexcept ImportError:\n    print(\"Gradio not available. Install with: pip install gradio\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Takeaways\n",
    "\n",
    "**Two-stage retrieval is the standard approach in production search systems.** Nearly every modern search engine -- from Google to academic paper search -- uses some form of this pattern.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- **Bi-encoders** encode queries and documents independently. They are fast and scalable (millions of documents) but miss fine-grained query-document interactions.\n",
    "- **Cross-encoders** process the query and document together. They are much more accurate but too slow to apply to an entire corpus.\n",
    "- **The two-stage pipeline** combines both: bi-encoder for fast candidate retrieval, cross-encoder for precise reranking. This gives us speed *and* accuracy.\n",
    "- **The pipeline is modular**: you can swap out the bi-encoder, the cross-encoder, the vector index, or the candidate pool size independently. This makes it easy to experiment and improve.\n",
    "\n",
    "**Social science applications:**\n",
    "\n",
    "- **Finding relevant policy documents** in large government archives\n",
    "- **Academic paper retrieval** for literature reviews\n",
    "- **Case law search** in legal research\n",
    "- **Survey response matching** for qualitative analysis\n",
    "- **Media analysis** -- finding relevant news articles on specific social issues\n",
    "\n",
    "In the next notebook, we will look at how to fine-tune these models on domain-specific data to further improve retrieval quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}