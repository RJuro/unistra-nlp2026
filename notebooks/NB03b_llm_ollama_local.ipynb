{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB03b: LLM Zero-shot Classification + Structured Output (Local Ollama)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB03b_llm_ollama_local.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**This is the local/offline variant of NB03.** Instead of calling a cloud API (Groq), we run an LLM locally using [Ollama](https://ollama.com).\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "- **Run a local LLM** with Ollama — no API key, no data leaves your machine\n",
    "- **Use LLMs as classifiers without training data** — zero-shot classification\n",
    "- **Enforce structured output** — Ollama's native `format` parameter guarantees valid JSON matching your schema\n",
    "- **Extract structured data from text** — turn articles into typed records\n",
    "- **Compare local vs cloud** — understand the trade-offs\n",
    "\n",
    "**Model:** `ministral-3:8b` — Mistral's 8B edge-optimized model (Apache 2.0, 256K context, 6GB)\n",
    "\n",
    "**Estimated time:** ~50 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Setup ────────────────────────────────────────────────────────────\n",
    "!pip install ollama pydantic pandas scikit-learn tqdm -q\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import subprocess\n",
    "import requests\n",
    "\n",
    "import ollama\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "MODEL_NAME = \"ministral-3:8b\"\n",
    "\n",
    "print(\"All imports successful.\")\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ── Install Ollama (Colab only — skip if running locally) ───────────\n# Uncomment the next two lines if running in Google Colab:\n# !sudo apt-get install -y zstd pciutils -qq\n# !curl -fsSL https://ollama.com/install.sh | sh"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def start_ollama():\n    \"\"\"Start the Ollama server and wait until it's ready.\"\"\"\n    # Set CUDA paths so Ollama can find the GPU in Colab\n    os.environ['PATH'] = os.environ.get('PATH', '') + ':/usr/local/cuda/bin'\n    os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n\n    # Check if already running\n    try:\n        r = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n        if r.status_code == 200:\n            print(\"Ollama server already running.\")\n            return None\n    except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):\n        pass\n\n    # Start the server\n    p = subprocess.Popen(\n        [\"ollama\", \"serve\"],\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL\n    )\n    print(\"Starting Ollama server...\")\n\n    for _ in range(30):\n        try:\n            r = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n            if r.status_code == 200:\n                print(\"Ollama server is ready.\")\n                return p\n        except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):\n            time.sleep(1)\n\n    raise RuntimeError(\"Ollama server did not start within 30 seconds.\")\n\n\nserver_process = start_ollama()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_ollama():\n",
    "    \"\"\"Start the Ollama server and wait until it's ready.\"\"\"\n",
    "    # Check if already running\n",
    "    try:\n",
    "        r = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(\"Ollama server already running.\")\n",
    "            return None\n",
    "    except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):\n",
    "        pass\n",
    "\n",
    "    # Start the server\n",
    "    p = subprocess.Popen(\n",
    "        [\"ollama\", \"serve\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    print(\"Starting Ollama server...\")\n",
    "\n",
    "    for _ in range(30):\n",
    "        try:\n",
    "            r = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "            if r.status_code == 200:\n",
    "                print(\"Ollama server is ready.\")\n",
    "                return p\n",
    "        except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):\n",
    "            time.sleep(1)\n",
    "\n",
    "    raise RuntimeError(\"Ollama server did not start within 30 seconds.\")\n",
    "\n",
    "\n",
    "server_process = start_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Pull the model (downloads ~6GB on first run) ──────────────────\n",
    "print(f\"Pulling {MODEL_NAME}...\")\n",
    "!ollama pull {MODEL_NAME}\n",
    "print(\"\\nInstalled models:\")\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Quick test ────────────────────────────────────────────────────────\n",
    "test_resp = ollama.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'API working!' in exactly 2 words.\"}],\n",
    "    stream=False\n",
    ")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Response: {test_resp['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Zero-shot Classification (25 min)\n",
    "\n",
    "Same task as NB03 cloud version: classify advice posts into 8 categories using **zero training data**.\n",
    "\n",
    "The difference: instead of `client.chat.completions.create()` (OpenAI API), we use `ollama.chat()` with native schema enforcement via the `format` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "Same **dk_posts** dataset from NB01 and NB02 — 457 English advice posts across 8 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load data from GitHub ────────────────────────────────────────────\n",
    "DATA_URL = \"https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/data/dk_posts_synth_en_processed.json\"\n",
    "\n",
    "df = pd.read_json(DATA_URL)\n",
    "\n",
    "# ── Same preprocessing as NB01 ──────────────────────────────────────\n",
    "df[\"text\"] = df[\"title\"] + \" . \" + df[\"selftext\"]\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, strip, and collapse whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\n── Label distribution ──\")\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Output Schema\n",
    "\n",
    "Ollama supports **native structured output** via the `format` parameter. You pass a JSON schema and Ollama uses constrained decoding to guarantee the output matches. This is equivalent to Groq's `json_schema` mode — but available for *any* Ollama model.\n",
    "\n",
    "We define our schema with Pydantic, then pass `Model.model_json_schema()` to `format=`.\n",
    "\n",
    "| | Groq (NB03) | Ollama (this notebook) |\n",
    "|---|---|---|\n",
    "| Schema enforcement | `response_format={\"type\": \"json_schema\", ...}` | `format=Model.model_json_schema()` |\n",
    "| Constrained decoding | Only select models | All models |\n",
    "| Validation | Post-hoc with Pydantic | Post-hoc with Pydantic (belt + suspenders) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Define categories and schema ────────────────────────────────────\n",
    "CATEGORIES = [\n",
    "    \"Love & Dating\",\n",
    "    \"Family Dynamics\",\n",
    "    \"Work, Study & Career\",\n",
    "    \"Friendship & Social Life\",\n",
    "    \"Health & Wellness (Physical and Mental)\",\n",
    "    \"Personal Finance & Housing\",\n",
    "    \"Practical Questions & Everyday Life\",\n",
    "    \"Everyday Observations & Rants\"\n",
    "]\n",
    "\n",
    "\n",
    "class SingleLabelPrediction(BaseModel):\n",
    "    predicted_label: Literal[\n",
    "        \"Love & Dating\",\n",
    "        \"Family Dynamics\",\n",
    "        \"Work, Study & Career\",\n",
    "        \"Friendship & Social Life\",\n",
    "        \"Health & Wellness (Physical and Mental)\",\n",
    "        \"Personal Finance & Housing\",\n",
    "        \"Practical Questions & Everyday Life\",\n",
    "        \"Everyday Observations & Rants\"\n",
    "    ] = Field(description=\"The single best-fit category for this post.\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score 0-1\")\n",
    "\n",
    "\n",
    "print(\"Schema defined: SingleLabelPrediction\")\n",
    "print(f\"Categories: {len(CATEGORIES)}\")\n",
    "print(f\"\\nOllama format parameter will use:\")\n",
    "print(json.dumps(SingleLabelPrediction.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying a Single Post\n",
    "\n",
    "The key difference from NB03: we use `ollama.chat()` with `format=SingleLabelPrediction.model_json_schema()`. Ollama's constrained decoding guarantees the output matches our schema — every field, every type, every enum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_post(text: str) -> Optional[SingleLabelPrediction]:\n",
    "    \"\"\"Classify a single post using the local LLM with schema-enforced output.\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        f\"You classify personal advice posts into exactly one of these categories: {CATEGORIES}. \"\n",
    "                        \"Return JSON with exactly two fields: 'predicted_label' (one of the categories above) \"\n",
    "                        \"and 'confidence' (a float between 0 and 1).\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Classify this post:\\n\\n{text}\"\n",
    "                }\n",
    "            ],\n",
    "            format=SingleLabelPrediction.model_json_schema(),\n",
    "            stream=False\n",
    "        )\n",
    "        result = SingleLabelPrediction.model_validate_json(\n",
    "            response[\"message\"][\"content\"]\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test on one example\n",
    "sample = df.iloc[0]\n",
    "print(f\"Text: {sample['text_clean'][:100]}...\")\n",
    "print(f\"True label: {sample['label']}\")\n",
    "\n",
    "result = classify_post(sample[\"text_clean\"])\n",
    "if result:\n",
    "    print(f\"Predicted: {result.predicted_label} (confidence: {result.confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Classification\n",
    "\n",
    "Local inference has no rate limits — the bottleneck is your GPU speed. On a T4, expect ~1-3 seconds per post for an 8B model. We classify 50 posts to match the NB03 comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Classify a sample ────────────────────────────────────────────────\n# Use a shuffled random sample instead of head() to avoid sampling bias\n# (first rows tend to be clearer examples, which can inflate accuracy)\nsample_df = df.sample(n=50, random_state=42).reset_index(drop=True).copy()\npredictions = []\n\nfor idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Classifying\"):\n    result = classify_post(row[\"text_clean\"])\n    predictions.append({\n        \"true_label\": row[\"label\"],\n        \"predicted_label\": result.predicted_label if result else \"Error\",\n        \"confidence\": result.confidence if result else 0.0\n    })\n    # No sleep needed — local inference, no rate limits!\n\npred_df = pd.DataFrame(predictions)\nvalid = pred_df[pred_df.predicted_label != \"Error\"]\n\nprint(f\"\\nSuccessful predictions: {len(valid)}/{len(pred_df)}\")\npred_df.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Evaluate ─────────────────────────────────────────────────────────\n",
    "acc = accuracy_score(valid[\"true_label\"], valid[\"predicted_label\"])\n",
    "print(f\"\\nZero-shot LLM Accuracy (local, {MODEL_NAME}): {acc:.1%}\")\n",
    "print(f\"(on {len(valid)}/{len(pred_df)} successful predictions)\\n\")\n",
    "print(classification_report(valid[\"true_label\"], valid[\"predicted_label\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement: Few-shot with Integer IDs\n",
    "\n",
    "Just like in the old ollama notebook, we can improve performance by:\n",
    "1. Giving the model a labeled example (few-shot)\n",
    "2. Asking for a compact integer ID instead of the full label string\n",
    "\n",
    "This reduces output tokens and makes the task clearer for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Category mappings ────────────────────────────────────────────────\n",
    "CATEGORY_MAP = {name: i for i, name in enumerate(CATEGORIES)}\n",
    "ID_TO_CATEGORY = {i: name for name, i in CATEGORY_MAP.items()}\n",
    "\n",
    "CATEGORY_DESCRIPTIONS = {\n",
    "    0: \"Romantic relationships, dating, partners, jealousy, breakups.\",\n",
    "    1: \"Family issues like siblings, parents, children, boundaries.\",\n",
    "    2: \"Job, coworkers, boss, stress at work, studies, exams, career.\",\n",
    "    3: \"Friends, loneliness, social life, making/keeping friends.\",\n",
    "    4: \"Physical or mental health, stress, anxiety, pain, symptoms.\",\n",
    "    5: \"Money, rent, moving in, shared finances, budgeting, housing.\",\n",
    "    6: \"Practical everyday how-to questions (cleaning, chores, tips).\",\n",
    "    7: \"Annoyances/rants about other people's behavior in daily life.\"\n",
    "}\n",
    "\n",
    "\n",
    "class MinimalPrediction(BaseModel):\n",
    "    \"\"\"Compact schema — just an integer ID.\"\"\"\n",
    "    id: int = Field(description=\"Category ID (0-7)\", ge=0, le=7)\n",
    "\n",
    "\n",
    "# One-shot example\n",
    "EXAMPLE_POST = {\n",
    "    \"title\": \"Partner won't meet my friends\",\n",
    "    \"text\": \"I've been with my boyfriend for almost a year, and he still hasn't met my friends.\",\n",
    "    \"label\": \"Love & Dating\"\n",
    "}\n",
    "EXAMPLE_ID = CATEGORY_MAP[EXAMPLE_POST[\"label\"]]\n",
    "\n",
    "print(\"Category mappings:\")\n",
    "for cat_id, name in ID_TO_CATEGORY.items():\n",
    "    print(f\"  {cat_id}: {name} — {CATEGORY_DESCRIPTIONS[cat_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_fast(text: str, title: str = \"\") -> Optional[int]:\n",
    "    \"\"\"Few-shot classifier returning a category ID (0-7).\"\"\"\n",
    "    categories_text = \"\\n\".join(\n",
    "        f\"{cid}: {ID_TO_CATEGORY[cid]} — {CATEGORY_DESCRIPTIONS[cid]}\"\n",
    "        for cid in ID_TO_CATEGORY\n",
    "    )\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a fast text classifier. \"\n",
    "        \"Assign EXACTLY ONE category ID to the post. \"\n",
    "        \"Return ONLY valid JSON with one field 'id'. \"\n",
    "        \"No explanations.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"--- CATEGORY DEFINITIONS ---\n",
    "{categories_text}\n",
    "\n",
    "--- EXAMPLE ---\n",
    "Title: {EXAMPLE_POST['title']}\n",
    "Text: {EXAMPLE_POST['text']}\n",
    "Output: {{\"id\": {EXAMPLE_ID}}}\n",
    "\n",
    "--- NEW POST ---\n",
    "Title: {title}\n",
    "Text: {text}\n",
    "Output:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            format=MinimalPrediction.model_json_schema(),\n",
    "            stream=False\n",
    "        )\n",
    "        parsed = MinimalPrediction.model_validate_json(response[\"message\"][\"content\"])\n",
    "        return parsed.id\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Quick test\n",
    "test_id = classify_fast(df.iloc[0][\"text_clean\"], df.iloc[0].get(\"title\", \"\"))\n",
    "print(f\"Predicted ID: {test_id} -> {ID_TO_CATEGORY.get(test_id, 'Unknown')}\")\n",
    "print(f\"True label: {df.iloc[0]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Few-shot batch classification ────────────────────────────────────\n",
    "fewshot_preds = []\n",
    "\n",
    "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Few-shot\"):\n",
    "    pred_id = classify_fast(row[\"text_clean\"], row.get(\"title\", \"\"))\n",
    "    fewshot_preds.append({\n",
    "        \"true_label\": row[\"label\"],\n",
    "        \"predicted_id\": pred_id,\n",
    "        \"predicted_label\": ID_TO_CATEGORY.get(pred_id, \"Error\") if pred_id is not None else \"Error\"\n",
    "    })\n",
    "\n",
    "fs_df = pd.DataFrame(fewshot_preds)\n",
    "fs_valid = fs_df[fs_df.predicted_label != \"Error\"]\n",
    "\n",
    "fs_acc = accuracy_score(fs_valid[\"true_label\"], fs_valid[\"predicted_label\"])\n",
    "print(f\"\\n── Performance Comparison ──\")\n",
    "print(f\"Zero-shot accuracy:  {acc:.1%}\")\n",
    "print(f\"Few-shot accuracy:   {fs_acc:.1%}\")\n",
    "print(f\"\\nFew-shot classification report:\")\n",
    "print(classification_report(fs_valid[\"true_label\"], fs_valid[\"predicted_label\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Show misclassifications ──────────────────────────────────────────\n",
    "misclassified = fs_df[fs_df[\"true_label\"] != fs_df[\"predicted_label\"]].copy()\n",
    "misclassified = misclassified.merge(\n",
    "    sample_df[[\"title\"]].reset_index(),\n",
    "    left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Misclassified: {len(misclassified)}/{len(fs_df)}\")\n",
    "if len(misclassified) > 0:\n",
    "    display(misclassified[[\"title\", \"true_label\", \"predicted_label\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Comparison: Local vs Cloud\n\n| | NB03 (Groq Cloud) | NB03b (Ollama Local) |\n|---|---|---|\n| Model | moonshotai/kimi-k2-instruct | ministral-3:8b |\n| API key required | Yes (free) | No |\n| Data privacy | Sent to API | Stays on your machine |\n| Rate limits | 1K req/day | Unlimited |\n| Speed per post | ~0.2s | ~1-3s (GPU dependent) |\n| GPU required | No (server-side) | Yes (T4 minimum) |\n| Cost | Free tier / pay per token | Free (your hardware) |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Structured Extraction (25 min)\n",
    "\n",
    "Same goal as NB03 Part B: extract multiple structured fields from an article. With Ollama, `format=` works the same way for complex nested schemas — constrained decoding guarantees compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\"\"\n",
    "MIT researchers have published a study questioning the long-term viability of scaling \n",
    "large language models. The paper, authored by Dr. Sarah Chen and colleagues at MIT's \n",
    "Computer Science and Artificial Intelligence Laboratory (CSAIL), suggests that the \n",
    "current approach of training ever-larger models is hitting diminishing returns.\n",
    "\n",
    "The study analyzed performance curves across recent frontier models and found that \n",
    "doubling model size no longer produces the breakthroughs seen two or three generations \n",
    "ago. Instead, gains are increasingly coming from smarter training approaches, \n",
    "architectural innovations, and inference-time optimizations that squeeze more \n",
    "performance out of smaller systems.\n",
    "\n",
    "The researchers point to the emergence of highly efficient models like DeepSeek, which \n",
    "demonstrated in early 2025 that competitive reasoning and coding capabilities could be \n",
    "achieved at a fraction of the compute cost of larger rivals. This challenges the \n",
    "prevailing Silicon Valley strategy of massive GPU cluster buildouts.\n",
    "\n",
    "Meanwhile, companies like OpenAI and major hyperscalers are committing hundreds of \n",
    "billions of dollars to long-term data center and energy infrastructure deals. Economists \n",
    "quoted in the report warn this resembles a speculative bubble, with enormous capital \n",
    "intensity and uncertain returns. If efficiency innovation continues to outpace brute-force \n",
    "scaling, the industry's current infrastructure investments may significantly overshoot \n",
    "actual demand.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Article length: {len(article_text)} characters\")\n",
    "print(article_text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Define the extraction schema ─────────────────────────────────────\n",
    "class ArticleAnalysis(BaseModel):\n",
    "    title: str = Field(description=\"A concise title for the article\")\n",
    "    summary: str = Field(description=\"2-3 sentence summary\")\n",
    "    institutions: List[str] = Field(description=\"Organizations mentioned\")\n",
    "    key_claims: List[str] = Field(description=\"Main claims or findings (3-5)\")\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\", \"mixed\"] = Field(\n",
    "        description=\"Overall sentiment\"\n",
    "    )\n",
    "    topics: List[str] = Field(description=\"Main topics discussed\")\n",
    "\n",
    "\n",
    "print(\"Extraction schema:\")\n",
    "print(json.dumps(ArticleAnalysis.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extract with retry ───────────────────────────────────────────────\n",
    "def extract_with_retry(text: str, schema, max_retries: int = 3):\n",
    "    \"\"\"Extract structured data from text using Ollama with schema enforcement.\n",
    "\n",
    "    Uses the `format` parameter for constrained decoding.\n",
    "    Retries on transient errors with exponential backoff.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"Extract structured information from the text. \"\n",
    "                            \"Do not invent facts not present in the text.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ],\n",
    "                format=schema.model_json_schema(),\n",
    "                stream=False\n",
    "            )\n",
    "            return schema.model_validate_json(\n",
    "                response[\"message\"][\"content\"]\n",
    "            )\n",
    "        except (ValidationError, Exception) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Run extraction\n",
    "result = extract_with_retry(article_text, ArticleAnalysis)\n",
    "\n",
    "if result:\n",
    "    print(json.dumps(result.model_dump(), indent=2))\n",
    "else:\n",
    "    print(\"Extraction failed after all retries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Extraction to Analysis\n",
    "\n",
    "Once you have validated Pydantic objects, they plug directly into DataFrames, databases, or dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Turn extraction into a DataFrame ─────────────────────────────────\n",
    "if result:\n",
    "    results_data = [result.model_dump()]\n",
    "    analysis_df = pd.DataFrame(results_data)\n",
    "\n",
    "    print(\"── Extracted Data as DataFrame ──\\n\")\n",
    "    for col in analysis_df.columns:\n",
    "        print(f\"{col}: {analysis_df[col].iloc[0]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No data to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Design Your Own Schema\n",
    "\n",
    "Design a Pydantic schema to extract structured information from a text of your choice. Some ideas:\n",
    "\n",
    "- **Movie review:** title, rating (1-5), pros, cons, recommendation (yes/no)\n",
    "- **Job posting:** company, role, required skills, salary range, location\n",
    "- **Recipe:** dish name, ingredients list, prep time, difficulty\n",
    "\n",
    "With Ollama, the `format=` parameter works with any schema — no model restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise: Design your own schema ─────────────────────────────────\n",
    "\n",
    "# Step 1: Define your schema\n",
    "# class MySchema(BaseModel):\n",
    "#     # YOUR CODE HERE\n",
    "#     pass\n",
    "\n",
    "# Step 2: Provide a text to extract from\n",
    "# my_text = \"\"\"\n",
    "# YOUR TEXT HERE\n",
    "# \"\"\"\n",
    "\n",
    "# Step 3: Run extraction — same function works with any schema!\n",
    "# my_result = extract_with_retry(my_text, MySchema)\n",
    "# if my_result:\n",
    "#     print(json.dumps(my_result.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis\n",
    "\n",
    "Running locally changes the cost equation entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cost comparison ──────────────────────────────────────────────────\nprint(\"Cost Analysis: Local (Ollama) vs Cloud (Groq)\")\nprint(f\"\\n  Local ({MODEL_NAME}):\")\nprint(f\"    API cost: $0.00 — always free\")\nprint(f\"    Rate limits: None — limited by your GPU speed\")\nprint(f\"    Privacy: Full — data never leaves your machine\")\nprint(f\"    Hardware: Needs GPU (T4 = free Colab, or local GPU)\")\nprint(f\"    Speed: ~1-3s per post (8B model on T4)\")\nprint(f\"\\n  Cloud (Groq, moonshotai/kimi-k2-instruct):\")\nprint(f\"    API cost: $0.00 on free tier\")\nprint(f\"    Rate limits: 1,000 requests/day\")\nprint(f\"    Privacy: Data sent to Groq servers\")\nprint(f\"    Hardware: None needed (server-side)\")\nprint(f\"    Speed: ~0.1-0.3s per post\")\nprint(f\"\\n  When to use local:\")\nprint(f\"    - Sensitive data (medical, legal, financial)\")\nprint(f\"    - No internet access\")\nprint(f\"    - Need to process >1K items/day\")\nprint(f\"    - Want full control over the model\")\nprint(f\"\\n  When to use cloud:\")\nprint(f\"    - No GPU available\")\nprint(f\"    - Need fastest possible inference\")\nprint(f\"    - Quick prototyping\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stop the Ollama server (if we started it) ────────────────────────\n",
    "if server_process is not None:\n",
    "    server_process.terminate()\n",
    "    server_process.wait()\n",
    "    print(\"Ollama server stopped.\")\n",
    "else:\n",
    "    print(\"Server was already running — not stopping it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Ollama makes local LLM deployment simple.** Install, pull a model, and you have a local inference server. No API keys, no data leaves your machine.\n",
    "\n",
    "2. **Schema enforcement via `format=` works with any Ollama model.** Unlike cloud APIs where only select models support structured outputs, Ollama's constrained decoding works universally. Pass `format=Model.model_json_schema()` and the output is guaranteed to match.\n",
    "\n",
    "3. **Few-shot + integer IDs improve accuracy.** Giving the model one example and asking for a compact output (integer ID instead of full label) makes classification faster and more reliable.\n",
    "\n",
    "4. **Same Pydantic schemas work everywhere.** We used the same `SingleLabelPrediction` and `ArticleAnalysis` schemas from NB03 — only the inference call changed (`ollama.chat` vs `client.chat.completions.create`).\n",
    "\n",
    "5. **Choose based on your constraints:**\n",
    "\n",
    "| | NB03: Groq Cloud | NB03b: Ollama Local |\n",
    "|---|---|---|\n",
    "| Best for | Quick prototyping, no GPU | Sensitive data, unlimited volume |\n",
    "| Schema enforcement | Model-dependent | Universal (any model) |\n",
    "| Speed | Fast (~0.2s) | Slower (~1-3s) |\n",
    "| Privacy | Data sent to API | Full privacy |\n",
    "| Rate limits | Yes (free tier) | None |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **NB04** we explore **unsupervised topic discovery** — finding structure in text when we don't even know what the categories should be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}