{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB02: Sentence Embeddings -- Universal Features\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB02_sentence_embeddings.ipynb)\n",
    "\n",
    "**Duration:** ~85 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Understand sentence embeddings** -- what they are, how they differ from sparse representations like TF-IDF, and why they matter.\n",
    "2. **Use Sentence-BERT (SBERT)** to encode text into dense vector representations and use them for downstream classification.\n",
    "3. **Compare embedding-based classifiers to TF-IDF baselines** on the same dataset and the same train/test split.\n",
    "4. **Build label-efficiency curves** to see how embeddings outperform TF-IDF when labeled data is scarce.\n",
    "5. **Visualize the embedding space** with t-SNE to understand how well classes separate.\n",
    "\n",
    "---\n",
    "\n",
    "| Section | Topic | Approx. Time |\n",
    "|---------|-------|---------------|\n",
    "| 1 | Quick Recap: The Dataset | 5 min |\n",
    "| 2 | What Are Sentence Embeddings? | 10 min |\n",
    "| 3 | Encoding Text with SBERT | 10 min |\n",
    "| 4 | Classification with Embeddings | 10 min |\n",
    "| 5 | Head-to-Head: TF-IDF vs SBERT | 15 min |\n",
    "| 6 | Label Efficiency Curves | 15 min |\n",
    "| 7 | Visualizing the Embedding Space | 10 min |\n",
    "| 8 | Exercise: Try a Different Model | 10 min |\n",
    "| 9 | Summary & Takeaways | -- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Setup ----\n",
    "!pip install sentence-transformers scikit-learn pandas numpy matplotlib seaborn -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Recap: The Dataset\n",
    "\n",
    "We use the same **dk_posts** dataset from NB01 -- synthetic Reddit-style posts in English, each labeled with one of 8 life-topic categories. We apply the identical preprocessing and train/test split so that every accuracy number in this notebook is directly comparable to the TF-IDF baselines from NB01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load dataset (same as NB01) ----\n",
    "url = \"https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/data/dk_posts_synth_en_processed.json\"\n",
    "df = pd.read_json(url, orient=\"records\")\n",
    "\n",
    "# Combine title + selftext into a single feature\n",
    "df[\"text\"] = df[\"title\"] + \" . \" + df[\"selftext\"]\n",
    "\n",
    "# Simple text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Labels ({df['label'].nunique()}): {df['label'].unique().tolist()}\")\n",
    "df[[\"text_clean\", \"label\"]].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Train / Test split (identical to NB01 for fair comparison) ----\n",
    "X = df[\"text_clean\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples:  {len(X_test)}\")\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What Are Sentence Embeddings?\n",
    "\n",
    "In NB01 we turned text into numbers with **TF-IDF** -- a *sparse*, high-dimensional vector where each dimension corresponds to one word (or n-gram) in the vocabulary. TF-IDF is fast and interpretable, but it has a fundamental limitation: it treats every word as independent. The words *\"happy\"* and *\"joyful\"* end up as completely separate dimensions with no connection between them.\n",
    "\n",
    "**Sentence embeddings** take a radically different approach. A pre-trained neural network reads the entire sentence and compresses it into a single **dense vector** -- typically 384 to 1024 floating-point numbers. These vectors are trained so that sentences with similar meaning end up close together in the embedding space, even if they use completely different words. For example:\n",
    "\n",
    "- *\"I feel so lonely after moving to a new city\"*\n",
    "- *\"Making friends as an adult is really hard\"*\n",
    "\n",
    "would receive vectors that are very close to each other, because the model has learned from millions of sentence pairs that these sentences express related ideas.\n",
    "\n",
    "### Sentence-BERT (SBERT)\n",
    "\n",
    "**SBERT** (Reimers & Gurevych, 2019) is a modification of the BERT architecture that is optimized for producing high-quality sentence embeddings efficiently. Instead of using BERT's `[CLS]` token (which was not designed for sentence similarity), SBERT adds a pooling layer and is fine-tuned on sentence-pair tasks (Natural Language Inference, Semantic Textual Similarity) so that the resulting vectors are directly comparable via cosine similarity.\n",
    "\n",
    "We will use the `all-MiniLM-L6-v2` model -- a compact (80 MB) model that produces 384-dimensional embeddings and strikes an excellent balance between speed and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding Text with SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load pre-trained SBERT model ----\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ---- Encode training and test texts ----\n",
    "print(\"Encoding training texts...\")\n",
    "X_train_emb = model.encode(X_train.tolist(), show_progress_bar=True)\n",
    "\n",
    "print(\"Encoding test texts...\")\n",
    "X_test_emb = model.encode(X_test.tolist(), show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nTraining embeddings shape: {X_train_emb.shape}  (samples x dimensions)\")\n",
    "print(f\"Test embeddings shape:     {X_test_emb.shape}\")\n",
    "print(f\"\\nEach text is now a dense vector of {X_train_emb.shape[1]} floating-point numbers.\")\n",
    "print(f\"First embedding (truncated): {X_train_emb[0][:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification with Embeddings\n",
    "\n",
    "The embeddings are plain NumPy arrays, so we can feed them directly into any scikit-learn classifier -- no pipeline or vectorizer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SBERT + Logistic Regression ----\n",
    "clf_lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf_lr.fit(X_train_emb, y_train)\n",
    "\n",
    "y_pred_lr = clf_lr.predict(X_test_emb)\n",
    "acc_sbert_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SBERT + Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {acc_sbert_lr:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SBERT + LinearSVC ----\n",
    "clf_svc = LinearSVC(random_state=42, max_iter=2000)\n",
    "clf_svc.fit(X_train_emb, y_train)\n",
    "\n",
    "y_pred_svc = clf_svc.predict(X_test_emb)\n",
    "acc_sbert_svc = accuracy_score(y_test, y_pred_svc)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SBERT + LinearSVC\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {acc_sbert_svc:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Head-to-Head: TF-IDF vs SBERT\n",
    "\n",
    "To make the comparison fair, we build the TF-IDF baselines right here using the exact same train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TF-IDF + Logistic Regression ----\n",
    "pipe_tfidf_lr = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))),\n",
    "    (\"clf\", LogisticRegression(random_state=42, max_iter=1000)),\n",
    "])\n",
    "pipe_tfidf_lr.fit(X_train, y_train)\n",
    "acc_tfidf_lr = accuracy_score(y_test, pipe_tfidf_lr.predict(X_test))\n",
    "\n",
    "# ---- TF-IDF + LinearSVC ----\n",
    "pipe_tfidf_svc = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))),\n",
    "    (\"clf\", LinearSVC(random_state=42, max_iter=2000)),\n",
    "])\n",
    "pipe_tfidf_svc.fit(X_train, y_train)\n",
    "acc_tfidf_svc = accuracy_score(y_test, pipe_tfidf_svc.predict(X_test))\n",
    "\n",
    "# ---- Collect all results ----\n",
    "results = {\n",
    "    \"TF-IDF + LogisticRegression\": acc_tfidf_lr,\n",
    "    \"TF-IDF + LinearSVC\": acc_tfidf_svc,\n",
    "    \"SBERT + LogisticRegression\": acc_sbert_lr,\n",
    "    \"SBERT + LinearSVC\": acc_sbert_svc,\n",
    "}\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(list(results.items()), columns=[\"Model\", \"Accuracy\"])\n",
    "    .sort_values(\"Accuracy\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# ---- Bar chart ----\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "colors = [\"#2196F3\" if \"SBERT\" in m else \"#FF9800\" for m in results_df[\"Model\"]]\n",
    "bars = ax.barh(results_df[\"Model\"], results_df[\"Accuracy\"], color=colors, edgecolor=\"white\")\n",
    "ax.set_xlim(0.75, 0.95)\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "ax.set_title(\"Model Comparison: TF-IDF vs SBERT\")\n",
    "for bar, acc in zip(bars, results_df[\"Accuracy\"]):\n",
    "    ax.text(acc + 0.003, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{acc:.1%}\", va=\"center\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Label Efficiency: How Much Data Do You Really Need?\n",
    "\n",
    "This is where sentence embeddings truly shine. Because SBERT was pre-trained on hundreds of millions of sentence pairs, it already \"knows\" a lot about language. This means it can generalize well even from a tiny labeled training set.\n",
    "\n",
    "TF-IDF, on the other hand, builds its vocabulary from scratch and needs enough examples to learn which words are discriminative for each class.\n",
    "\n",
    "Let us test this hypothesis by training both approaches on progressively larger subsets of the training data and measuring accuracy on the same held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Label efficiency experiment ----\nsample_sizes = [10, 25, 50, 100, 200, len(X_train)]\nn_repeats = 5\nseeds = [0, 1, 2, 3, 4]\n\ndef run_efficiency_experiment(n, seed, X_train, y_train, X_train_emb, X_test, X_test_emb, y_test):\n    \"\"\"Run one trial of the label efficiency experiment.\n    \n    Returns (tfidf_accuracy, sbert_accuracy) for a given sample size and seed.\n    \"\"\"\n    # Subsample training data (stratified when possible)\n    if n >= len(X_train):\n        idx = X_train.index\n    else:\n        n_classes = y_train.nunique()\n        if n >= n_classes * 2:\n            idx, _, _, _ = train_test_split(\n                X_train.index, y_train,\n                train_size=n,\n                random_state=seed,\n                stratify=y_train,\n            )\n        else:\n            idx = X_train.sample(n=n, random_state=seed).index\n\n    X_sub = X_train.loc[idx]\n    y_sub = y_train.loc[idx]\n\n    # Map indices to embedding rows\n    train_indices = X_train.index.tolist()\n    emb_idx = [train_indices.index(i) for i in idx]\n    X_sub_emb = X_train_emb[emb_idx]\n\n    # -- TF-IDF + LR --\n    pipe = Pipeline([\n        (\"tfidf\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))),\n        (\"clf\", LogisticRegression(random_state=42, max_iter=1000)),\n    ])\n    pipe.fit(X_sub, y_sub)\n    acc_tf = accuracy_score(y_test, pipe.predict(X_test))\n\n    # -- SBERT + LR --\n    clf = LogisticRegression(random_state=42, max_iter=1000)\n    clf.fit(X_sub_emb, y_sub)\n    acc_sb = accuracy_score(y_test, clf.predict(X_test_emb))\n\n    return acc_tf, acc_sb\n\nprint(f\"Will test sample sizes: {sample_sizes}\")\nprint(f\"With {n_repeats} random seeds each\")"
  },
  {
   "cell_type": "code",
   "source": "# ---- Run experiments and collect results ----\nrecords = []\n\nfor n in sample_sizes:\n    for seed in seeds:\n        acc_tf, acc_sb = run_efficiency_experiment(\n            n, seed, X_train, y_train, X_train_emb, X_test, X_test_emb, y_test\n        )\n        records.append({\"n_train\": n, \"seed\": seed, \"method\": \"TF-IDF + LR\", \"accuracy\": acc_tf})\n        records.append({\"n_train\": n, \"seed\": seed, \"method\": \"SBERT + LR\", \"accuracy\": acc_sb})\n\neff_df = pd.DataFrame(records)\n\n# ---- Aggregate ----\nagg = eff_df.groupby([\"n_train\", \"method\"])[\"accuracy\"].agg([\"mean\", \"std\"]).reset_index()\n\n# ---- Plot ----\nfig, ax = plt.subplots(figsize=(9, 5))\n\nfor method, color, marker in [(\"SBERT + LR\", \"#2196F3\", \"o\"), (\"TF-IDF + LR\", \"#FF9800\", \"s\")]:\n    subset = agg[agg[\"method\"] == method]\n    ax.plot(subset[\"n_train\"], subset[\"mean\"], marker=marker, label=method,\n            color=color, linewidth=2, markersize=7)\n    ax.fill_between(subset[\"n_train\"],\n                    subset[\"mean\"] - subset[\"std\"],\n                    subset[\"mean\"] + subset[\"std\"],\n                    alpha=0.15, color=color)\n\nax.set_xlabel(\"Number of Training Samples\", fontsize=12)\nax.set_ylabel(\"Test Accuracy\", fontsize=12)\nax.set_title(\"Label Efficiency: SBERT vs TF-IDF\", fontsize=14)\nax.legend(fontsize=11)\nax.set_ylim(0.0, 1.0)\nax.set_xticks(sample_sizes)\nax.set_xticklabels([str(s) if s < len(X_train) else f\"{s}\\n(all)\" for s in sample_sizes])\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMean accuracy by training-set size:\")\nprint(agg.pivot(index=\"n_train\", columns=\"method\", values=\"mean\").round(3).to_string())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Embedding Space\n",
    "\n",
    "Let us use **t-SNE** to project the 384-dimensional embeddings down to 2D and see whether the 8 classes form distinct clusters. Good clustering in the embedding space means the classifier's job is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ---- Project to 2D ----\n",
    "# We use all data (train + test) for a richer visualization\n",
    "all_emb = np.vstack([X_train_emb, X_test_emb])\n",
    "all_labels = pd.concat([y_train, y_test], ignore_index=True)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "coords = tsne.fit_transform(all_emb)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "palette = sns.color_palette(\"husl\", n_colors=all_labels.nunique())\n",
    "\n",
    "for i, label in enumerate(sorted(all_labels.unique())):\n",
    "    mask = all_labels == label\n",
    "    ax.scatter(coords[mask, 0], coords[mask, 1],\n",
    "               label=label, color=palette[i], s=40, alpha=0.7, edgecolors=\"white\", linewidths=0.3)\n",
    "\n",
    "ax.set_title(\"t-SNE of SBERT Embeddings (all-MiniLM-L6-v2)\", fontsize=14)\n",
    "ax.set_xlabel(\"t-SNE 1\")\n",
    "ax.set_ylabel(\"t-SNE 2\")\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise: Try a Different Model\n",
    "\n",
    "The `all-MiniLM-L6-v2` model is great for English text, but there are many other options on the [SBERT model hub](https://www.sbert.net/docs/pretrained_models.html). Try one of these and compare:\n",
    "\n",
    "| Model | Dimensions | Notes |\n",
    "|-------|-----------|-------|\n",
    "| `BAAI/bge-small-en-v1.5` | 384 | Strong English model, newer architecture |\n",
    "| `intfloat/multilingual-e5-small` | 384 | Multilingual -- works across 100+ languages |\n",
    "| `all-mpnet-base-v2` | 768 | Larger, often higher quality |\n",
    "\n",
    "**Tasks:**\n",
    "1. Load a different model with `SentenceTransformer(...)`\n",
    "2. Encode the training and test data\n",
    "3. Train a `LogisticRegression` on the new embeddings\n",
    "4. Compare the accuracy to our baseline (`all-MiniLM-L6-v2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Exercise: Try a different embedding model ----\n",
    "\n",
    "# Step 1: Load the model\n",
    "# model_new = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")  # YOUR CODE HERE\n",
    "\n",
    "# Step 2: Encode train and test\n",
    "# X_train_emb_new = model_new.encode(X_train.tolist(), show_progress_bar=True)  # YOUR CODE HERE\n",
    "# X_test_emb_new = model_new.encode(X_test.tolist(), show_progress_bar=True)    # YOUR CODE HERE\n",
    "\n",
    "# Step 3: Train a classifier\n",
    "# clf_new = LogisticRegression(random_state=42, max_iter=1000)  # YOUR CODE HERE\n",
    "# clf_new.fit(X_train_emb_new, y_train)                        # YOUR CODE HERE\n",
    "\n",
    "# Step 4: Evaluate\n",
    "# y_pred_new = clf_new.predict(X_test_emb_new)                 # YOUR CODE HERE\n",
    "# acc_new = accuracy_score(y_test, y_pred_new)                 # YOUR CODE HERE\n",
    "# print(f\"New model accuracy: {acc_new:.4f}\")\n",
    "# print(f\"Baseline (all-MiniLM-L6-v2): {acc_sbert_lr:.4f}\")\n",
    "# print(f\"Difference: {acc_new - acc_sbert_lr:+.4f}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SAMPLE SOLUTION (uncomment to run)\n",
    "# -------------------------------------------------------------------\n",
    "# model_new = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "# X_train_emb_new = model_new.encode(X_train.tolist(), show_progress_bar=True)\n",
    "# X_test_emb_new = model_new.encode(X_test.tolist(), show_progress_bar=True)\n",
    "# clf_new = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# clf_new.fit(X_train_emb_new, y_train)\n",
    "# y_pred_new = clf_new.predict(X_test_emb_new)\n",
    "# acc_new = accuracy_score(y_test, y_pred_new)\n",
    "# print(f\"BAAI/bge-small-en-v1.5 accuracy: {acc_new:.4f}\")\n",
    "# print(f\"Baseline (all-MiniLM-L6-v2):     {acc_sbert_lr:.4f}\")\n",
    "# print(f\"Difference: {acc_new - acc_sbert_lr:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Summary & Takeaways\n\n### Results at a Glance\n\n| Approach | Representation | Classifier | Expected Accuracy |\n|----------|---------------|------------|-------------------|\n| TF-IDF + LR | Sparse, high-dim | Logistic Regression | ~84.4% |\n| TF-IDF + SVC | Sparse, high-dim | LinearSVC | ~85.2% |\n| **SBERT + LR** | **Dense, 384-dim** | **Logistic Regression** | **~87.8%** |\n| SBERT + SVC | Dense, 384-dim | LinearSVC | ~87.0% |\n\n### When to Use What?\n\n| Criterion | TF-IDF | Sentence Embeddings (SBERT) |\n|-----------|--------|-----------------------------|\n| **Speed** | Very fast (no GPU needed) | Slower (model inference) |\n| **Interpretability** | High (inspect top words) | Low (opaque 384-d vector) |\n| **Semantic understanding** | None (bag-of-words) | Strong (synonyms, paraphrases) |\n| **Low-data regimes** | Degrades quickly | Robust -- pre-trained knowledge transfers |\n| **Multilingual** | Needs per-language vocabulary | One model covers 100+ languages |\n| **Best for** | Quick baselines, keyword-driven tasks | Production systems, semantic tasks, few-shot |\n\n### Key Takeaways\n\n1. **Sentence embeddings capture meaning, not just words.** Two sentences can use entirely different vocabulary and still receive similar embeddings if they express the same idea.\n2. **SBERT + Logistic Regression is a strong, practical baseline.** It is simple to implement, fast to train (the encoding is the bottleneck), and competitive with much more complex approaches.\n3. **Embeddings shine when labeled data is scarce.** The label-efficiency curve shows that SBERT maintains reasonable accuracy even with just 25-50 training examples, while TF-IDF struggles.\n4. **Always start with a simple baseline (TF-IDF) and then move to embeddings** to quantify the improvement and justify the added complexity.\n\n**Next up (NB03):** We will explore **LLM-based zero-shot classification** -- using a large language model to classify text with zero training examples, plus structured output with Pydantic for reliable data extraction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Final summary ----\n",
    "print(\"=\" * 60)\n",
    "print(\"NB02 -- Final Results Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, acc in sorted(results.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name:<35s}  {acc:.1%}\")\n",
    "\n",
    "best_name = max(results, key=results.get)\n",
    "best_acc = results[best_name]\n",
    "print()\n",
    "print(f\"Best model: {best_name}  ({best_acc:.1%})\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}