{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB10: LLM App Evaluation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB10_llm_evaluation.ipynb)\n",
    "\n",
    "**Time:** ~40 min\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- Design evaluation rubrics for LLM-based applications\n",
    "- Implement automated metrics (accuracy, F1, precision, recall)\n",
    "- Combine quantitative and qualitative evaluation approaches\n",
    "- Understand RAGAS concepts for evaluating retrieval-augmented generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!pip install openai pydantic pandas scikit-learn numpy datasets -q\n\nimport os\nimport json\nimport time\n\nimport pandas as pd\nimport numpy as np\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, Optional\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score\n\nprint(\"All imports successful.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Evaluation Matters\n",
    "\n",
    "LLM outputs often *look* good -- they are fluent, coherent, and confident. But looking good is not the same as being correct. Without systematic evaluation, you can easily deploy a system that:\n",
    "\n",
    "- Misclassifies edge cases that seem reasonable on the surface\n",
    "- Hallucinates facts in a retrieval-augmented pipeline\n",
    "- Degrades silently when the input distribution shifts\n",
    "\n",
    "Evaluation is how we move from \"it seems to work\" to \"we know it works, and we know where it fails.\"\n",
    "\n",
    "There are two complementary approaches:\n",
    "\n",
    "1. **Automated metrics** -- fast, reproducible, scalable. Use them to catch regressions and compare models.\n",
    "2. **Human judgment** -- slower but catches things metrics miss. Use rubric-based scoring and qualitative error analysis.\n",
    "\n",
    "A robust evaluation strategy uses **both**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n\n# If not set above, try Colab secrets → then environment variable\nif not GROQ_API_KEY:\n    try:\n        from google.colab import userdata\n        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n    except (ImportError, Exception):\n        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n\nclient = OpenAI(\n    api_key=GROQ_API_KEY,\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\n# === Model Selection ===\nMODEL_FAST = \"moonshotai/kimi-k2-instruct\"   # Classification + judgment\nMODEL_SMART = \"moonshotai/kimi-k2-instruct\"  # Rubric scoring (same model, strong enough for both)\n\n# Test the connection\nresp = client.chat.completions.create(\n    model=MODEL_FAST,\n    messages=[{\"role\": \"user\", \"content\": \"Say 'ready'\"}],\n    max_tokens=5\n)\nprint(resp.choices[0].message.content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building an Eval Set\n",
    "\n",
    "The foundation of any evaluation is a **gold-standard eval set** -- a curated collection of examples where we know the correct answer. This set should:\n",
    "\n",
    "- Be representative of your real data distribution\n",
    "- Include edge cases and difficult examples\n",
    "- Have verified, high-quality ground-truth labels\n",
    "- Be large enough to give stable metrics (30+ examples minimum, 100+ for per-class metrics)\n",
    "\n",
    "We use the **tweet_eval stance detection** dataset (climate change topic). Stance classification asks: does this tweet express a **favorable**, **against**, or **neutral** position toward climate change action? This is inherently harder than topic classification — it requires understanding the author's *position*, not just the *subject*. This makes it an excellent testbed for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load tweet_eval stance dataset (climate change topic)\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"stance_climate\")\n",
    "\n",
    "STANCE_LABELS = {0: \"none\", 1: \"against\", 2: \"favor\"}\n",
    "\n",
    "# Convert test split to DataFrame\n",
    "test_full = pd.DataFrame(ds[\"test\"])\n",
    "test_full[\"label_name\"] = test_full[\"label\"].map(STANCE_LABELS)\n",
    "\n",
    "# Sample 30 examples as eval set\n",
    "eval_set = test_full.sample(30, random_state=42)[['text', 'label_name']].reset_index(drop=True)\n",
    "eval_set.columns = ['input_text', 'expected_label']\n",
    "\n",
    "print(f\"Eval set: {len(eval_set)} examples\")\n",
    "print(f\"\\nStance distribution:\")\n",
    "print(eval_set['expected_label'].value_counts())\n",
    "print(f\"\\nExample tweet:\")\n",
    "print(eval_set.iloc[0]['input_text'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automated Metrics\n",
    "\n",
    "For classification tasks, the standard automated metrics are:\n",
    "\n",
    "- **Accuracy** -- fraction of correct predictions. Simple but can be misleading with imbalanced classes.\n",
    "- **Macro F1** -- average of per-class F1 scores. Treats all classes equally regardless of their size.\n",
    "- **Per-class precision/recall** -- shows exactly where the model succeeds and where it fails.\n",
    "\n",
    "Stance classification is particularly interesting because the three classes have **different confusion costs**: confusing \"favor\" with \"against\" is a much more serious error than confusing either with \"none.\" We will run the LLM on every example in our eval set and compute these metrics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "CATEGORIES = [\"none\", \"against\", \"favor\"]\n\nclass StancePrediction(BaseModel):\n    label: Literal[\"none\", \"against\", \"favor\"] = Field(description=\"Stance toward climate change action\")\n\ndef classify_stance(text: str, max_retries: int = 3) -> Optional[StancePrediction]:\n    \"\"\"Classify stance with retry logic and Pydantic validation.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            resp = client.chat.completions.create(\n                model=MODEL_FAST,\n                messages=[\n                    {\"role\": \"system\", \"content\": f\"Determine the stance of this tweet toward climate change action. Classify into one of: {CATEGORIES}. 'favor' means supporting action on climate change, 'against' means opposing it, 'none' means neutral or unrelated. Return JSON with 'label' field only.\"},\n                    {\"role\": \"user\", \"content\": text[:500]}\n                ],\n                response_format={\"type\": \"json_object\"},\n                temperature=0.0,\n                max_tokens=50\n            )\n            return StancePrediction.model_validate_json(resp.choices[0].message.content)\n        except Exception as e:\n            if attempt < max_retries - 1:\n                time.sleep(2 ** attempt)\n            else:\n                return None\n\npredictions = []\nfor _, row in eval_set.iterrows():\n    result = classify_stance(row['input_text'])\n    predictions.append(result.label if result else 'Unknown')\n    time.sleep(0.1)\n\neval_set['predicted_label'] = predictions\nvalid = eval_set[eval_set.predicted_label != 'Unknown']\n\nprint(f\"Accuracy: {accuracy_score(valid.expected_label, valid.predicted_label):.1%}\")\nprint(f\"Macro F1: {f1_score(valid.expected_label, valid.predicted_label, average='macro', zero_division=0):.3f}\")\nprint(f\"\\n{classification_report(valid.expected_label, valid.predicted_label, zero_division=0)}\")\n\n# Note on interpreting these results:\n# The eval set is likely imbalanced (mostly \"favor\" in this dataset).\n# The LLM tends to hedge toward \"none\" — it under-predicts \"favor\" for tweets\n# with implicit stance (e.g., discussing climate impacts without explicit policy language).\n# This is a common LLM behavior on stance detection and makes the task a good eval testbed.\nprint(f\"\\nEval set class balance: {dict(valid.expected_label.value_counts())}\")\nprint(\"Tip: Look at per-class recall to see which stances the LLM misses most.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rubric-based Evaluation\n",
    "\n",
    "Automated metrics give you a number, but they treat all errors equally. A rubric-based evaluation lets you define **degrees of correctness**. For stance classification, we define an asymmetric rubric:\n",
    "\n",
    "- Confusing \"favor\" with \"against\" (or vice versa) is the **worst** error — the model got the position exactly backwards\n",
    "- Confusing either stance with \"none\" is a **moderate** error — the model missed the stance but didn't invert it\n",
    "- Exact match is a **perfect** score\n",
    "\n",
    "We use an LLM as a **judge** to apply this rubric consistently. This is sometimes called \"LLM-as-judge\" evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "RUBRIC = \"\"\"Score the stance classification on a scale of 1-5:\n5: Exact match with ground truth\n4: Close — predicted 'none' when true stance was mild, or vice versa\n3: Partially correct — got the general sentiment but wrong specific label\n2: Opposite direction — confused 'favor' with 'against' or vice versa\n1: Completely wrong, no reasonable connection\n\nReturn JSON: {\"score\": <int>, \"explanation\": \"<brief reason>\"}\"\"\"\n\nclass RubricScore(BaseModel):\n    score: int = Field(ge=1, le=5, description=\"Rubric score 1-5\")\n    explanation: str = Field(description=\"Brief reason for the score\")\n\ndef score_with_rubric(input_text: str, expected: str, predicted: str, max_retries: int = 3) -> Optional[RubricScore]:\n    \"\"\"Score a prediction using rubric + LLM-as-judge (uses smart model for quality).\"\"\"\n    for attempt in range(max_retries):\n        try:\n            resp = client.chat.completions.create(\n                model=MODEL_SMART,  # Kimi-k2 for nuanced judgment\n                messages=[\n                    {\"role\": \"system\", \"content\": RUBRIC},\n                    {\"role\": \"user\", \"content\": f\"Tweet: {input_text[:200]}\\nExpected stance: {expected}\\nPredicted stance: {predicted}\"}\n                ],\n                response_format={\"type\": \"json_object\"},\n                temperature=0.0,\n                max_tokens=100\n            )\n            return RubricScore.model_validate_json(resp.choices[0].message.content)\n        except Exception as e:\n            if attempt < max_retries - 1:\n                time.sleep(2 ** attempt)\n            else:\n                return None\n\n# Score ALL examples in the eval set\nscores = []\nfor i, (_, row) in enumerate(eval_set.iterrows()):\n    result = score_with_rubric(row['input_text'], row['expected_label'], row['predicted_label'])\n    if result:\n        scores.append(result)\n    time.sleep(0.15)  # Rate limiting for smart model\n    if (i + 1) % 10 == 0:\n        print(f\"Scored {i+1}/{len(eval_set)}...\")\n\n# Show a few examples\nfor s, (_, row) in zip(scores[:5], eval_set.head(5).iterrows()):\n    print(f\"Score: {s.score} | Expected: {row['expected_label']:<10} | Predicted: {row['predicted_label']:<10} | {s.explanation[:60]}\")\n\navg_score = np.mean([s.score for s in scores])\nprint(f\"\\nAverage rubric score: {avg_score:.1f}/5 (across {len(scores)} examples)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Qualitative Error Analysis\n",
    "\n",
    "This is often the most useful part of evaluation. Numbers tell you *how much* the model fails; error analysis tells you *why*.\n",
    "\n",
    "Stance detection is particularly revealing for error analysis because:\n",
    "- Some tweets use **sarcasm** — the model may take them literally\n",
    "- Some tweets discuss climate change without taking a stance — the boundary between \"none\" and a mild stance is genuinely ambiguous\n",
    "- Short tweets may lack sufficient context for reliable classification\n",
    "\n",
    "Look at the actual misclassifications below. Are there patterns?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "errors = eval_set[eval_set.expected_label != eval_set.predicted_label]\n",
    "print(f\"Errors: {len(errors)}/{len(eval_set)} ({len(errors)/len(eval_set):.0%})\")\n",
    "\n",
    "# Check for the worst kind of error: favor↔against confusion\n",
    "if len(errors) > 0:\n",
    "    flipped = errors[\n",
    "        ((errors.expected_label == 'favor') & (errors.predicted_label == 'against')) |\n",
    "        ((errors.expected_label == 'against') & (errors.predicted_label == 'favor'))\n",
    "    ]\n",
    "    print(f\"Stance inversions (favor↔against): {len(flipped)}/{len(errors)} errors\")\n",
    "\n",
    "for i, (_, row) in enumerate(errors.head(5).iterrows()):\n",
    "    print(f\"\\n--- Error {i+1} ---\")\n",
    "    print(f\"Tweet: {row['input_text'][:150]}...\")\n",
    "    print(f\"Expected: {row['expected_label']}\")\n",
    "    print(f\"Predicted: {row['predicted_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAGAS Concepts for Retrieval\n",
    "\n",
    "When evaluating **retrieval-augmented generation (RAG)** systems, classification metrics are not enough. You need to evaluate the full pipeline: retrieval quality *and* generation quality.\n",
    "\n",
    "The [RAGAS](https://docs.ragas.io/) framework defines three key metrics:\n",
    "\n",
    "1. **Faithfulness** -- Is the generated answer grounded in (supported by) the retrieved documents? A faithful answer does not add information beyond what the context provides.\n",
    "\n",
    "2. **Context Relevance** -- Are the retrieved documents actually relevant to the question? Irrelevant context can mislead the generator.\n",
    "\n",
    "3. **Answer Correctness** -- Is the final answer factually correct? This combines faithfulness with factual accuracy.\n",
    "\n",
    "Stance classification connects to these ideas: when an LLM classifies stance, we can ask whether its *reasoning* is faithful to the tweet's actual content, or whether it is projecting assumptions. Below is a simple faithfulness check you can adapt for RAG evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class FaithfulnessCheck(BaseModel):\n    faithful: bool = Field(description=\"Whether the answer is faithful to the context\")\n    explanation: str = Field(description=\"Brief reason\")\n\ndef check_faithfulness(question: str, context: str, answer: str, max_retries: int = 3) -> Optional[FaithfulnessCheck]:\n    \"\"\"Simple faithfulness check: is the answer supported by the context?\"\"\"\n    for attempt in range(max_retries):\n        try:\n            resp = client.chat.completions.create(\n                model=MODEL_FAST,\n                messages=[{\"role\": \"user\", \"content\": f\"\"\"Given this context and answer, is the answer faithful to (supported by) the context?\nContext: {context[:500]}\nAnswer: {answer}\nReturn JSON: {{\"faithful\": true/false, \"explanation\": \"brief reason\"}}\"\"\"}],\n                response_format={\"type\": \"json_object\"},\n                temperature=0.0,\n                max_tokens=100\n            )\n            return FaithfulnessCheck.model_validate_json(resp.choices[0].message.content)\n        except Exception as e:\n            if attempt < max_retries - 1:\n                time.sleep(2 ** attempt)\n            else:\n                return None\n\n# Example\nresult = check_faithfulness(\n    \"What causes climate change?\",\n    \"Climate change is primarily caused by greenhouse gas emissions from burning fossil fuels.\",\n    \"Climate change is caused by solar activity.\"\n)\nif result:\n    print(f\"Faithful: {result.faithful} — {result.explanation}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Exercise: Build Your Own Evaluation Pipeline\n\nApply the evaluation framework from this notebook to a different classification task:\n\n1. **Pick a different dataset or label set** — you could use `dair-ai/emotion` (6 emotions) or any classification output from your project\n2. **Run the LLM classifier** on 30+ examples\n3. **Compute automated metrics** (accuracy, macro F1, classification report)\n4. **Design a rubric** appropriate for your task (what are the worst errors? what's a \"close miss\"?)\n5. **Score with rubric** and compare the rubric scores to the automated metrics\n\n**Bonus:** Try two different LLM models (e.g., `llama-3.1-8b-instant` vs `qwen/qwen3-32b`) and compare their evaluation results. Does the more capable model score higher on both metrics and rubric?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# YOUR CODE HERE\n\n# Step 1: Load a different dataset\n# from datasets import load_dataset\n# ds = load_dataset(\"dair-ai/emotion\")\n# eval_examples = pd.DataFrame(ds[\"test\"]).sample(30, random_state=42)\n\n# Step 2: Classify with LLM\n# ...\n\n# Step 3: Compute automated metrics\n# accuracy = accuracy_score(eval_examples['true_label'], eval_examples['predicted_label'])\n# print(classification_report(...))\n\n# Step 4: Design and apply a rubric\n# MY_RUBRIC = \"\"\"...\"\"\"\n\n# Step 5: Compare metrics vs rubric scores",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "A complete evaluation framework combines three approaches:\n",
    "\n",
    "1. **Automated metrics** (accuracy, F1, precision/recall) -- fast, reproducible, good for regression testing and model comparison.\n",
    "2. **Rubric-based scoring** (LLM-as-judge with defined criteria) -- captures degrees of correctness. For stance detection, this reveals that not all errors are equal.\n",
    "3. **Qualitative error analysis** (manual inspection of failures) -- reveals *why* the system fails. For stance detection, common failure modes include sarcasm, implicit stance, and topic-adjacent tweets.\n",
    "\n",
    "**Always use all three.** Metrics alone miss nuance. Rubrics alone miss scale. Qualitative analysis alone misses the big picture.\n",
    "\n",
    "For RAG systems, add faithfulness and relevance checks (RAGAS concepts) to ensure the retrieval and generation components work together correctly."
   ]
  }
 ]
}