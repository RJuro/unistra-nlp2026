{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NB10: LLM App Evaluation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB10_llm_evaluation.ipynb)\n",
        "\n",
        "**Time:** ~40 min\n",
        "\n",
        "## Learning Goals\n",
        "\n",
        "- Design evaluation rubrics for LLM-based applications\n",
        "- Implement automated metrics (accuracy, F1, precision, recall)\n",
        "- Combine quantitative and qualitative evaluation approaches\n",
        "- Understand RAGAS concepts for evaluating retrieval-augmented generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install openai pandas scikit-learn numpy datasets -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from openai import OpenAI\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Why Evaluation Matters\n",
        "\n",
        "LLM outputs often *look* good -- they are fluent, coherent, and confident. But looking good is not the same as being correct. Without systematic evaluation, you can easily deploy a system that:\n",
        "\n",
        "- Misclassifies edge cases that seem reasonable on the surface\n",
        "- Hallucinates facts in a retrieval-augmented pipeline\n",
        "- Degrades silently when the input distribution shifts\n",
        "\n",
        "Evaluation is how we move from \"it seems to work\" to \"we know it works, and we know where it fails.\"\n",
        "\n",
        "There are two complementary approaches:\n",
        "\n",
        "1. **Automated metrics** -- fast, reproducible, scalable. Use them to catch regressions and compare models.\n",
        "2. **Human judgment** -- slower but catches things metrics miss. Use rubric-based scoring and qualitative error analysis.\n",
        "\n",
        "A robust evaluation strategy uses **both**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=GROQ_API_KEY,\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Building an Eval Set\n",
        "\n",
        "The foundation of any evaluation is a **gold-standard eval set** -- a curated collection of examples where we know the correct answer. This set should:\n",
        "\n",
        "- Be representative of your real data distribution\n",
        "- Include edge cases and difficult examples\n",
        "- Have verified, high-quality ground-truth labels\n",
        "- Be large enough to give stable metrics (30+ examples minimum, 100+ for per-class metrics)\n",
        "\n",
        "We use the **tweet_eval stance detection** dataset (climate change topic). Stance classification asks: does this tweet express a **favorable**, **against**, or **neutral** position toward climate change action? This is inherently harder than topic classification — it requires understanding the author's *position*, not just the *subject*. This makes it an excellent testbed for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load tweet_eval stance dataset (climate change topic)\n",
        "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"stance_climate\")\n",
        "\n",
        "STANCE_LABELS = {0: \"none\", 1: \"against\", 2: \"favor\"}\n",
        "\n",
        "# Convert test split to DataFrame\n",
        "test_full = pd.DataFrame(ds[\"test\"])\n",
        "test_full[\"label_name\"] = test_full[\"label\"].map(STANCE_LABELS)\n",
        "\n",
        "# Sample 30 examples as eval set\n",
        "eval_set = test_full.sample(30, random_state=42)[['text', 'label_name']].reset_index(drop=True)\n",
        "eval_set.columns = ['input_text', 'expected_label']\n",
        "\n",
        "print(f\"Eval set: {len(eval_set)} examples\")\n",
        "print(f\"\\nStance distribution:\")\n",
        "print(eval_set['expected_label'].value_counts())\n",
        "print(f\"\\nExample tweet:\")\n",
        "print(eval_set.iloc[0]['input_text'][:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Automated Metrics\n",
        "\n",
        "For classification tasks, the standard automated metrics are:\n",
        "\n",
        "- **Accuracy** -- fraction of correct predictions. Simple but can be misleading with imbalanced classes.\n",
        "- **Macro F1** -- average of per-class F1 scores. Treats all classes equally regardless of their size.\n",
        "- **Per-class precision/recall** -- shows exactly where the model succeeds and where it fails.\n",
        "\n",
        "Stance classification is particularly interesting because the three classes have **different confusion costs**: confusing \"favor\" with \"against\" is a much more serious error than confusing either with \"none.\" We will run the LLM on every example in our eval set and compute these metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import json, time\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "\n",
        "client = OpenAI(api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n",
        "CATEGORIES = [\"none\", \"against\", \"favor\"]\n",
        "\n",
        "predictions = []\n",
        "for _, row in eval_set.iterrows():\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"Determine the stance of this tweet toward climate change action. Classify into one of: {CATEGORIES}. 'favor' means supporting action on climate change, 'against' means opposing it, 'none' means neutral or unrelated. Return JSON with 'label' field only.\"},\n",
        "                {\"role\": \"user\", \"content\": row['input_text'][:500]}\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "            temperature=0.0,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        pred = json.loads(resp.choices[0].message.content).get('label', 'Unknown')\n",
        "        predictions.append(pred)\n",
        "    except:\n",
        "        predictions.append('Unknown')\n",
        "    time.sleep(0.1)\n",
        "\n",
        "eval_set['predicted_label'] = predictions\n",
        "valid = eval_set[eval_set.predicted_label != 'Unknown']\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(valid.expected_label, valid.predicted_label):.1%}\")\n",
        "print(f\"Macro F1: {f1_score(valid.expected_label, valid.predicted_label, average='macro', zero_division=0):.3f}\")\n",
        "print(f\"\\n{classification_report(valid.expected_label, valid.predicted_label, zero_division=0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Rubric-based Evaluation\n",
        "\n",
        "Automated metrics give you a number, but they treat all errors equally. A rubric-based evaluation lets you define **degrees of correctness**. For stance classification, we define an asymmetric rubric:\n",
        "\n",
        "- Confusing \"favor\" with \"against\" (or vice versa) is the **worst** error — the model got the position exactly backwards\n",
        "- Confusing either stance with \"none\" is a **moderate** error — the model missed the stance but didn't invert it\n",
        "- Exact match is a **perfect** score\n",
        "\n",
        "We use an LLM as a **judge** to apply this rubric consistently. This is sometimes called \"LLM-as-judge\" evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "RUBRIC = \"\"\"Score the stance classification on a scale of 1-5:\n",
        "5: Exact match with ground truth\n",
        "4: Close — predicted 'none' when true stance was mild, or vice versa\n",
        "3: Partially correct — got the general sentiment but wrong specific label\n",
        "2: Opposite direction — confused 'favor' with 'against' or vice versa\n",
        "1: Completely wrong, no reasonable connection\n",
        "\n",
        "Return JSON: {\"score\": <int>, \"explanation\": \"<brief reason>\"}\"\"\"\n",
        "\n",
        "def score_with_rubric(input_text, expected, predicted):\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": RUBRIC},\n",
        "            {\"role\": \"user\", \"content\": f\"Tweet: {input_text[:200]}\\nExpected stance: {expected}\\nPredicted stance: {predicted}\"}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.0,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return json.loads(resp.choices[0].message.content)\n",
        "\n",
        "# Score a sample\n",
        "scores = []\n",
        "for _, row in eval_set.head(10).iterrows():\n",
        "    s = score_with_rubric(row['input_text'], row['expected_label'], row['predicted_label'])\n",
        "    scores.append(s)\n",
        "    print(f\"Score: {s.get('score','?')} | Expected: {row['expected_label']:<10} | Predicted: {row['predicted_label']:<10}\")\n",
        "\n",
        "avg_score = np.mean([s.get('score', 0) for s in scores if isinstance(s.get('score'), (int, float))])\n",
        "print(f\"\\nAverage rubric score: {avg_score:.1f}/5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Qualitative Error Analysis\n",
        "\n",
        "This is often the most useful part of evaluation. Numbers tell you *how much* the model fails; error analysis tells you *why*.\n",
        "\n",
        "Stance detection is particularly revealing for error analysis because:\n",
        "- Some tweets use **sarcasm** — the model may take them literally\n",
        "- Some tweets discuss climate change without taking a stance — the boundary between \"none\" and a mild stance is genuinely ambiguous\n",
        "- Short tweets may lack sufficient context for reliable classification\n",
        "\n",
        "Look at the actual misclassifications below. Are there patterns?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "errors = eval_set[eval_set.expected_label != eval_set.predicted_label]\n",
        "print(f\"Errors: {len(errors)}/{len(eval_set)} ({len(errors)/len(eval_set):.0%})\")\n",
        "\n",
        "# Check for the worst kind of error: favor↔against confusion\n",
        "if len(errors) > 0:\n",
        "    flipped = errors[\n",
        "        ((errors.expected_label == 'favor') & (errors.predicted_label == 'against')) |\n",
        "        ((errors.expected_label == 'against') & (errors.predicted_label == 'favor'))\n",
        "    ]\n",
        "    print(f\"Stance inversions (favor↔against): {len(flipped)}/{len(errors)} errors\")\n",
        "\n",
        "for i, (_, row) in enumerate(errors.head(5).iterrows()):\n",
        "    print(f\"\\n--- Error {i+1} ---\")\n",
        "    print(f\"Tweet: {row['input_text'][:150]}...\")\n",
        "    print(f\"Expected: {row['expected_label']}\")\n",
        "    print(f\"Predicted: {row['predicted_label']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. RAGAS Concepts for Retrieval\n",
        "\n",
        "When evaluating **retrieval-augmented generation (RAG)** systems, classification metrics are not enough. You need to evaluate the full pipeline: retrieval quality *and* generation quality.\n",
        "\n",
        "The [RAGAS](https://docs.ragas.io/) framework defines three key metrics:\n",
        "\n",
        "1. **Faithfulness** -- Is the generated answer grounded in (supported by) the retrieved documents? A faithful answer does not add information beyond what the context provides.\n",
        "\n",
        "2. **Context Relevance** -- Are the retrieved documents actually relevant to the question? Irrelevant context can mislead the generator.\n",
        "\n",
        "3. **Answer Correctness** -- Is the final answer factually correct? This combines faithfulness with factual accuracy.\n",
        "\n",
        "Stance classification connects to these ideas: when an LLM classifies stance, we can ask whether its *reasoning* is faithful to the tweet's actual content, or whether it is projecting assumptions. Below is a simple faithfulness check you can adapt for RAG evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def check_faithfulness(question, context, answer):\n",
        "    \"\"\"Simple faithfulness check: is the answer supported by the context?\"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"\"\"Given this context and answer, is the answer faithful to (supported by) the context?\n",
        "Context: {context[:500]}\n",
        "Answer: {answer}\n",
        "Return JSON: {{\"faithful\": true/false, \"explanation\": \"brief reason\"}}\"\"\"}],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.0,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return json.loads(resp.choices[0].message.content)\n",
        "\n",
        "# Example\n",
        "result = check_faithfulness(\n",
        "    \"What causes climate change?\",\n",
        "    \"Climate change is primarily caused by greenhouse gas emissions from burning fossil fuels.\",\n",
        "    \"Climate change is caused by solar activity.\"\n",
        ")\n",
        "print(f\"Faithful: {result.get('faithful')} \u2014 {result.get('explanation')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "A complete evaluation framework combines three approaches:\n",
        "\n",
        "1. **Automated metrics** (accuracy, F1, precision/recall) -- fast, reproducible, good for regression testing and model comparison.\n",
        "2. **Rubric-based scoring** (LLM-as-judge with defined criteria) -- captures degrees of correctness. For stance detection, this reveals that not all errors are equal.\n",
        "3. **Qualitative error analysis** (manual inspection of failures) -- reveals *why* the system fails. For stance detection, common failure modes include sarcasm, implicit stance, and topic-adjacent tweets.\n",
        "\n",
        "**Always use all three.** Metrics alone miss nuance. Rubrics alone miss scale. Qualitative analysis alone misses the big picture.\n",
        "\n",
        "For RAG systems, add faithfulness and relevance checks (RAGAS concepts) to ensure the retrieval and generation components work together correctly."
      ]
    }
  ]
}