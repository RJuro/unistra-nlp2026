{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB11: Annotation & Inter-Rater Reliability\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB11_annotation_irr.ipynb)\n",
    "\n",
    "**Time:** ~30 min\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- Understand annotation workflows and why label quality matters\n",
    "- Measure inter-rater reliability using Cohen's kappa and Gwet's AC1\n",
    "- Compare human vs LLM labels systematically\n",
    "- Apply deductive coding as a structured labeling methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pandas scikit-learn numpy datasets -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Annotation Matters\n",
    "\n",
    "Every supervised model is only as good as its labels. If your training data has noisy, inconsistent, or biased labels, the model will learn those problems.\n",
    "\n",
    "**Inter-rater reliability (IRR)** measures how consistently different annotators label the same data. It answers the question: \"If two people independently label the same text, how often do they agree?\"\n",
    "\n",
    "Low IRR is a signal that:\n",
    "- The task definition is ambiguous\n",
    "- The categories overlap or are poorly defined\n",
    "- The annotators need better guidelines or training\n",
    "\n",
    "Stance detection is a **perfect case study** for annotation disagreement. Unlike topic classification (where the topic is usually clear), stance involves interpreting the author's *position* — which can be implicit, sarcastic, or genuinely ambiguous. This makes IRR measurement meaningful rather than a mere exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulating a Labeling Task\n",
    "\n",
    "In a real annotation project, you would have multiple human annotators labeling the same texts independently. Here, we simulate that setup:\n",
    "\n",
    "- **Annotator 1 (Human):** The ground-truth labels from the tweet_eval dataset\n",
    "- **Annotator 2 (LLM):** Labels produced by an LLM classifier\n",
    "\n",
    "We use the **stance_climate** subset of tweet_eval, where each tweet is labeled as **favor** (supports climate action), **against** (opposes it), or **none** (neutral/unrelated). This setup lets us measure how well the LLM agrees with human labels on a genuinely difficult classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweet_eval stance dataset (climate change topic)\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"stance_climate\")\n",
    "\n",
    "STANCE_LABELS = {0: \"none\", 1: \"against\", 2: \"favor\"}\n",
    "\n",
    "# Convert test split to DataFrame\n",
    "test_full = pd.DataFrame(ds[\"test\"])\n",
    "test_full[\"label_name\"] = test_full[\"label\"].map(STANCE_LABELS)\n",
    "\n",
    "# Select 50 examples for annotation\n",
    "annotation_set = test_full.sample(50, random_state=42)[['text', 'label_name']].reset_index(drop=True)\n",
    "annotation_set.columns = ['text', 'human_label']\n",
    "\n",
    "print(f\"Annotation set: {len(annotation_set)} examples\")\n",
    "print(f\"\\nStance distribution:\")\n",
    "print(annotation_set['human_label'].value_counts())\n",
    "print(f\"\\nExample tweet:\")\n",
    "print(annotation_set.iloc[0]['text'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "client = OpenAI(api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n",
    "CATEGORIES = [\"none\", \"against\", \"favor\"]\n",
    "\n",
    "llm_labels = []\n",
    "for _, row in annotation_set.iterrows():\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"Determine the stance of this tweet toward climate change action. Classify into one of: {CATEGORIES}. 'favor' means supporting action on climate change, 'against' means opposing it, 'none' means neutral or unrelated. Return JSON with 'label' field.\"},\n",
    "                {\"role\": \"user\", \"content\": row['text'][:500]}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.0,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        label = json.loads(resp.choices[0].message.content).get('label', 'Unknown')\n",
    "        llm_labels.append(label)\n",
    "    except:\n",
    "        llm_labels.append('Unknown')\n",
    "    time.sleep(0.1)\n",
    "\n",
    "annotation_set['llm_label'] = llm_labels\n",
    "valid = annotation_set[annotation_set.llm_label != 'Unknown']\n",
    "print(f\"Raw agreement: {accuracy_score(valid.human_label, valid.llm_label):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cohen's Kappa\n",
    "\n",
    "Raw agreement (\"how often do they pick the same label?\") is misleading because some agreement happens **by chance**. If you have 2 categories and both annotators guess randomly, you get 50% agreement just by luck.\n",
    "\n",
    "**Cohen's kappa** corrects for this:\n",
    "\n",
    "$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\n",
    "\n",
    "where $p_o$ is observed agreement and $p_e$ is expected agreement by chance.\n",
    "\n",
    "**Interpretation rules of thumb:**\n",
    "\n",
    "| Kappa | Interpretation |\n",
    "|-------|---------------|\n",
    "| < 0.20 | Slight agreement |\n",
    "| 0.20 -- 0.40 | Fair agreement |\n",
    "| 0.40 -- 0.60 | Moderate agreement |\n",
    "| 0.60 -- 0.80 | Substantial agreement |\n",
    "| > 0.80 | Almost perfect agreement |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kappa = cohen_kappa_score(valid['human_label'], valid['llm_label'])\n",
    "print(f\"Cohen's Kappa: {kappa:.3f}\")\n",
    "\n",
    "if kappa > 0.8: interpretation = \"Almost perfect agreement\"\n",
    "elif kappa > 0.6: interpretation = \"Substantial agreement\"\n",
    "elif kappa > 0.4: interpretation = \"Moderate agreement\"\n",
    "elif kappa > 0.2: interpretation = \"Fair agreement\"\n",
    "else: interpretation = \"Slight agreement\"\n",
    "print(f\"Interpretation: {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gwet's AC1\n",
    "\n",
    "Cohen's kappa has a well-known problem called the **kappa paradox**: when the category distribution is highly imbalanced (e.g., 90% of items belong to one class), kappa can be misleadingly low even when annotators agree most of the time. This happens because the chance-agreement correction assumes a specific model of random labeling that does not hold well with skewed distributions.\n",
    "\n",
    "**Gwet's AC1** is an alternative agreement coefficient that is more robust to this issue. It uses a different model for chance agreement that is less sensitive to marginal distributions.\n",
    "\n",
    "Since AC1 is not in standard libraries, we implement it manually below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gwet_ac1(labels1, labels2):\n",
    "    \"\"\"Calculate Gwet's AC1 agreement coefficient.\"\"\"\n",
    "    n = len(labels1)\n",
    "    categories = sorted(set(labels1) | set(labels2))\n",
    "    \n",
    "    # Observed agreement\n",
    "    po = sum(a == b for a, b in zip(labels1, labels2)) / n\n",
    "    \n",
    "    # Expected agreement under AC1\n",
    "    marginals = []\n",
    "    for cat in categories:\n",
    "        pi_k = (sum(1 for l in labels1 if l == cat) + sum(1 for l in labels2 if l == cat)) / (2 * n)\n",
    "        marginals.append(pi_k)\n",
    "    \n",
    "    pe = sum(pk * (1 - pk) for pk in marginals) / (len(categories) - 1) if len(categories) > 1 else 0\n",
    "    \n",
    "    if pe == 1: return 1.0\n",
    "    return (po - pe) / (1 - pe)\n",
    "\n",
    "ac1 = gwet_ac1(valid['human_label'].tolist(), valid['llm_label'].tolist())\n",
    "print(f\"Gwet's AC1: {ac1:.3f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.3f}\")\n",
    "print(f\"\\nAC1 is often higher than Kappa when categories are imbalanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Disagreement Analysis\n",
    "\n",
    "Where do the human and LLM annotators disagree? For stance detection, disagreements are particularly informative because they often reveal:\n",
    "\n",
    "- **Sarcasm and irony** — tweets where the literal text says one thing but means the opposite\n",
    "- **Implicit stance** — tweets that discuss climate without explicitly stating a position\n",
    "- **Ambiguous tweets** — genuinely borderline cases where reasonable annotators would disagree\n",
    "\n",
    "The most concerning pattern is **stance inversion**: cases where the LLM labels \"favor\" as \"against\" or vice versa. These are worse than \"none\" confusions because they reverse the meaning entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreements = valid[valid.human_label != valid.llm_label]\n",
    "print(f\"Disagreements: {len(disagreements)}/{len(valid)} ({len(disagreements)/len(valid):.0%})\")\n",
    "\n",
    "# Check for stance inversions\n",
    "flipped = disagreements[\n",
    "    ((disagreements.human_label == 'favor') & (disagreements.llm_label == 'against')) |\n",
    "    ((disagreements.human_label == 'against') & (disagreements.llm_label == 'favor'))\n",
    "]\n",
    "print(f\"Stance inversions (favor\\u2194against): {len(flipped)}/{len(disagreements)} disagreements\")\n",
    "\n",
    "print(f\"\\nConfusion pairs:\")\n",
    "confusion_pairs = disagreements.groupby(['human_label', 'llm_label']).size().sort_values(ascending=False)\n",
    "for (h, l), count in confusion_pairs.head(5).items():\n",
    "    print(f\"  Human: {h:<10} -> LLM: {l} ({count}x)\")\n",
    "\n",
    "print(f\"\\nExample disagreements:\")\n",
    "for i, (_, row) in enumerate(disagreements.head(3).iterrows()):\n",
    "    print(f\"\\n  Tweet: {row['text'][:120]}...\")\n",
    "    print(f\"  Human: {row['human_label']} | LLM: {row['llm_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deductive Coding Workflow\n",
    "\n",
    "**Deductive coding** is a structured annotation methodology from qualitative research. Instead of letting categories emerge from the data (inductive coding), you start with a predefined **codebook** and apply it systematically.\n",
    "\n",
    "For stance detection, a well-defined codebook is critical because the categories are *interpretive*, not *descriptive*. Unlike topic classification (where \"this post is about finance\" is fairly objective), stance classification requires judging the author's position — which depends on clear definitions of what counts as \"favor,\" \"against,\" and \"none.\"\n",
    "\n",
    "The workflow:\n",
    "\n",
    "1. **Define codebook** -- Write clear category definitions with inclusion/exclusion criteria and examples\n",
    "2. **LLM applies codes** -- Use the codebook as a prompt to classify all texts\n",
    "3. **Human validates** -- A human reviews a sample (e.g., 20%) of the LLM's labels\n",
    "4. **Measure agreement** -- Compute IRR between human and LLM labels\n",
    "5. **Refine codebook** -- If agreement is low, revise definitions to reduce ambiguity, then repeat\n",
    "\n",
    "This iterative process converges toward clear, reproducible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODEBOOK = {\n",
    "    \"favor\": \"Tweet explicitly or implicitly supports action on climate change. Includes: calling for policy action, expressing concern about climate impacts, supporting renewable energy, criticizing climate inaction.\",\n",
    "    \"against\": \"Tweet explicitly or implicitly opposes action on climate change. Includes: climate change denial/skepticism, opposing climate policies, dismissing environmental concerns, criticizing climate activists.\",\n",
    "    \"none\": \"Tweet mentions climate change but does not take a clear stance, OR is unrelated to climate change. Includes: neutral reporting, asking questions, discussing weather without connecting to climate policy.\",\n",
    "}\n",
    "\n",
    "print(\"Stance Detection Codebook:\")\n",
    "print(\"=\" * 60)\n",
    "for code, desc in CODEBOOK.items():\n",
    "    print(f\"\\n  {code.upper()}:\")\n",
    "    print(f\"  {desc}\")\n",
    "\n",
    "print(\"\\n\\nThis codebook would be used to:\")\n",
    "print(\"1. LLM codes all tweets using these definitions as the system prompt\")\n",
    "print(\"2. Human reviews a sample (e.g., 20%)\")\n",
    "print(\"3. Measure agreement\")\n",
    "print(\"4. If low agreement -> refine definitions -> repeat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Note on Argilla\n",
    "\n",
    "For real annotation projects that go beyond quick experiments, consider using [Argilla](https://docs.argilla.io/). Argilla is an open-source data annotation platform that provides:\n",
    "\n",
    "- A proper web UI for labeling (much better than spreadsheets)\n",
    "- Multi-annotator support with built-in IRR computation\n",
    "- Integration with Hugging Face datasets for easy export\n",
    "- Support for various task types (classification, NER, ranking, etc.)\n",
    "\n",
    "You can deploy Argilla for free on [Hugging Face Spaces](https://huggingface.co/spaces) and start annotating in minutes. This is the recommended approach when you need to label more than a few dozen examples or when you have multiple annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- **Always measure inter-rater reliability** before trusting your labels. Labels without measured reliability are labels of unknown quality.\n",
    "- **Cohen's kappa** is the standard IRR metric. It corrects for chance agreement and is widely understood.\n",
    "- **Gwet's AC1** handles imbalanced categories better than kappa. Use it when your class distribution is skewed.\n",
    "- **Stance detection reveals real annotation challenges.** Unlike simple topic classification, stance is inherently subjective — disagreements between human and LLM annotators are expected and informative.\n",
    "- **Human + LLM labeling** is a powerful combination: the LLM provides speed and scale, the human provides quality control and validation.\n",
    "- **Deductive coding** brings structure to qualitative analysis: define your codebook first, apply it systematically, measure agreement, and iterate. For stance detection, clear definitions of \"favor,\" \"against,\" and \"none\" are essential."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}