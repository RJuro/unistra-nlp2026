{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NB04: BERTopic — Topic Discovery + LLM Annotation\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB04_bertopic.ipynb)\n\n---\n\n**Learning Goals**\n\nBy the end of this notebook you will be able to:\n\n- **Discover latent topics** in text collections without any labels using BERTopic\n- **Configure BERTopic components** — embeddings, UMAP, HDBSCAN, and vectorizer — to control topic quality\n- **Use LLMs to name topics** via the Groq API, replacing cryptic keyword lists with human-readable labels\n- **Visualize and interpret topic models** with interactive charts, document maps, and hierarchies\n\n**Estimated time:** ~90 minutes\n\n> **GPU recommended** — go to **Runtime → Change runtime type → T4 GPU** for faster embedding of 5,000 documents (~30s on GPU vs ~5 min on CPU).\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Setup ──────────────────────────────────────────────────────────────────\n# Install dependencies (Colab-friendly)\n!pip install bertopic[visualization] sentence-transformers umap-learn hdbscan openai pandas numpy scikit-learn datasets -q\n\n# Core\nimport json\nimport re\nimport os\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# Sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n\n# Dimensionality reduction & clustering\nfrom umap import UMAP\nfrom hdbscan import HDBSCAN\n\n# Sentence embeddings\nfrom sentence_transformers import SentenceTransformer\n\n# BERTopic\nfrom bertopic import BERTopic\nfrom bertopic.representation import KeyBERTInspired\n\n# LLM client (OpenAI-compatible)\nfrom openai import OpenAI\n\nprint(\"All imports successful.\")"
  },
  {
   "cell_type": "code",
   "source": "# ── GPU Check ─────────────────────────────────────────────────────────────\nimport torch\n\nif torch.cuda.is_available():\n    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\nelse:\n    print(\"No GPU detected — running on CPU.\")\n    print(\"Embedding 5,000 documents will take ~5 minutes instead of ~30 seconds.\")\n    print(\"To enable GPU: Runtime → Change runtime type → T4 GPU\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Dataset: Moltbook\n",
    "\n",
    "**Moltbook** is a dataset of ~44K posts from an AI agent social network — think of it as a leaked database from the front page of the agent internet. The posts were generated by LLM-powered agents interacting in a simulated social platform, complete with submolts (like subreddits), upvotes, and comments.\n",
    "\n",
    "Each post is annotated with:\n",
    "- **9 content categories** (`topic_label`: A through I)\n",
    "- **5 toxicity levels** (`toxic_level`: 0–4)\n",
    "\n",
    "Our goal: **ignore the labels entirely** and see whether BERTopic can rediscover meaningful topic structure from the raw text alone. Then we will use an LLM to give those discovered topics human-readable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load Moltbook from HuggingFace ──────────────────────────────────────\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"TrustAIRLab/Moltbook\", \"posts\", split=\"train\")\n",
    "df = dataset.to_pandas()\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nSample post:\")\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Flatten the nested 'post' column and extract text ─────────────\n# The dataset has a nested 'post' dict with 'title' and 'content' keys\ndf[\"title\"] = df[\"post\"].apply(lambda x: x.get(\"title\", \"\") if isinstance(x, dict) else \"\")\ndf[\"content\"] = df[\"post\"].apply(lambda x: x.get(\"content\", \"\") if isinstance(x, dict) else \"\")\n\n# Fill NaN values and ensure string types (some entries may be missing/NaN)\ndf[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\ndf[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\ndf[\"text\"] = (df[\"title\"].str.strip() + \" . \" + df[\"content\"].str.strip()).str.strip()\n\n# Drop empty texts\ndf = df[df[\"text\"].str.len() > 3].reset_index(drop=True)\n\nprint(f\"Posts with valid text: {len(df)}\")\nprint(f\"\\nTopic label distribution:\\n{df['topic_label'].value_counts()}\")\nprint(f\"\\nToxic level distribution:\\n{df['toxic_level'].value_counts()}\")\nprint(f\"\\n--- Example text ---\")\nprint(df[\"text\"].iloc[0][:300])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Subsample for speed ────────────────────────────────────────────\n",
    "# 5000 posts is enough to find meaningful topics while keeping runtime short\n",
    "df_sample = df.sample(5000, random_state=42).reset_index(drop=True)\n",
    "print(f\"Working with {len(df_sample)} posts\")\n",
    "print(f\"Topic labels in sample: {sorted(df_sample['topic_label'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "We apply minimal cleaning: lowercase, remove URLs, and collapse whitespace. BERTopic relies on semantic embeddings, so aggressive preprocessing (stemming, removing punctuation) can actually hurt by destroying meaning that the embedding model understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Simple text cleaning ───────────────────────────────────────────\ndef clean_text(text) -> str:\n    \"\"\"Lowercase, remove URLs, and collapse whitespace. Handles NaN/float gracefully.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower()\n    text = re.sub(r\"https?://\\S+\", \"\", text)  # remove URLs\n    text = re.sub(r\"\\s+\", \" \", text)           # collapse whitespace\n    return text.strip()\n\n\ndf_sample[\"text_clean\"] = df_sample[\"text\"].apply(clean_text)\n\n# Drop any rows that ended up empty after cleaning\ndf_sample = df_sample[df_sample[\"text_clean\"].str.len() > 3].reset_index(drop=True)\n\n# Before / after\nprint(\"BEFORE cleaning:\")\nprint(df_sample[\"text\"].iloc[0][:200])\nprint()\nprint(\"AFTER cleaning:\")\nprint(df_sample[\"text_clean\"].iloc[0][:200])\nprint(f\"\\nSample size after cleaning: {len(df_sample)}\")\nprint(f\"Average text length: {df_sample['text_clean'].str.len().mean():.0f} chars\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Embeddings\n",
    "\n",
    "BERTopic needs **dense vector embeddings** to find semantic clusters. We use `all-MiniLM-L6-v2`, a fast sentence-transformer that maps each text to a 384-dimensional vector. Documents that are semantically similar end up close together in this vector space.\n",
    "\n",
    "We pre-compute embeddings separately so we can reuse them across experiments without re-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generate sentence embeddings ──────────────────────────────────\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = embedding_model.encode(\n",
    "    df_sample[\"text_clean\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Configuring BERTopic Components\n\nBERTopic is a **modular pipeline**. Each step can be configured independently:\n\n1. **Embeddings** → dense vectors from a sentence transformer (already done above)\n2. **UMAP** → reduces the 384-dimensional embeddings to 5 dimensions while preserving local structure\n3. **HDBSCAN** → finds density-based clusters in the reduced space (no need to specify *k*)\n4. **CountVectorizer** → extracts the most representative words per cluster using c-TF-IDF\n5. **Representation model** → refines topic keywords (we use KeyBERTInspired for coherent labels)\n\n### Key parameters to understand\n\n| Component | Parameter | Effect |\n|---|---|---|\n| UMAP | `n_neighbors` | Higher = more global structure, lower = more local detail |\n| UMAP | `n_components` | Dimensions for HDBSCAN (5 is a good default) |\n| HDBSCAN | `min_cluster_size` | Minimum documents per topic (higher = fewer, larger topics) |\n| HDBSCAN | `min_samples` | How conservative clustering is (higher = denser cores, more outliers) |\n| CountVectorizer | `stop_words` | Words to exclude — **must include domain terms** that appear everywhere |\n| CountVectorizer | `ngram_range` | (1,2) captures both single words and bigrams |\n\n### Domain-specific stopwords\n\nThis is a crucial practical insight: in the Moltbook dataset, words like \"ai\", \"agent\", \"agents\" appear in virtually every post. If we do not add them as stopwords, **every topic will look the same** — a wall of \"ai_agents_agent_ai\". Domain stopwords are terms that are too common *in your specific corpus* to be discriminative."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Configure BERTopic components ─────────────────────────────────\n\n# Custom stopwords: English defaults + common filler words + DOMAIN-SPECIFIC terms.\n# Since this is an AI agent social network, words like \"ai\", \"agent\", \"agents\"\n# appear in nearly every post and overwhelm topic keywords if not removed.\nstopwords = list(ENGLISH_STOP_WORDS) + [\n    # Common filler words\n    \"just\", \"like\", \"really\", \"think\", \"know\", \"want\",\n    \"got\", \"get\", \"one\", \"would\", \"could\", \"also\",\n    \"even\", \"much\", \"way\", \"thing\", \"things\", \"make\",\n    \"going\", \"need\", \"new\", \"use\", \"using\", \"used\",\n    # Moltbook domain words (appear in almost every post — not discriminative)\n    \"ai\", \"agent\", \"agents\", \"moltbook\", \"post\", \"posts\",\n    \"bot\", \"bots\", \"human\", \"humans\", \"world\", \"hello\",\n    \"sub\", \"submolt\", \"hackerclaw\", \"todos\",\n]\n\n# Vectorizer: unigrams + bigrams, require term in at least 5 docs\nvectorizer = CountVectorizer(\n    stop_words=stopwords,\n    ngram_range=(1, 2),\n    min_df=5,             # raised from 3 — filters out very rare terms\n)\n\n# UMAP: reduce to 5 dimensions, cosine distance\numap_model = UMAP(\n    n_neighbors=15,\n    n_components=5,\n    metric=\"cosine\",\n    random_state=42,\n)\n\n# HDBSCAN: larger clusters for more distinct, interpretable topics\nhdbscan_model = HDBSCAN(\n    min_cluster_size=50,   # raised from 15 — fewer but more coherent topics\n    min_samples=10,        # makes cluster cores denser, reduces noise topics\n    metric=\"euclidean\",\n    cluster_selection_method=\"eom\",\n    prediction_data=True,\n)\n\n# Representation: KeyBERT-inspired for more coherent topic keywords\nrepresentation_model = KeyBERTInspired()\n\nprint(\"Components configured:\")\nprint(f\"  Vectorizer:      CountVectorizer(ngram_range=(1,2), min_df=5, +domain stopwords)\")\nprint(f\"  UMAP:            n_neighbors=15, n_components=5, metric='cosine'\")\nprint(f\"  HDBSCAN:         min_cluster_size=50, min_samples=10, method='eom'\")\nprint(f\"  Representation:  KeyBERTInspired\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Topic Model\n",
    "\n",
    "Now we assemble the components into a BERTopic model and run `fit_transform`. Since we already pre-computed embeddings, we pass them directly — BERTopic skips the embedding step and goes straight to UMAP + HDBSCAN.\n",
    "\n",
    "The output is:\n",
    "- `topics`: a list of topic IDs (one per document). Topic `-1` = outlier (not assigned to any topic).\n",
    "- `probs`: soft assignment probabilities for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build and train BERTopic ───────────────────────────────────────\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(\n",
    "    df_sample[\"text_clean\"].tolist(),\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of topics found: {len(set(topics)) - 1}\")  # -1 for outlier topic\n",
    "print(f\"Outlier documents: {sum(t == -1 for t in topics)} ({sum(t == -1 for t in topics)/len(topics)*100:.1f}%)\")\n",
    "print(f\"Assigned documents: {sum(t != -1 for t in topics)} ({sum(t != -1 for t in topics)/len(topics)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring Topics\n",
    "\n",
    "BERTopic provides multiple ways to inspect the discovered topics. Let's start with the **topic info table**, which shows each topic's size and representative keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Topic info table ──────────────────────────────────────────────\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(f\"Total topics (including outliers): {len(topic_info)}\")\n",
    "print()\n",
    "print(topic_info.head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualize topics: bar chart of top keywords ─────────────────\n",
    "topic_model.visualize_barchart(top_n_topics=15, n_words=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualize topic map (intertopic distance) ───────────────────\n",
    "# Each circle is a topic; size = number of documents; distance = similarity\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualize documents in 2D ─────────────────────────────────────\n",
    "# Each dot is a document, colored by topic assignment\n",
    "topic_model.visualize_documents(\n",
    "    df_sample[\"text_clean\"].tolist(),\n",
    "    embeddings=embeddings,\n",
    "    hide_annotations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Topic hierarchy (dendrogram) ─────────────────────────────────\n",
    "# Shows how topics relate to each other and could be merged\n",
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. LLM-Powered Topic Naming\n\nThe keyword-based topic names from BERTopic are functional but not always intuitive. BERTopic has a built-in `OpenAI` representation model that can use any OpenAI-compatible API to generate descriptive topic labels.\n\nWe point it at **Groq** with `openai/gpt-oss-20b`. The representation model:\n1. Collects top keywords and representative documents for each topic\n2. Fills them into a prompt template (via `[DOCUMENTS]` and `[KEYWORDS]` placeholders)\n3. Sends it to the LLM and uses the response as the topic name\n\n### Avoiding rate limits\n\nGroq's free tier has strict token-per-minute (TPM) limits. Two key parameters help:\n- **`doc_length`** — truncate each representative document to N characters (saves tokens!)\n- **`delay_in_seconds`** — pause between API calls to stay under TPM limits\n- **`nr_docs`** — how many representative documents to send per topic"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Set up Groq client for LLM topic naming ──────────────────────\nimport openai\nfrom bertopic.representation import OpenAI as OpenAIRepresentation\n\nGROQ_API_KEY = \"\"  # @param {type:\"string\"}\n\n# If not set above, try Colab secrets → then environment variable\nif not GROQ_API_KEY:\n    try:\n        from google.colab import userdata\n        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n    except (ImportError, Exception):\n        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n\ngroq_client = openai.OpenAI(\n    api_key=GROQ_API_KEY,\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\n# Model choice: gpt-oss-20b supports structured outputs and has 8K TPM on free tier\n# (vs 6K TPM for llama-3.1-8b which caused the 413 \"Request too large\" errors)\nLLM_MODEL = \"openai/gpt-oss-20b\"\n\n# Quick connectivity test\nresp = groq_client.chat.completions.create(\n    model=LLM_MODEL,\n    messages=[{\"role\": \"user\", \"content\": \"Say 'ready' in one word.\"}],\n    max_tokens=5,\n)\nprint(f\"Model: {LLM_MODEL} — {resp.choices[0].message.content}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Create the LLM representation model ──────────────────────────\n# This uses BERTopic's built-in OpenAI integration.\n# [DOCUMENTS] and [KEYWORDS] are placeholders that BERTopic fills in automatically.\n\nllm_prompt = \"\"\"I will provide you with sample texts and keywords from a topic cluster.\nYour task is to create a concise, descriptive name (3-6 words) that captures the topic's essence.\n\nRequirements:\n- Use clear, specific language\n- Focus on the core theme, not peripheral details\n- Use natural phrasing (avoid generic words like \"issues\" or \"topics\")\n\n###EXAMPLES###\nTopic:\nSample texts from this topic:\n- I just started learning Python and I'm confused about lists vs dictionaries.\n- My code keeps throwing a 'KeyError' and I can't figure out why.\nKeywords: python, code, programming, error, syntax, function, debug\nTopic Name: Beginner Programming & Debugging\n\nTopic:\nSample texts from this topic:\n- I meal prep every Sunday but by Wednesday I'm tired of the same thing.\n- How do you make healthy eating sustainable with a busy schedule?\nKeywords: food, healthy, diet, meal, eating, nutrition, cook\nTopic Name: Healthy Eating & Meal Planning\n###END EXAMPLES###\n\nTopic:\nSample texts from this topic:\n[DOCUMENTS]\nKeywords: [KEYWORDS]\nOutput ONLY the topic name. No explanations. No preamble. Just the topic name:\"\"\"\n\n# Create BERTopic's OpenAI representation model pointed at Groq\nllm_representation = OpenAIRepresentation(\n    client=groq_client,\n    model=LLM_MODEL,\n    prompt=llm_prompt,\n    chat=True,\n    nr_docs=3,            # Only send 3 representative docs per topic (saves tokens)\n    doc_length=150,        # Truncate each doc to 150 chars (saves tokens!)\n    delay_in_seconds=2,    # Wait 2s between API calls to respect 8K TPM limit\n)\n\nprint(\"LLM representation model configured:\")\nprint(f\"  Model:    {LLM_MODEL}\")\nprint(f\"  nr_docs:  3 (representative documents per topic)\")\nprint(f\"  doc_length: 150 chars (truncated)\")\nprint(f\"  delay:    2s between API calls\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Apply LLM names to existing topic model ──────────────────────\n# update_topics() re-runs ONLY the representation step with the LLM,\n# keeping all existing topics and assignments unchanged.\n# This will make one API call per topic — expect ~20-60 seconds depending on topic count.\n\nprint(f\"Updating {len(topic_info) - 1} topics with LLM names (this takes a moment)...\\n\")\n\ntopic_model.update_topics(\n    df_sample[\"text_clean\"].tolist(),\n    representation_model=llm_representation,\n)\n\n# Show the updated topic names\nupdated_info = topic_model.get_topic_info()\nfor _, row in updated_info.head(15).iterrows():\n    print(f\"Topic {row['Topic']:3d} ({row['Count']:4d} docs): {row['Name']}\")\n\n# Re-visualize with new LLM-generated names\ntopic_model.visualize_barchart(top_n_topics=10, n_words=8)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Topics to Ground Truth\n",
    "\n",
    "Moltbook posts have ground-truth `topic_label` categories (A-I). Topic modeling is **unsupervised** — it does not know about these labels. But we can check whether the discovered topics align with the known categories using a cross-tabulation.\n",
    "\n",
    "A strong alignment means BERTopic found meaningful structure. Mismatches might reveal sub-topics within categories or cross-cutting themes that span multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cross-tabulation: discovered topics vs ground truth ───────────\n",
    "df_sample[\"topic\"] = topics\n",
    "\n",
    "if \"topic_label\" in df_sample.columns:\n",
    "    # Show cross-tab for top 15 topics (excluding outliers)\n",
    "    df_assigned = df_sample[df_sample[\"topic\"] != -1].copy()\n",
    "\n",
    "    # Limit to top 15 topics by size for readability\n",
    "    top_topics = df_assigned[\"topic\"].value_counts().head(15).index.tolist()\n",
    "    df_top = df_assigned[df_assigned[\"topic\"].isin(top_topics)]\n",
    "\n",
    "    ct = pd.crosstab(\n",
    "        df_top[\"topic_label\"],\n",
    "        df_top[\"topic\"],\n",
    "        margins=True,\n",
    "    )\n",
    "    print(\"Cross-tabulation: Ground-truth category (rows) vs BERTopic topic (columns)\")\n",
    "    print()\n",
    "    print(ct.to_string())\n",
    "else:\n",
    "    print(\"No 'topic_label' column found -- skipping comparison.\")\n",
    "    print(\"Topic distribution:\")\n",
    "    print(df_sample[\"topic\"].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercise: Tune the Model\n",
    "\n",
    "BERTopic is sensitive to its hyperparameters. Your task: **change one or more settings and observe the effect on topic quality.**\n",
    "\n",
    "Suggestions to try:\n",
    "- **`min_cluster_size`**: try 10, 20, or 50. Smaller values = more fine-grained topics, larger = fewer broader topics.\n",
    "- **`n_neighbors`**: try 5, 15, or 30. Controls UMAP's balance between local and global structure.\n",
    "- **`n_components`**: try 3 or 10 instead of 5.\n",
    "- **Embedding model**: try `\"all-mpnet-base-v2\"` (larger, more accurate) or `\"paraphrase-MiniLM-L3-v2\"` (faster, less accurate).\n",
    "- **`ngram_range`**: try `(1, 3)` for trigrams.\n",
    "\n",
    "Compare: How many topics are found? How many outliers? Do the topic keywords look more or less coherent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── YOUR CODE HERE ─────────────────────────────────────────────────\n",
    "# Step 1: Change one or more parameters below\n",
    "\n",
    "# umap_model_v2 = UMAP(\n",
    "#     n_neighbors=??,       # try 5, 15, 30\n",
    "#     n_components=??,      # try 3, 5, 10\n",
    "#     metric=\"cosine\",\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# hdbscan_model_v2 = HDBSCAN(\n",
    "#     min_cluster_size=??,  # try 10, 20, 50\n",
    "#     metric=\"euclidean\",\n",
    "#     cluster_selection_method=\"eom\",\n",
    "#     prediction_data=True,\n",
    "# )\n",
    "\n",
    "# Step 2: Build and fit a new topic model\n",
    "\n",
    "# topic_model_v2 = BERTopic(\n",
    "#     embedding_model=embedding_model,\n",
    "#     vectorizer_model=vectorizer,\n",
    "#     umap_model=umap_model_v2,\n",
    "#     hdbscan_model=hdbscan_model_v2,\n",
    "#     representation_model=KeyBERTInspired(),\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# topics_v2, probs_v2 = topic_model_v2.fit_transform(\n",
    "#     df_sample[\"text_clean\"].tolist(),\n",
    "#     embeddings=embeddings,\n",
    "# )\n",
    "\n",
    "# Step 3: Compare\n",
    "# print(f\"Number of topics: {len(set(topics_v2)) - 1}\")\n",
    "# print(f\"Outliers: {sum(t == -1 for t in topics_v2)}\")\n",
    "# topic_model_v2.get_topic_info().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Summary & Takeaways\n\n### What we learned\n\n1. **BERTopic discovers topics without labels.** It combines sentence embeddings, dimensionality reduction (UMAP), and density-based clustering (HDBSCAN) to find groups of semantically similar documents.\n\n2. **LLMs dramatically improve topic naming.** Instead of reading keyword lists like `[\"climate\", \"carbon\", \"emissions\", \"energy\"]`, an LLM can produce labels like \"Climate Change & Carbon Policy\" — much easier for humans to interpret.\n\n3. **Key parameters to tune:**\n   - `min_cluster_size` controls topic granularity (most impactful parameter)\n   - `n_neighbors` balances local vs global structure in UMAP\n   - The embedding model determines the quality of the semantic space\n\n4. **When to use topic modeling in research:**\n   - Exploratory analysis of large text corpora\n   - Discovering themes in survey responses, reviews, or social media\n   - Content analysis where manual coding is too expensive\n   - As a preprocessing step: topic assignments can become features for downstream tasks\n\n### The BERTopic pipeline at a glance\n\n![BERTopic Pipeline](https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/notebooks/figures/bertopic_pipeline.png)\n\n### Next steps\n\n- Try **topic merging** with `topic_model.merge_topics()` to combine related topics\n- Explore **dynamic topic modeling** for temporal analysis\n- Use discovered topics as **features in a classifier** (semi-supervised approach)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}