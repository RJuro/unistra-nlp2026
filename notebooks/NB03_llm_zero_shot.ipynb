{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB03: LLM Zero-shot Classification + Structured Output\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB03_llm_zero_shot.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- **Use LLMs as classifiers without any training data** -- zero-shot classification via the Groq API\n",
    "- **Enforce structured output with Pydantic** -- guarantee that the LLM returns valid, typed JSON\n",
    "- **Extract structured data from unstructured text** -- turn messy articles into clean, machine-readable records\n",
    "- **Compare to NB01/NB02** -- understand the trade-offs between classical ML, embeddings, and LLM-based approaches\n",
    "\n",
    "**Estimated time:** ~50 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Setup ────────────────────────────────────────────────────────────\n",
    "!pip install openai pydantic pandas scikit-learn tqdm -q\n",
    "\n",
    "# Core\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "# LLM client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Schema enforcement\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal, List, Optional\n",
    "\n",
    "# Data & evaluation\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. API Setup\n\nWe use **Groq** as our LLM provider. Groq offers:\n\n- **Free tier** -- generous daily limits, no credit card required\n- **Fast inference** -- custom LPU hardware delivers very low latency\n- **OpenAI-compatible API** -- we use the standard `openai` Python client, just pointed at Groq's endpoint\n\nGet your free API key at [console.groq.com/keys](https://console.groq.com/keys).\n\n> **Model strategy:** We use two models in this notebook. For **classification** (Part A, 50 API calls), we use `llama-3.1-8b-instant` — it's fast, has high rate limits (14.4K requests/day), and works well with basic `json_object` mode. For **structured extraction** (Part B, 1–3 API calls), we switch to `openai/gpt-oss-20b` which supports **strict Structured Outputs** (`json_schema` mode with constrained decoding). This way you see both patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom openai import OpenAI\n\n# Groq (primary -- free, fast)\n# Get your key at https://console.groq.com/keys\nGROQ_API_KEY = \"\"  # @param {type:\"string\"}\n\n# If not set above, try Colab secrets → then environment variable\nif not GROQ_API_KEY:\n    try:\n        from google.colab import userdata\n        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n    except (ImportError, Exception):\n        GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n\nclient = OpenAI(\n    api_key=GROQ_API_KEY,\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\n# === Model Selection ===\n# Part A (classification, 50 calls): fast model with json_object mode\nMODEL_FAST = \"llama-3.1-8b-instant\"    # 14.4K RPD, 500K TPD — best for batch work\n\n# Part B (extraction, 1-3 calls): strict structured output model\nMODEL_STRICT = \"openai/gpt-oss-20b\"    # 1K RPD, 200K TPD — strict json_schema mode\n\n# Quick test\nresponse = client.chat.completions.create(\n    model=MODEL_FAST,\n    messages=[{\"role\": \"user\", \"content\": \"Say 'API working!' in exactly 2 words.\"}],\n    max_tokens=10\n)\nprint(f\"Fast model: {MODEL_FAST}\")\nprint(f\"Strict model: {MODEL_STRICT}\")\nprint(f\"Response: {response.choices[0].message.content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Zero-shot Classification (25 min)\n",
    "\n",
    "The core idea: **LLMs can classify text without ANY training data.** You simply describe the categories in the prompt and ask the model to pick one. This is called **zero-shot classification** because the model has seen zero labeled examples from your dataset.\n",
    "\n",
    "Compare this to NB01 (TF-IDF) and NB02 (SBERT), where we needed hundreds of labeled examples to train a classifier. Here, the LLM's pre-trained knowledge does all the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "We use the same **dk_posts** dataset from NB01 and NB02 -- 457 synthetic English advice posts across 8 categories. This ensures a fair comparison across all three notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load data from GitHub ────────────────────────────────────────────\n",
    "DATA_URL = \"https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/data/dk_posts_synth_en_processed.json\"\n",
    "\n",
    "df = pd.read_json(DATA_URL)\n",
    "\n",
    "# ── Same preprocessing as NB01 ──────────────────────────────────────\n",
    "df[\"text\"] = df[\"title\"] + \" . \" + df[\"selftext\"]\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, strip, and collapse whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\n── Label distribution ──\")\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Defining the Output Schema\n\nA key challenge with LLMs is that they return free-form text. If we ask \"classify this post\", the model might respond with:\n\n- `\"I think this is about love and dating.\"`\n- `\"Category: Love & Dating\"`\n- `\"{\\\"label\\\": \\\"love\\\"}\"` (close but not quite right)\n\nWe need **structured, predictable output**. Groq supports two approaches:\n\n| Mode | `response_format` | Schema enforced? | Model support |\n|------|-------------------|-----------------|---------------|\n| **JSON Object** | `{\"type\": \"json_object\"}` | No -- just valid JSON syntax | All models |\n| **Structured Outputs** | `{\"type\": \"json_schema\", ...}` | Yes -- constrained decoding | Select models only |\n\nFor **Part A** (classification), we use **JSON Object mode** with `llama-3.1-8b-instant`. The model returns valid JSON, and we validate it with Pydantic *after* receiving it. This is fast and works with any model.\n\nIn **Part B** (extraction), we will switch to **Structured Outputs** with `openai/gpt-oss-20b` — the API uses constrained decoding to *guarantee* the output matches our schema. More reliable, but only available on select models and slower.\n\nThe workflow for Part A:\n1. **Pydantic `Literal` types** define our 8 categories as a schema\n2. **`json_object` mode** ensures the LLM returns valid JSON\n3. **`model_validate_json()`** validates the response matches our Pydantic schema after receiving it"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Define categories and schema ────────────────────────────────────\nCATEGORIES = [\n    \"Love & Dating\",\n    \"Family Dynamics\",\n    \"Work, Study & Career\",\n    \"Friendship & Social Life\",\n    \"Health & Wellness (Physical and Mental)\",\n    \"Personal Finance & Housing\",\n    \"Practical Questions & Everyday Life\",\n    \"Everyday Observations & Rants\"\n]\n\n\nclass SingleLabelPrediction(BaseModel):\n    predicted_label: Literal[\n        \"Love & Dating\",\n        \"Family Dynamics\",\n        \"Work, Study & Career\",\n        \"Friendship & Social Life\",\n        \"Health & Wellness (Physical and Mental)\",\n        \"Personal Finance & Housing\",\n        \"Practical Questions & Everyday Life\",\n        \"Everyday Observations & Rants\"\n    ] = Field(description=\"The single best-fit category for this post.\")\n    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score 0-1\")\n\n\n# For Part A we use json_object mode (works with any model, fast).\n# Pydantic validates AFTER receiving the response.\nprint(\"Schema defined: SingleLabelPrediction\")\nprint(f\"Categories: {len(CATEGORIES)}\")\nprint(f\"Fields: predicted_label (Literal), confidence (float 0-1)\")\nprint(f\"\\nPart A uses json_object mode → Pydantic validates after response\")\nprint(f\"Part B will use json_schema mode → API guarantees schema compliance\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Classifying a Single Post\n\nLet's build a function that takes a post's text and returns a validated `SingleLabelPrediction`. The workflow is:\n\n1. Send the text to the LLM with a system prompt listing the categories\n2. Request `json_object` mode so the response is valid JSON\n3. Parse and validate the response with Pydantic (`model_validate_json`)\n\nIf the LLM returns unexpected fields or wrong types, Pydantic catches it. This is the \"validate after\" pattern — fast and compatible with any model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def classify_post(text: str) -> Optional[SingleLabelPrediction]:\n    \"\"\"Classify a single post using the LLM with json_object mode + Pydantic validation.\"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=MODEL_FAST,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        f\"You classify personal advice posts into exactly one of these categories: {CATEGORIES}. \"\n                        \"Return JSON with exactly two fields: 'predicted_label' (one of the categories above) \"\n                        \"and 'confidence' (a float between 0 and 1).\"\n                    )\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Classify this post:\\n\\n{text}\"\n                }\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.0,\n            max_tokens=100\n        )\n        result = SingleLabelPrediction.model_validate_json(\n            response.choices[0].message.content\n        )\n        return result\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n\n# Test on one example\nsample = df.iloc[0]\nprint(f\"Text: {sample['text_clean'][:100]}...\")\nprint(f\"True label: {sample['label']}\")\n\nresult = classify_post(sample[\"text_clean\"])\nif result:\n    print(f\"Predicted: {result.predicted_label} (confidence: {result.confidence:.2f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Classification\n",
    "\n",
    "Now let's classify a batch of posts. We limit to the first 50 to stay well within Groq's free tier rate limits. A small `time.sleep(0.1)` between requests prevents hitting the rate limiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Classify a sample (full dataset takes ~3 minutes on free tier) ──\n",
    "sample_df = df.head(50).copy()\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Classifying\"):\n",
    "    result = classify_post(row[\"text_clean\"])\n",
    "    predictions.append({\n",
    "        \"true_label\": row[\"label\"],\n",
    "        \"predicted_label\": result.predicted_label if result else \"Error\",\n",
    "        \"confidence\": result.confidence if result else 0.0\n",
    "    })\n",
    "    time.sleep(0.1)  # Be nice to the API\n",
    "\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "valid = pred_df[pred_df.predicted_label != \"Error\"]\n",
    "\n",
    "print(f\"\\nSuccessful predictions: {len(valid)}/{len(pred_df)}\")\n",
    "display(pred_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Evaluate ─────────────────────────────────────────────────────────\n",
    "acc = accuracy_score(valid[\"true_label\"], valid[\"predicted_label\"])\n",
    "print(f\"\\nZero-shot LLM Accuracy: {acc:.1%}\")\n",
    "print(f\"(on {len(valid)}/{len(pred_df)} successful predictions)\\n\")\n",
    "print(classification_report(valid[\"true_label\"], valid[\"predicted_label\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: No Training Required!\n",
    "\n",
    "Here is the key insight. Compare the LLM approach to what we built in NB01 and NB02:\n",
    "\n",
    "| Approach | Training Data | Training Time | Accuracy |\n",
    "|---|---|---|---|\n",
    "| NB01: TF-IDF + Logistic Regression | 342 labeled posts | ~1 second | ~84% |\n",
    "| NB02: SBERT + Logistic Regression | 342 labeled posts | ~30 seconds | ~88% |\n",
    "| **NB03: LLM Zero-shot (this notebook)** | **0 labeled posts** | **0 seconds** | **see above** |\n",
    "\n",
    "The LLM achieves competitive accuracy with **zero training examples**. This is remarkable -- we simply described the categories and the model figured out how to classify posts using its pre-trained knowledge.\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Speed:** LLM inference is slower (~0.5s per post vs. microseconds for TF-IDF)\n",
    "- **Cost:** Free tier has daily limits; at scale, API costs add up\n",
    "- **Determinism:** LLM outputs can vary slightly between runs\n",
    "- **Privacy:** Data is sent to an external API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part B: Structured Extraction (25 min)\n\nClassification gives us a single label per document. But what if we need to **extract multiple structured fields** from text? For example:\n\n- From a news article: title, summary, organizations mentioned, key claims, sentiment\n- From a medical report: diagnosis, symptoms, medications, follow-up dates\n- From a policy document: stakeholders, economic figures, risk flags\n\nThis is **structured extraction** -- turning unstructured text into a typed, validated data record.\n\nFor extraction, schema compliance matters much more than for simple classification — we have nested fields, lists, and enums that must all be correct. This is where **Structured Outputs** (`json_schema` mode) shines: the API uses constrained decoding to *guarantee* the output matches our schema.\n\nWe switch to `openai/gpt-oss-20b` here because:\n- It supports `json_schema` with **strict mode** (constrained decoding)\n- We only make 1–3 API calls, so the slower speed is fine\n- Schema compliance is critical for downstream data pipelines"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world Example: Article Analysis\n",
    "\n",
    "We will extract structured information from a real-world article about AI infrastructure and scaling. This demonstrates how LLMs can power data pipelines that would otherwise require expensive manual annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\"\"\n",
    "MIT researchers have published a study questioning the long-term viability of scaling \n",
    "large language models. The paper, authored by Dr. Sarah Chen and colleagues at MIT's \n",
    "Computer Science and Artificial Intelligence Laboratory (CSAIL), suggests that the \n",
    "current approach of training ever-larger models is hitting diminishing returns.\n",
    "\n",
    "The study analyzed performance curves across recent frontier models and found that \n",
    "doubling model size no longer produces the breakthroughs seen two or three generations \n",
    "ago. Instead, gains are increasingly coming from smarter training approaches, \n",
    "architectural innovations, and inference-time optimizations that squeeze more \n",
    "performance out of smaller systems.\n",
    "\n",
    "The researchers point to the emergence of highly efficient models like DeepSeek, which \n",
    "demonstrated in early 2025 that competitive reasoning and coding capabilities could be \n",
    "achieved at a fraction of the compute cost of larger rivals. This challenges the \n",
    "prevailing Silicon Valley strategy of massive GPU cluster buildouts.\n",
    "\n",
    "Meanwhile, companies like OpenAI and major hyperscalers are committing hundreds of \n",
    "billions of dollars to long-term data center and energy infrastructure deals. Economists \n",
    "quoted in the report warn this resembles a speculative bubble, with enormous capital \n",
    "intensity and uncertain returns. If efficiency innovation continues to outpace brute-force \n",
    "scaling, the industry's current infrastructure investments may significantly overshoot \n",
    "actual demand.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Article length: {len(article_text)} characters\")\n",
    "print(article_text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Define the extraction schema ─────────────────────────────────────\nclass ArticleAnalysis(BaseModel):\n    title: str = Field(description=\"A concise title for the article\")\n    summary: str = Field(description=\"2-3 sentence summary\")\n    institutions: List[str] = Field(description=\"Organizations mentioned\")\n    key_claims: List[str] = Field(description=\"Main claims or findings (3-5)\")\n    sentiment: Literal[\"positive\", \"negative\", \"neutral\", \"mixed\"] = Field(\n        description=\"Overall sentiment\"\n    )\n    topics: List[str] = Field(description=\"Main topics discussed\")\n\n\n# Build strict-mode response format from the Pydantic schema.\n# This uses json_schema mode — the API constrains the LLM's output tokens\n# so the response is GUARANTEED to match our schema. No validation retries needed.\nextraction_schema = ArticleAnalysis.model_json_schema()\n\nEXTRACTION_FORMAT = {\n    \"type\": \"json_schema\",\n    \"json_schema\": {\n        \"name\": \"article_analysis\",\n        \"strict\": True,\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": extraction_schema[\"properties\"],\n            \"required\": list(extraction_schema[\"properties\"].keys()),\n            \"additionalProperties\": False\n        }\n    }\n}\n\nprint(f\"Using MODEL_STRICT = '{MODEL_STRICT}' for extraction (supports json_schema)\")\nprint(f\"\\nExtraction schema:\")\nprint(json.dumps(EXTRACTION_FORMAT, indent=2))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Extract with retry logic ─────────────────────────────────────────\ndef extract_with_retry(text: str, schema, response_format: dict, max_retries=3):\n    \"\"\"Extract structured data from text with retry logic.\n\n    Uses exponential backoff: wait 1s, 2s, 4s between retries.\n    This handles transient API errors (rate limits, timeouts).\n    With strict mode, schema violations should not occur.\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = client.chat.completions.create(\n                model=MODEL_STRICT,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": (\n                            \"Extract structured information from the text. \"\n                            \"Do not invent facts not present in the text.\"\n                        )\n                    },\n                    {\"role\": \"user\", \"content\": text}\n                ],\n                response_format=response_format,\n                temperature=0.0,\n                max_tokens=500\n            )\n            return schema.model_validate_json(\n                response.choices[0].message.content\n            )\n        except (ValidationError, Exception) as e:\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            time.sleep(2 ** attempt)  # Exponential backoff: 1s, 2s, 4s\n    return None\n\n\n# Run extraction\nresult = extract_with_retry(article_text, ArticleAnalysis, EXTRACTION_FORMAT)\n\nif result:\n    print(json.dumps(result.model_dump(), indent=2))\nelse:\n    print(\"Extraction failed after all retries.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Extraction to Analysis\n",
    "\n",
    "The power of structured extraction is that it feeds directly into downstream analysis. Once you have validated Pydantic objects, you can:\n",
    "\n",
    "- Load them into a **DataFrame** for tabular analysis\n",
    "- Store them in a **database** for querying\n",
    "- Feed them into **dashboards** for visualization\n",
    "- Use them as inputs to **other models** or rule-based systems\n",
    "\n",
    "Let's demonstrate turning our extraction into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Turn extractions into a DataFrame ────────────────────────────────\n",
    "# If we had multiple articles, we could build a structured dataset:\n",
    "results_data = [result.model_dump()] if result else []\n",
    "analysis_df = pd.DataFrame(results_data)\n",
    "\n",
    "if not analysis_df.empty:\n",
    "    print(\"── Extracted Data as DataFrame ──\\n\")\n",
    "    for col in analysis_df.columns:\n",
    "        print(f\"{col}: {analysis_df[col].iloc[0]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No data to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Design Your Own Schema\n",
    "\n",
    "Now it's your turn. Design a Pydantic schema to extract structured information from a text of your choice. Some ideas:\n",
    "\n",
    "- **Movie review:** title, rating (1-5), pros, cons, recommendation (yes/no)\n",
    "- **Job posting:** company, role, required skills, salary range, location\n",
    "- **Recipe:** dish name, ingredients list, prep time, difficulty\n",
    "\n",
    "Use `extract_with_retry()` to run your extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Exercise: Design your own schema ─────────────────────────────────\n\n# Step 1: Define your schema\n# class MySchema(BaseModel):\n#     # YOUR CODE HERE\n#     pass\n\n# Step 2: Build the response_format (helper function)\ndef make_strict_format(schema_class, name: str) -> dict:\n    \"\"\"Build a strict-mode response_format from a Pydantic model.\n    Use this with MODEL_STRICT (openai/gpt-oss-20b) for guaranteed schema compliance.\n    \"\"\"\n    s = schema_class.model_json_schema()\n    return {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": name,\n            \"strict\": True,\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": s[\"properties\"],\n                \"required\": list(s[\"properties\"].keys()),\n                \"additionalProperties\": False\n            }\n        }\n    }\n\n# Step 3: Provide a text to extract from\n# my_text = \"\"\"\n# YOUR TEXT HERE\n# \"\"\"\n\n# Step 4: Run extraction (uses MODEL_STRICT for strict schema)\n# my_format = make_strict_format(MySchema, \"my_extraction\")\n# my_result = extract_with_retry(my_text, MySchema, my_format)\n# if my_result:\n#     print(json.dumps(my_result.model_dump(), indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis\n",
    "\n",
    "A practical question: how much does this cost at scale? The answer depends on the provider and tier. Groq's free tier is remarkably generous for prototyping and teaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cost analysis ────────────────────────────────────────────────────\nn_posts = len(df)\n\nprint(\"Cost Analysis (Groq Free Tier):\")\nprint(f\"\\n  Part A — Classification with {MODEL_FAST}:\")\nprint(f\"    50 posts x ~200 tokens = ~10,000 tokens\")\nprint(f\"    Daily limit: 14,400 requests / 500K tokens per day\")\nprint(f\"    Full dataset ({n_posts} posts): {n_posts * 200:,} tokens — within limits\")\nprint(f\"    Cost: $0.00 (free tier)\")\nprint(f\"\\n  Part B — Extraction with {MODEL_STRICT}:\")\nprint(f\"    1-3 articles x ~800 tokens = ~2,400 tokens\")\nprint(f\"    Daily limit: 1,000 requests / 200K tokens per day\")\nprint(f\"    Cost: $0.00 (free tier)\")\nprint(f\"\\n  Model strategy recap:\")\nprint(f\"    {MODEL_FAST}: fast, high limits, json_object mode (post-hoc validation)\")\nprint(f\"    {MODEL_STRICT}: strict json_schema mode (constrained decoding), lower limits\")\nprint(f\"    → Use fast model for batch work, strict model when schema compliance is critical\")\nprint(f\"\\n  At scale (100,000 posts):\")\nprint(f\"    ~20M tokens — would need paid tier or multiple days\")\nprint(f\"    Compare: manual labeling at $0.10/post = $10,000\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Bonus: Deploy as a Gradio App\n\nWith just a few lines of code, we can turn our extraction pipeline into an **interactive web app** using [Gradio](https://gradio.app/). This creates a shareable URL that anyone can use — no coding required on their end.\n\n> This cell requires a Groq API key to be set above. The `share=True` parameter creates a temporary public URL (valid for 72 hours) that you can share with colleagues.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "try:\n    !pip install gradio -q\n    import gradio as gr\n\n    def extract_article(text):\n        \"\"\"Extract structured information from an article.\"\"\"\n        if not text.strip():\n            return {\"error\": \"Please enter some text\"}\n        result = extract_with_retry(text, ArticleAnalysis, EXTRACTION_FORMAT)\n        if result:\n            return result.model_dump()\n        return {\"error\": \"Extraction failed — check your API key\"}\n\n    demo = gr.Interface(\n        fn=extract_article,\n        inputs=gr.Textbox(lines=10, placeholder=\"Paste an article here...\"),\n        outputs=gr.JSON(label=\"Extracted Data\"),\n        title=\"Article Analyzer\",\n        description=\"Extract structured information (title, summary, institutions, claims, sentiment) from any article using LLM + Pydantic.\",\n        examples=[[article_text[:500]]],\n    )\n    demo.launch(share=True)\n\nexcept ImportError:\n    print(\"Gradio not available. Install with: pip install gradio\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary & Takeaways\n\n### What We Learned\n\n1. **LLMs are powerful zero-shot classifiers.** By simply describing categories in a prompt, we achieve competitive accuracy without any training data. This is transformative for cold-start problems where labeled data does not exist.\n\n2. **Structured Outputs guarantee schema compliance.** Using `response_format={\"type\": \"json_schema\", ...}` with strict mode, the API uses constrained decoding to ensure the output *always* matches our Pydantic schema. No retries, no parsing hacks.\n\n3. **Not all models support Structured Outputs.** On Groq, only select models (like `openai/gpt-oss-20b`) support `json_schema` mode. Others (like `llama-3.1-8b-instant`) only support basic `json_object` mode which returns valid JSON but doesn't enforce your schema. Always check the [Groq docs](https://console.groq.com/docs/structured-outputs) for current model support.\n\n4. **`extract_with_retry()` handles transient API errors.** Real-world APIs have rate limits and occasional timeouts. Exponential backoff is a simple but essential production pattern.\n\n5. **The right tool for the job depends on your constraints:**\n\n| | NB01: TF-IDF | NB02: SBERT | NB03: LLM Zero-shot |\n|---|---|---|---|\n| Training data needed | Yes (hundreds) | Yes (hundreds) | No |\n| Training time | Seconds | Minutes | None |\n| Inference speed | Microseconds | Milliseconds | ~0.5 seconds |\n| GPU required | No | Optional | No (API) |\n| Privacy | Full (local) | Full (local) | Data sent to API |\n| Structured extraction | No | No | Yes |\n| Cost at scale | Free | Free | Pay per token |\n\n### What's Next?\n\nIn **NB04** we will explore **unsupervised topic discovery** -- finding structure in text when we do not even know what the categories should be."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}