{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB03: LLM Zero-shot Classification + Structured Output\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB03_llm_zero_shot.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- **Use LLMs as classifiers without any training data** -- zero-shot classification via the Groq API\n",
    "- **Enforce structured output with Pydantic** -- guarantee that the LLM returns valid, typed JSON\n",
    "- **Extract structured data from unstructured text** -- turn messy articles into clean, machine-readable records\n",
    "- **Compare to NB01/NB02** -- understand the trade-offs between classical ML, embeddings, and LLM-based approaches\n",
    "\n",
    "**Estimated time:** ~50 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Setup ────────────────────────────────────────────────────────────\n",
    "!pip install openai pydantic pandas scikit-learn tqdm -q\n",
    "\n",
    "# Core\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "# LLM client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Schema enforcement\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal, List, Optional\n",
    "\n",
    "# Data & evaluation\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. API Setup\n",
    "\n",
    "We use **Groq** as our LLM provider. Groq offers:\n",
    "\n",
    "- **Free tier** -- generous daily limits, no credit card required\n",
    "- **Fast inference** -- custom LPU hardware delivers very low latency\n",
    "- **OpenAI-compatible API** -- we use the standard `openai` Python client, just pointed at Groq's endpoint\n",
    "\n",
    "Get your free API key at [console.groq.com/keys](https://console.groq.com/keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Groq (primary -- free, fast)\n",
    "# Get your key at https://console.groq.com/keys\n",
    "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# If running locally with .env file:\n",
    "# from dotenv import load_dotenv; load_dotenv()\n",
    "# GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "MODEL = \"llama-3.1-8b-instant\"\n",
    "\n",
    "# Quick test\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'API working!' in exactly 2 words.\"}],\n",
    "    max_tokens=10\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Zero-shot Classification (25 min)\n",
    "\n",
    "The core idea: **LLMs can classify text without ANY training data.** You simply describe the categories in the prompt and ask the model to pick one. This is called **zero-shot classification** because the model has seen zero labeled examples from your dataset.\n",
    "\n",
    "Compare this to NB01 (TF-IDF) and NB02 (SBERT), where we needed hundreds of labeled examples to train a classifier. Here, the LLM's pre-trained knowledge does all the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "We use the same **dk_posts** dataset from NB01 and NB02 -- 457 synthetic English advice posts across 8 categories. This ensures a fair comparison across all three notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load data from GitHub ────────────────────────────────────────────\n",
    "DATA_URL = \"https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/data/dk_posts_synth_en_processed.json\"\n",
    "\n",
    "df = pd.read_json(DATA_URL)\n",
    "\n",
    "# ── Same preprocessing as NB01 ──────────────────────────────────────\n",
    "df[\"text\"] = df[\"title\"] + \" . \" + df[\"selftext\"]\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, strip, and collapse whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\n── Label distribution ──\")\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Output Schema\n",
    "\n",
    "A key challenge with LLMs is that they return free-form text. If we ask \"classify this post\", the model might respond with:\n",
    "\n",
    "- `\"I think this is about love and dating.\"`\n",
    "- `\"Category: Love & Dating\"`\n",
    "- `\"{\\\"label\\\": \\\"love\\\"}\"` (close but not quite right)\n",
    "\n",
    "We need **structured, predictable output**. The solution is **Pydantic** + **JSON mode**:\n",
    "\n",
    "1. **Pydantic `Literal` types** constrain the output to exactly our 8 categories -- nothing else is accepted\n",
    "2. **JSON mode** (`response_format={\"type\": \"json_object\"}`) tells the API to only return valid JSON\n",
    "3. **`model_validate_json()`** parses and validates the response against our schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Define categories and schema ────────────────────────────────────\n",
    "CATEGORIES = [\n",
    "    \"Love & Dating\",\n",
    "    \"Family Dynamics\",\n",
    "    \"Work, Study & Career\",\n",
    "    \"Friendship & Social Life\",\n",
    "    \"Health & Wellness (Physical and Mental)\",\n",
    "    \"Personal Finance & Housing\",\n",
    "    \"Practical Questions & Everyday Life\",\n",
    "    \"Everyday Observations & Rants\"\n",
    "]\n",
    "\n",
    "\n",
    "class SingleLabelPrediction(BaseModel):\n",
    "    predicted_label: Literal[\n",
    "        \"Love & Dating\",\n",
    "        \"Family Dynamics\",\n",
    "        \"Work, Study & Career\",\n",
    "        \"Friendship & Social Life\",\n",
    "        \"Health & Wellness (Physical and Mental)\",\n",
    "        \"Personal Finance & Housing\",\n",
    "        \"Practical Questions & Everyday Life\",\n",
    "        \"Everyday Observations & Rants\"\n",
    "    ] = Field(description=\"The single best-fit category for this post.\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score 0-1\")\n",
    "\n",
    "\n",
    "# Show the JSON schema that Pydantic generates\n",
    "print(json.dumps(SingleLabelPrediction.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying a Single Post\n",
    "\n",
    "Let's build a function that takes a post's text and returns a validated `SingleLabelPrediction`. The workflow is:\n",
    "\n",
    "1. Send the text to the LLM with a system prompt listing the categories\n",
    "2. Request JSON output via `response_format`\n",
    "3. Validate the response with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_post(text: str) -> Optional[SingleLabelPrediction]:\n",
    "    \"\"\"Classify a single post using the LLM.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        f\"You classify personal advice posts into exactly one of these categories: {CATEGORIES}. \"\n",
    "                        \"Return valid JSON only with fields 'predicted_label' and 'confidence'.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Classify this post:\\n\\n{text}\"\n",
    "                }\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        result = SingleLabelPrediction.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test on one example\n",
    "sample = df.iloc[0]\n",
    "print(f\"Text: {sample['text_clean'][:100]}...\")\n",
    "print(f\"True label: {sample['label']}\")\n",
    "\n",
    "result = classify_post(sample[\"text_clean\"])\n",
    "if result:\n",
    "    print(f\"Predicted: {result.predicted_label} (confidence: {result.confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Classification\n",
    "\n",
    "Now let's classify a batch of posts. We limit to the first 50 to stay well within Groq's free tier rate limits. A small `time.sleep(0.1)` between requests prevents hitting the rate limiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Classify a sample (full dataset takes ~3 minutes on free tier) ──\n",
    "sample_df = df.head(50).copy()\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Classifying\"):\n",
    "    result = classify_post(row[\"text_clean\"])\n",
    "    predictions.append({\n",
    "        \"true_label\": row[\"label\"],\n",
    "        \"predicted_label\": result.predicted_label if result else \"Error\",\n",
    "        \"confidence\": result.confidence if result else 0.0\n",
    "    })\n",
    "    time.sleep(0.1)  # Be nice to the API\n",
    "\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "valid = pred_df[pred_df.predicted_label != \"Error\"]\n",
    "\n",
    "print(f\"\\nSuccessful predictions: {len(valid)}/{len(pred_df)}\")\n",
    "display(pred_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Evaluate ─────────────────────────────────────────────────────────\n",
    "acc = accuracy_score(valid[\"true_label\"], valid[\"predicted_label\"])\n",
    "print(f\"\\nZero-shot LLM Accuracy: {acc:.1%}\")\n",
    "print(f\"(on {len(valid)}/{len(pred_df)} successful predictions)\\n\")\n",
    "print(classification_report(valid[\"true_label\"], valid[\"predicted_label\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: No Training Required!\n",
    "\n",
    "Here is the key insight. Compare the LLM approach to what we built in NB01 and NB02:\n",
    "\n",
    "| Approach | Training Data | Training Time | Accuracy |\n",
    "|---|---|---|---|\n",
    "| NB01: TF-IDF + Logistic Regression | 342 labeled posts | ~1 second | ~84% |\n",
    "| NB02: SBERT + Logistic Regression | 342 labeled posts | ~30 seconds | ~88% |\n",
    "| **NB03: LLM Zero-shot (this notebook)** | **0 labeled posts** | **0 seconds** | **see above** |\n",
    "\n",
    "The LLM achieves competitive accuracy with **zero training examples**. This is remarkable -- we simply described the categories and the model figured out how to classify posts using its pre-trained knowledge.\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Speed:** LLM inference is slower (~0.5s per post vs. microseconds for TF-IDF)\n",
    "- **Cost:** Free tier has daily limits; at scale, API costs add up\n",
    "- **Determinism:** LLM outputs can vary slightly between runs\n",
    "- **Privacy:** Data is sent to an external API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Structured Extraction (25 min)\n",
    "\n",
    "Classification gives us a single label per document. But what if we need to **extract multiple structured fields** from text? For example:\n",
    "\n",
    "- From a news article: title, summary, organizations mentioned, key claims, sentiment\n",
    "- From a medical report: diagnosis, symptoms, medications, follow-up dates\n",
    "- From a policy document: stakeholders, economic figures, risk flags\n",
    "\n",
    "This is **structured extraction** -- turning unstructured text into a typed, validated data record. The same Pydantic + LLM pattern scales beautifully to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world Example: Article Analysis\n",
    "\n",
    "We will extract structured information from a real-world article about AI infrastructure and scaling. This demonstrates how LLMs can power data pipelines that would otherwise require expensive manual annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\"\"\n",
    "MIT researchers have published a study questioning the long-term viability of scaling \n",
    "large language models. The paper, authored by Dr. Sarah Chen and colleagues at MIT's \n",
    "Computer Science and Artificial Intelligence Laboratory (CSAIL), suggests that the \n",
    "current approach of training ever-larger models is hitting diminishing returns.\n",
    "\n",
    "The study analyzed performance curves across recent frontier models and found that \n",
    "doubling model size no longer produces the breakthroughs seen two or three generations \n",
    "ago. Instead, gains are increasingly coming from smarter training approaches, \n",
    "architectural innovations, and inference-time optimizations that squeeze more \n",
    "performance out of smaller systems.\n",
    "\n",
    "The researchers point to the emergence of highly efficient models like DeepSeek, which \n",
    "demonstrated in early 2025 that competitive reasoning and coding capabilities could be \n",
    "achieved at a fraction of the compute cost of larger rivals. This challenges the \n",
    "prevailing Silicon Valley strategy of massive GPU cluster buildouts.\n",
    "\n",
    "Meanwhile, companies like OpenAI and major hyperscalers are committing hundreds of \n",
    "billions of dollars to long-term data center and energy infrastructure deals. Economists \n",
    "quoted in the report warn this resembles a speculative bubble, with enormous capital \n",
    "intensity and uncertain returns. If efficiency innovation continues to outpace brute-force \n",
    "scaling, the industry's current infrastructure investments may significantly overshoot \n",
    "actual demand.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Article length: {len(article_text)} characters\")\n",
    "print(article_text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Define the extraction schema ─────────────────────────────────────\n",
    "class ArticleAnalysis(BaseModel):\n",
    "    title: str = Field(description=\"A concise title for the article\")\n",
    "    summary: str = Field(description=\"2-3 sentence summary\")\n",
    "    institutions: List[str] = Field(description=\"Organizations mentioned\")\n",
    "    key_claims: List[str] = Field(description=\"Main claims or findings (3-5)\")\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\", \"mixed\"] = Field(\n",
    "        description=\"Overall sentiment\"\n",
    "    )\n",
    "    topics: List[str] = Field(description=\"Main topics discussed\")\n",
    "\n",
    "\n",
    "# Show the schema\n",
    "print(json.dumps(ArticleAnalysis.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extract with retry logic ─────────────────────────────────────────\n",
    "def extract_with_retry(text: str, schema, max_retries=3):\n",
    "    \"\"\"Extract structured data from text with retry logic.\n",
    "    \n",
    "    Uses exponential backoff: wait 1s, 2s, 4s between retries.\n",
    "    This handles transient API errors and occasional malformed responses.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"Extract structured information from the text. \"\n",
    "                            \"Return valid JSON matching the requested schema. \"\n",
    "                            \"Do not invent facts not present in the text.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.0,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            return schema.model_validate_json(\n",
    "                response.choices[0].message.content\n",
    "            )\n",
    "        except (ValidationError, Exception) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff: 1s, 2s, 4s\n",
    "    return None\n",
    "\n",
    "\n",
    "# Run extraction\n",
    "result = extract_with_retry(article_text, ArticleAnalysis)\n",
    "\n",
    "if result:\n",
    "    print(json.dumps(result.model_dump(), indent=2))\n",
    "else:\n",
    "    print(\"Extraction failed after all retries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Extraction to Analysis\n",
    "\n",
    "The power of structured extraction is that it feeds directly into downstream analysis. Once you have validated Pydantic objects, you can:\n",
    "\n",
    "- Load them into a **DataFrame** for tabular analysis\n",
    "- Store them in a **database** for querying\n",
    "- Feed them into **dashboards** for visualization\n",
    "- Use them as inputs to **other models** or rule-based systems\n",
    "\n",
    "Let's demonstrate turning our extraction into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Turn extractions into a DataFrame ────────────────────────────────\n",
    "# If we had multiple articles, we could build a structured dataset:\n",
    "results_data = [result.model_dump()] if result else []\n",
    "analysis_df = pd.DataFrame(results_data)\n",
    "\n",
    "if not analysis_df.empty:\n",
    "    print(\"── Extracted Data as DataFrame ──\\n\")\n",
    "    for col in analysis_df.columns:\n",
    "        print(f\"{col}: {analysis_df[col].iloc[0]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No data to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Design Your Own Schema\n",
    "\n",
    "Now it's your turn. Design a Pydantic schema to extract structured information from a text of your choice. Some ideas:\n",
    "\n",
    "- **Movie review:** title, rating (1-5), pros, cons, recommendation (yes/no)\n",
    "- **Job posting:** company, role, required skills, salary range, location\n",
    "- **Recipe:** dish name, ingredients list, prep time, difficulty\n",
    "\n",
    "Use `extract_with_retry()` to run your extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise: Design your own schema ─────────────────────────────────\n",
    "\n",
    "# Step 1: Define your schema\n",
    "# class MySchema(BaseModel):\n",
    "#     # YOUR CODE HERE\n",
    "#     pass\n",
    "\n",
    "# Step 2: Provide a text to extract from\n",
    "# my_text = \"\"\"\n",
    "# YOUR TEXT HERE\n",
    "# \"\"\"\n",
    "\n",
    "# Step 3: Run extraction\n",
    "# my_result = extract_with_retry(my_text, MySchema)\n",
    "# if my_result:\n",
    "#     print(json.dumps(my_result.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis\n",
    "\n",
    "A practical question: how much does this cost at scale? The answer depends on the provider and tier. Groq's free tier is remarkably generous for prototyping and teaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cost analysis ────────────────────────────────────────────────────\n",
    "# Groq free tier: 14,400 requests/day for llama-3.1-8b-instant\n",
    "# Rough token estimate: ~200 tokens per classification request\n",
    "# For 457 posts: ~91,400 tokens = well within free tier\n",
    "\n",
    "n_posts = len(df)\n",
    "\n",
    "print(\"Cost Analysis (Groq Free Tier):\")\n",
    "print(f\"  Classification: {n_posts} posts x ~200 tokens = {n_posts * 200:,} tokens\")\n",
    "print(f\"  Daily limit: 14,400 requests -- can classify {14_400} posts/day\")\n",
    "print(f\"  Cost: $0.00 (free tier)\")\n",
    "print(f\"\\nFor comparison, Groq paid tier:\")\n",
    "print(f\"  Input: $0.05/M tokens, Output: $0.08/M tokens\")\n",
    "print(f\"  {n_posts} posts ~ $0.01 total\")\n",
    "print(f\"\\nAt scale (100,000 posts):\")\n",
    "print(f\"  ~20M tokens = ~$1.00-$2.60 total\")\n",
    "print(f\"  Compare: manual labeling at $0.10/post = $10,000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Bonus: Deploy as a Gradio App\n\nWith just a few lines of code, we can turn our extraction pipeline into an **interactive web app** using [Gradio](https://gradio.app/). This creates a shareable URL that anyone can use — no coding required on their end.\n\n> This cell requires a Groq API key to be set above. The `share=True` parameter creates a temporary public URL (valid for 72 hours) that you can share with colleagues.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "try:\n    !pip install gradio -q\n    import gradio as gr\n\n    def extract_article(text):\n        \"\"\"Extract structured information from an article.\"\"\"\n        if not text.strip():\n            return {\"error\": \"Please enter some text\"}\n        result = extract_with_retry(text, ArticleAnalysis)\n        if result:\n            return result.model_dump()\n        return {\"error\": \"Extraction failed — check your API key\"}\n\n    demo = gr.Interface(\n        fn=extract_article,\n        inputs=gr.Textbox(lines=10, placeholder=\"Paste an article here...\"),\n        outputs=gr.JSON(label=\"Extracted Data\"),\n        title=\"Article Analyzer\",\n        description=\"Extract structured information (title, summary, institutions, claims, sentiment) from any article using LLM + Pydantic.\",\n        examples=[[article_text[:500]]],\n    )\n    demo.launch(share=True)\n\nexcept ImportError:\n    print(\"Gradio not available. Install with: pip install gradio\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **LLMs are powerful zero-shot classifiers.** By simply describing categories in a prompt, we achieve competitive accuracy without any training data. This is transformative for cold-start problems where labeled data does not exist.\n",
    "\n",
    "2. **Structured output via Pydantic makes LLM outputs reliable.** Combining `response_format={\"type\": \"json_object\"}` with Pydantic validation ensures we always get clean, typed data -- no more parsing free-form text.\n",
    "\n",
    "3. **`extract_with_retry()` handles API flakiness.** Real-world APIs have transient errors, rate limits, and occasional malformed responses. Exponential backoff is a simple but essential production pattern.\n",
    "\n",
    "4. **The right tool for the job depends on your constraints:**\n",
    "\n",
    "| | NB01: TF-IDF | NB02: SBERT | NB03: LLM Zero-shot |\n",
    "|---|---|---|---|\n",
    "| Training data needed | Yes (hundreds) | Yes (hundreds) | No |\n",
    "| Training time | Seconds | Minutes | None |\n",
    "| Inference speed | Microseconds | Milliseconds | ~0.5 seconds |\n",
    "| GPU required | No | Optional | No (API) |\n",
    "| Privacy | Full (local) | Full (local) | Data sent to API |\n",
    "| Structured extraction | No | No | Yes |\n",
    "| Cost at scale | Free | Free | Pay per token |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **NB04** we will explore **unsupervised topic discovery** -- finding structure in text when we do not even know what the categories should be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}