{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB09: Fine-tuning Qwen3-4B for Structured Extraction\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB09_finetuning_qwen3.ipynb)\n",
    "\n",
    "**Duration:** 75 minutes\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Understand LoRA** parameter-efficient fine-tuning and why it matters\n",
    "2. **Prepare instruction-tuning data** in the conversational chat format\n",
    "3. **Fine-tune a 4B parameter model** on a free Colab T4 GPU using Unsloth\n",
    "4. **Evaluate structured output quality** — not just classification accuracy, but JSON validity and extraction completeness\n",
    "\n",
    "---\n",
    "\n",
    "> **Requires T4 GPU runtime** — go to **Runtime -> Change runtime type -> T4 GPU** before running any cells.\n",
    ">\n",
    "> We fine-tune Qwen3-4B to produce **full EU AI Act structured assessments** from EUIPO trademark descriptions — the same structured output that the kimi-k2 teacher produced in NB08, but from a model 250x smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
    "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install scikit-learn pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Fine-tune for Structured Output?\n",
    "\n",
    "In NB08, we saw two approaches to distillation:\n",
    "- **sklearn on embeddings** — fast, but can only predict a label (\"high\", \"minimal\", etc.)\n",
    "- **Teacher LLM (kimi-k2)** — produces full structured JSON, but requires API calls and is slow\n",
    "\n",
    "Fine-tuning bridges the gap: we teach a **small, local model** (Qwen3-4B, ~4 GB) to produce the full structured output that the large teacher model generated. The result:\n",
    "- Classification **and** extraction (capabilities, sectors, rationale)\n",
    "- Runs locally — no API calls, no cost, full privacy\n",
    "- Fast inference (~50ms vs ~300ms for the teacher API)\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "**LoRA** (Low-Rank Adaptation) makes fine-tuning feasible on free hardware:\n",
    "\n",
    "- **Freeze** all original model weights (4B parameters)\n",
    "- **Add** small trainable adapter matrices to attention layers\n",
    "- These adapters use **low-rank decomposition**: instead of a full `d×d` update, we use two smaller matrices `d×r` and `r×d` where `r=32 << d`\n",
    "- Result: **~1.6% trainable parameters**, fits in a free T4 (16 GB VRAM)\n",
    "\n",
    "Combined with **4-bit quantization** (QLoRA), the full model + training overhead fits comfortably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-4B-Instruct-2507\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable:,} / {total:,} ({trainable/total:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing Training Data\n",
    "\n",
    "We load the conversation data saved from NB08. Each example is a user/assistant pair:\n",
    "- **User:** \"Assess this EUIPO trademark under the EU AI Act: ...\"\n",
    "- **Assistant:** JSON with `is_ai_related`, `risk_tier`, `confidence`, `ai_capabilities`, `target_sectors`, `risk_rationale`\n",
    "\n",
    "We apply the `qwen3-instruct` chat template and use `train_on_responses_only` so the model only learns from the assistant outputs (not the user prompts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load NB08 output\n",
    "NB08_TRAIN = \"trademark_ai_act_conversations.json\"\n",
    "NB08_TEST = \"trademark_ai_act_test.json\"\n",
    "REPO_BASE = \"https://raw.githubusercontent.com/RJuro/unistra-nlp2026/main/data/trademarks\"\n",
    "\n",
    "def load_conversations(filepath, fallback_url=None):\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath) as f:\n",
    "            return json.load(f)\n",
    "    elif fallback_url:\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(fallback_url, filepath)\n",
    "        with open(filepath) as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{filepath} not found. Run NB08 first to generate it.\")\n",
    "\n",
    "train_convos = load_conversations(NB08_TRAIN)\n",
    "test_convos = load_conversations(NB08_TEST)\n",
    "\n",
    "print(f\"Training conversations: {len(train_convos)}\")\n",
    "print(f\"Test conversations: {len(test_convos)}\")\n",
    "print(f\"\\nExample (user):\")\n",
    "print(train_convos[0]['conversations'][0]['content'][:200])\n",
    "print(f\"\\nExample (assistant):\")\n",
    "print(train_convos[0]['conversations'][1]['content'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen3-instruct\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_convos)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "eval_dataset = Dataset.from_list(test_convos)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Eval examples: {len(eval_dataset)}\")\n",
    "print(f\"\\nFormatted example (first 400 chars):\")\n",
    "print(train_dataset[0]['text'][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training with SFTTrainer\n",
    "\n",
    "We use the `SFTTrainer` from `trl` with these key settings:\n",
    "- **Batch size 2 x 4 accumulation** = effective batch size 8\n",
    "- **Learning rate 2e-4** — standard for LoRA\n",
    "- **3 epochs** over ~350 examples = ~130 steps\n",
    "- **`train_on_responses_only`** — the model only learns from the JSON output, not the user prompt\n",
    "\n",
    "Training takes about 10-15 minutes on a T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Only train on assistant responses\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|im_start|>user\\n\",\n",
    "    response_part=\"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "# Verify masking: show that only assistant tokens have labels\n",
    "sample_labels = trainer.train_dataset[0][\"labels\"]\n",
    "masked = tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in sample_labels])\n",
    "print(\"Masked tokens (only assistant response shown):\")\n",
    "print(masked.replace(tokenizer.pad_token, \" \")[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_mem = torch.cuda.max_memory_reserved() / 1e9\n",
    "print(f\"GPU: {gpu_stats.name} ({gpu_stats.total_mem/1e9:.0f} GB)\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "peak_mem = torch.cuda.max_memory_reserved() / 1e9\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Time: {trainer_stats.metrics['train_runtime']:.0f}s ({trainer_stats.metrics['train_runtime']/60:.1f} min)\")\n",
    "print(f\"  Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"  Peak GPU memory: {peak_mem:.1f} GB (training used {peak_mem - start_mem:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation: Structured Output Quality\n",
    "\n",
    "We evaluate the fine-tuned model on the held-out test set from NB08. Unlike simple classification, we measure:\n",
    "\n",
    "1. **JSON validity** — does the model produce parseable JSON?\n",
    "2. **Classification accuracy** — is the `risk_tier` correct?\n",
    "3. **Field completeness** — does the model fill in `ai_capabilities`, `target_sectors`, and `risk_rationale`?\n",
    "4. **Qualitative review** — do the extracted fields make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def predict_structured(user_content: str) -> dict:\n",
    "    \"\"\"Generate a structured assessment from the fine-tuned model.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "        top_k=20,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    # Try to parse JSON from response\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to find JSON in the response\n",
    "        start = response.find('{')\n",
    "        end = response.rfind('}') + 1\n",
    "        if start >= 0 and end > start:\n",
    "            try:\n",
    "                return json.loads(response[start:end])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return {\"_raw\": response, \"_parse_error\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "RISK_TIERS = [\"unacceptable\", \"high\", \"limited\", \"minimal\", \"not_ai\"]\n",
    "\n",
    "results = []\n",
    "for convo in tqdm(test_convos, desc=\"Evaluating\"):\n",
    "    user_msg = convo['conversations'][0]['content']\n",
    "    gold = json.loads(convo['conversations'][1]['content'])\n",
    "    pred = predict_structured(user_msg)\n",
    "\n",
    "    results.append({\n",
    "        'gold_tier': gold.get('risk_tier', ''),\n",
    "        'pred_tier': pred.get('risk_tier', ''),\n",
    "        'json_valid': '_parse_error' not in pred,\n",
    "        'has_capabilities': len(pred.get('ai_capabilities', [])) > 0,\n",
    "        'has_sectors': len(pred.get('target_sectors', [])) > 0,\n",
    "        'has_rationale': len(pred.get('risk_rationale', '')) > 10,\n",
    "        'gold': gold,\n",
    "        'pred': pred,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"Evaluated: {len(results_df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. JSON validity\n",
    "json_rate = results_df['json_valid'].mean()\n",
    "print(f\"JSON validity rate: {json_rate:.0%}\")\n",
    "\n",
    "# 2. Classification accuracy (on valid JSON only)\n",
    "valid = results_df[results_df['json_valid'] & results_df['pred_tier'].isin(RISK_TIERS)]\n",
    "if len(valid) > 0:\n",
    "    acc = accuracy_score(valid['gold_tier'], valid['pred_tier'])\n",
    "    print(f\"\\nClassification accuracy (on valid outputs): {acc:.1%}\")\n",
    "    print(classification_report(valid['gold_tier'], valid['pred_tier'], zero_division=0))\n",
    "\n",
    "# 3. Field completeness\n",
    "print(f\"\\nField completeness (on valid JSON):\")\n",
    "valid_json = results_df[results_df['json_valid']]\n",
    "print(f\"  ai_capabilities present: {valid_json['has_capabilities'].mean():.0%}\")\n",
    "print(f\"  target_sectors present:  {valid_json['has_sectors'].mean():.0%}\")\n",
    "print(f\"  risk_rationale present:  {valid_json['has_rationale'].mean():.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Qualitative review: show 5 examples side-by-side\n",
    "print(\"=\" * 70)\n",
    "print(\"QUALITATIVE COMPARISON: Teacher (gold) vs Fine-tuned Model (pred)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    desc = test_convos[i]['conversations'][0]['content']\n",
    "    # Extract just the goods/services line\n",
    "    goods_line = [l for l in desc.split('\\n') if l.startswith('Goods/Services:')]\n",
    "    goods_text = goods_line[0][:150] + '...' if goods_line else desc[:150] + '...'\n",
    "\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Input: {goods_text}\")\n",
    "    print(f\"\\nTeacher:    tier={row['gold']['risk_tier']}, caps={row['gold'].get('ai_capabilities', [])}\")\n",
    "    if row['json_valid']:\n",
    "        print(f\"Fine-tuned: tier={row['pred'].get('risk_tier', '?')}, caps={row['pred'].get('ai_capabilities', [])}\")\n",
    "        print(f\"Rationale:  {row['pred'].get('risk_rationale', 'N/A')[:120]}\")\n",
    "    else:\n",
    "        print(f\"Fine-tuned: [INVALID JSON] {str(row['pred'].get('_raw', ''))[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The 3-Way Comparison\n",
    "\n",
    "Let's put all approaches side by side to see the full picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "ft_acc = accuracy_score(valid['gold_tier'], valid['pred_tier']) if len(valid) > 0 else 0\n",
    "\n",
    "print(f\"{'='*65}\")\n",
    "print(f\"{'Method':<30} {'Classification':>15} {'Extraction':>15}\")\n",
    "print(f\"{'-'*65}\")\n",
    "print(f\"{'kimi-k2 teacher (NB08)':<30} {'baseline':>15} {'full JSON':>15}\")\n",
    "print(f\"{'sklearn E5+LR (NB08)':<30} {'comparable':>15} {'CANNOT':>15}\")\n",
    "print(f\"{'Qwen3-4B LoRA (this NB)':<30} {f'{ft_acc:.0%}':>15} {f'{json_rate:.0%} valid':>15}\")\n",
    "print(f\"{'='*65}\")\n",
    "print(f\"\\nThe fine-tuned 4B model produces structured output that sklearn cannot.\")\n",
    "print(f\"It runs locally, costs nothing, and is ~6x faster than the teacher API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Export to GGUF & Publish on HuggingFace Hub\n\nInstead of downloading a GGUF file from Colab (slow and error-prone), we push directly to HuggingFace Hub. Then Ollama can load it in one command:\n\n```bash\nollama run hf.co/YOUR_USERNAME/trademark-aiact-GGUF\n```\n\nNo manual file transfers needed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 1: Log in to HuggingFace ---\n# In Colab: add your HF token to Secrets (key icon in left sidebar)\n# Name it HF_TOKEN, toggle \"Notebook access\" on\n\nfrom huggingface_hub import login\nimport os\n\n# Try Colab secrets first, then env var, then prompt\ntry:\n    from google.colab import userdata\n    hf_token = userdata.get(\"HF_TOKEN\")\nexcept Exception:\n    hf_token = os.environ.get(\"HF_TOKEN\")\n\nif hf_token:\n    login(token=hf_token)\n    print(\"Logged in to HuggingFace Hub!\")\nelse:\n    print(\"No HF_TOKEN found. Running login() — paste your token below:\")\n    login()\n\n# --- Step 2: Choose your repo name ---\nHF_USERNAME = \"YOUR_USERNAME\"  # <-- change this to your HF username\nREPO_NAME = f\"{HF_USERNAME}/trademark-aiact-GGUF\"\nprint(f\"\\nWill publish to: https://huggingface.co/{REPO_NAME}\")\n\n# --- Step 3: Push GGUF to Hub ---\n# This merges LoRA weights back into the base model, quantizes to Q4_K_M,\n# and uploads the .gguf file directly to your HF repo.\nmodel.push_to_hub_gguf(\n    REPO_NAME,\n    tokenizer,\n    quantization_method=\"q4_k_m\",\n    token=hf_token,\n)\n\nprint(f\"\\nModel published! Use it with Ollama:\")\nprint(f\"  ollama run hf.co/{REPO_NAME}\")\nprint(f\"\\nOr pull first, then run:\")\nprint(f\"  ollama pull hf.co/{REPO_NAME}\")\nprint(f\"  ollama run hf.co/{REPO_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    !pip install gradio -q\n",
    "    import gradio as gr\n",
    "\n",
    "    def assess_ui(description, owner=\"\"):\n",
    "        if not description.strip():\n",
    "            return \"Please enter a trademark description.\"\n",
    "        user_msg = f\"Assess this EUIPO trademark under the EU AI Act:\\n\\nOwner: {owner}\\nGoods/Services: {description}\"\n",
    "        result = predict_structured(user_msg)\n",
    "        return f\"```json\\n{json.dumps(result, indent=2)}\\n```\"\n",
    "\n",
    "    demo = gr.Interface(\n",
    "        fn=assess_ui,\n",
    "        inputs=[\n",
    "            gr.Textbox(lines=4, placeholder=\"Enter EUIPO goods/services description...\", label=\"Description\"),\n",
    "            gr.Textbox(placeholder=\"Optional owner name\", label=\"Owner\"),\n",
    "        ],\n",
    "        outputs=gr.Markdown(label=\"EU AI Act Assessment\"),\n",
    "        title=\"EU AI Act Trademark Classifier (Fine-tuned Qwen3-4B)\",\n",
    "        description=\"Classify EUIPO trademark filings under the EU AI Act risk framework with structured extraction.\",\n",
    "        examples=[\n",
    "            [\"facial recognition software; biometric identification systems; software for law enforcement agencies; real-time surveillance camera software\", \"CLEARVIEW AI INC.\"],\n",
    "            [\"computer keyboards; screens; computer hardware; headphones; software for mobile phones\", \"SAMSUNG ELECTRONICS\"],\n",
    "            [\"chatbot software; conversational AI platforms; virtual assistant software for customer service\", \"INTERCOM INC.\"],\n",
    "            [\"automated hiring assessment software; candidate screening tools; AI-powered recruitment platforms\", \"HIREVUE INC.\"],\n",
    "        ],\n",
    "    )\n",
    "    demo.launch(share=True)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Gradio not available. Install with: pip install gradio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise\n",
    "\n",
    "Try modifying the fine-tuning setup:\n",
    "\n",
    "1. **Different LoRA rank**: Try `r=8` or `r=64`. How does this affect trainable parameters and output quality?\n",
    "2. **Different epochs**: Try `num_train_epochs=1` or `5`. Does more training improve JSON validity?\n",
    "3. **Classify-only format**: Reformat the training data so the assistant just returns the `risk_tier` label (no JSON). Compare accuracy to the full structured output format. Is classification accuracy better when the model doesn't have to produce JSON?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Try a different LoRA rank\n",
    "# ------------------------------------\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Reload the base model:\n",
    "#    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#        model_name=\"unsloth/Qwen3-4B-Instruct-2507\", ...)\n",
    "\n",
    "# 2. Try r=8:\n",
    "#    model = FastLanguageModel.get_peft_model(model, r=8, ...)\n",
    "\n",
    "# 3. Train and compare:\n",
    "#    trainer = SFTTrainer(...)\n",
    "#    trainer.train()\n",
    "\n",
    "# Record your observations:\n",
    "# r=8:  trainable params = ???, JSON valid = ???%, accuracy = ???%\n",
    "# r=32: trainable params = ???, JSON valid = ???%, accuracy = ???%\n",
    "# r=64: trainable params = ???, JSON valid = ???%, accuracy = ???%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Summary & Takeaways\n\n**What we built:**\n- A fine-tuned Qwen3-4B model that produces **full EU AI Act structured assessments** from EUIPO trademark descriptions\n- The model learned to output valid JSON with classification, extracted capabilities, sectors, and rationale\n- Published to HuggingFace Hub as GGUF — loadable directly via `ollama run hf.co/...`\n\n**The NB08 → NB09 pipeline:**\n1. **Teacher labels** (kimi-k2, 1T params) → structured JSON annotations\n2. **Synthetic augmentation** → balanced training set (~400 examples)\n3. **Quality filtering** → confidence threshold + validation\n4. **Fine-tuning** (Qwen3-4B, LoRA r=32) → learns to reproduce the full structured output\n5. **Publish** → GGUF on HuggingFace Hub → `ollama run` for zero-cost local inference\n\n**Key takeaways:**\n- **LoRA makes fine-tuning accessible.** ~1.6% trainable parameters, fits a free T4 GPU\n- **Structured output is learnable.** A 4B model can learn to produce valid JSON with multiple extracted fields — not just classification labels\n- **~400 examples is enough for LoRA.** You don't need thousands of examples for task-specific fine-tuning\n- **The distillation story is complete:** 1T-param teacher → 4B student, full structured output preserved, local and free"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}