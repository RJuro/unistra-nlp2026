{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB09: Fine-tuning Qwen3-4B with LoRA\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RJuro/unistra-nlp2026/blob/main/notebooks/NB09_finetuning_qwen3.ipynb)\n",
    "\n",
    "**Duration:** 75 minutes\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Understand LoRA** parameter-efficient fine-tuning and why it matters\n",
    "2. **Prepare instruction-tuning data** in the conversational chat format\n",
    "3. **Fine-tune a 4B parameter model** on a free Colab T4 GPU using Unsloth\n",
    "4. **Evaluate fine-tuned vs zero-shot** performance on a classification task\n",
    "\n",
    "---\n",
    "\n",
    "> **Requires T4 GPU runtime** -- go to **Runtime -> Change runtime type -> T4 GPU** before running any cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get latest transformers and trl\n",
    "!pip install --upgrade transformers trl datasets pandas scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Fine-tune?\n",
    "\n",
    "Zero-shot LLMs are impressively versatile -- you can prompt them for almost any task and get reasonable results out of the box. But they are **generic**: they have no knowledge of your specific task, domain, or label set.\n",
    "\n",
    "**Fine-tuning** adapts a pre-trained model to **your** specific task by continuing training on task-specific examples. The benefits:\n",
    "\n",
    "- **Higher accuracy** on your target task\n",
    "- **More consistent** output format (no parsing headaches)\n",
    "- **Faster inference** (the model \"knows\" the task, needs fewer tokens)\n",
    "- **Smaller models can match larger ones** when fine-tuned\n",
    "\n",
    "The catch? Full fine-tuning of a 4B parameter model requires enormous GPU memory. This is where **LoRA** comes in -- with LoRA, we only train **~1% of parameters**, which fits comfortably in the free Colab T4 (16 GB VRAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is LoRA?\n",
    "\n",
    "**LoRA** (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique introduced by Hu et al. (2021).\n",
    "\n",
    "The key idea is simple:\n",
    "\n",
    "- **Freeze** all the original model weights (billions of parameters)\n",
    "- **Add** small trainable matrices (adapters) to specific layers\n",
    "- These adapters use a **low-rank decomposition**: instead of a full weight update matrix of size `d x d`, we use two smaller matrices of size `d x r` and `r x d`, where `r << d` (the \"rank\")\n",
    "\n",
    "**Why does this work?**\n",
    "\n",
    "- Research shows that weight updates during fine-tuning have low intrinsic rank\n",
    "- A rank of 16 (out of thousands) captures most of the task-specific adaptation\n",
    "- Result: **~1-2% trainable parameters**, **~70% less memory**, **same quality** for many tasks\n",
    "\n",
    "Combined with **4-bit quantization** (QLoRA), we can fine-tune a 4B parameter model on a free T4 GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-4B\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,  # QLoRA — 4-bit quantization\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,                    # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable:,} / {total:,} ({trainable/total:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing Training Data\n",
    "\n",
    "For instruction-tuning (also called supervised fine-tuning / SFT), we need to format each training example as a **conversation**:\n",
    "\n",
    "1. **System message** -- tells the model its role and the task\n",
    "2. **User message** -- the input (the tweet to classify)\n",
    "3. **Assistant message** -- the expected output (the emotion label)\n",
    "\n",
    "We use the **dair-ai/emotion** dataset — ~416K English tweets labeled with 6 emotions (sadness, joy, love, anger, fear, surprise). This creates a natural NB08→NB09 pipeline: in NB08 we distilled emotion labels from an LLM, and now we fine-tune a small model on them.\n",
    "\n",
    "We use the tokenizer's built-in `apply_chat_template()` to format this correctly for Qwen3, including all special tokens the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset, Dataset\n\nEMOTION_LABELS = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\nCATEGORIES = sorted(EMOTION_LABELS)\n\n# ── Option A: Load distilled labels from NB08 (if available) ────────\nNB08_CSV = \"emotion_distilled_labels.csv\"\n\nif os.path.exists(NB08_CSV):\n    print(f\"Found NB08 distilled labels: {NB08_CSV}\")\n    distilled = pd.read_csv(NB08_CSV)\n    distilled = distilled.rename(columns={\"llm_label\": \"label_name\"})\n    \n    # Use distilled data for training, fresh HF data for eval\n    train_subset = distilled.sample(min(800, len(distilled)), random_state=42)\n    \n    emotion_ds = load_dataset(\"dair-ai/emotion\")\n    test_full = pd.DataFrame(emotion_ds[\"test\"])\n    test_full[\"label_name\"] = test_full[\"label\"].map(lambda x: EMOTION_LABELS[x])\n    eval_subset = test_full.sample(200, random_state=42)\n    \n    print(f\"Training on {len(train_subset)} distilled examples from NB08\")\n    print(f\"Evaluating on {len(eval_subset)} gold-labeled examples from HF\")\n\nelse:\n    # ── Option B: Load fresh from HuggingFace ───────────────────────\n    print(f\"NB08 CSV not found — loading fresh data from HuggingFace\")\n    emotion_ds = load_dataset(\"dair-ai/emotion\")\n    \n    train_full = pd.DataFrame(emotion_ds[\"train\"])\n    train_full[\"label_name\"] = train_full[\"label\"].map(lambda x: EMOTION_LABELS[x])\n    \n    test_full = pd.DataFrame(emotion_ds[\"test\"])\n    test_full[\"label_name\"] = test_full[\"label\"].map(lambda x: EMOTION_LABELS[x])\n    \n    np.random.seed(42)\n    train_subset = train_full.sample(800, random_state=42)\n    eval_subset = test_full.sample(200, random_state=42)\n    \n    print(f\"Training on {len(train_subset)} examples from HuggingFace\")\n    print(f\"Evaluating on {len(eval_subset)} examples from HuggingFace\")\n\nprint(f\"\\nEmotion distribution (train):\")\nprint(train_subset['label_name'].value_counts())\n\ndef format_instruction(row):\n    \"\"\"Convert a labeled example into an instruction-tuning format.\"\"\"\n    return {\n        \"text\": tokenizer.apply_chat_template([\n            {\"role\": \"system\", \"content\": f\"You classify tweets into one of these emotion categories: {CATEGORIES}. Respond with only the emotion label.\"},\n            {\"role\": \"user\", \"content\": f\"Classify the emotion in this tweet:\\n\\n{row['text'][:500]}\"},\n            {\"role\": \"assistant\", \"content\": row['label_name']}\n        ], tokenize=False)\n    }\n\n# Apply formatting\ntrain_data = [format_instruction(row) for _, row in train_subset.iterrows()]\neval_data = [format_instruction(row) for _, row in eval_subset.iterrows()]\n\ntrain_dataset = Dataset.from_list(train_data)\neval_dataset = Dataset.from_list(eval_data)\n\nprint(f\"\\nTraining examples: {len(train_dataset)}\")\nprint(f\"Eval examples: {len(eval_dataset)}\")\nprint(f\"\\nExample formatted input:\")\nprint(train_dataset[0]['text'][:500])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training with SFTTrainer\n",
    "\n",
    "We use the `SFTTrainer` from the `trl` library (Transformer Reinforcement Learning), which handles all the details of supervised fine-tuning.\n",
    "\n",
    "Key training hyperparameters:\n",
    "- **Batch size 2 x 4 accumulation steps** = effective batch size of 8\n",
    "- **Learning rate 2e-4** -- standard for LoRA fine-tuning\n",
    "- **3 epochs** -- enough to learn the task without overfitting\n",
    "- **8-bit AdamW** -- memory-efficient optimizer\n",
    "\n",
    "Since tweets are short, training on 800 examples should be fast -- about 5-10 minutes on a T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "print(f\"GPU: {gpu_stats.name} ({gpu_stats.total_mem/1e9:.0f} GB)\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Time: {trainer_stats.metrics['train_runtime']:.0f}s\")\n",
    "print(f\"  Loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"  GPU memory peak: {torch.cuda.max_memory_allocated()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation: Fine-tuned vs Zero-shot\n",
    "\n",
    "Now let's see how our fine-tuned model performs on emotion classification compared to zero-shot prompting. We switch the model to inference mode (disables dropout, merges LoRA weights for speed) and run it over our held-out evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def classify_with_model(text: str) -> str:\n",
    "    \"\"\"Classify text using the fine-tuned model.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You classify tweets into one of these emotion categories: {CATEGORIES}. Respond with only the emotion label.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the emotion in this tweet:\\n\\n{text[:500]}\"}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=50, temperature=0.0, do_sample=False)\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "for _, row in tqdm(eval_subset.iterrows(), total=len(eval_subset), desc=\"Evaluating\"):\n",
    "    pred = classify_with_model(row['text'])\n",
    "    # Match to closest valid category\n",
    "    pred_clean = pred.strip().strip('\"').strip(\"'\").lower()\n",
    "    if pred_clean in CATEGORIES:\n",
    "        predictions.append(pred_clean)\n",
    "    else:\n",
    "        # Fuzzy match\n",
    "        from difflib import get_close_matches\n",
    "        match = get_close_matches(pred_clean, CATEGORIES, n=1, cutoff=0.3)\n",
    "        predictions.append(match[0] if match else CATEGORIES[0])\n",
    "\n",
    "eval_subset = eval_subset.copy()\n",
    "eval_subset['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "acc = accuracy_score(eval_subset['label_name'], eval_subset['prediction'])\n",
    "print(f\"Fine-tuned Model Accuracy: {acc:.1%}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(eval_subset['label_name'], eval_subset['prediction'], zero_division=0))\n",
    "\n",
    "# Compare to baselines\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Comparison to approaches on this dataset:\")\n",
    "print(f\"  NB08 LLM zero-shot (teacher): ~70-80%\")\n",
    "print(f\"  NB08 TF-IDF student:          ~60-70%\")\n",
    "print(f\"  NB08 SBERT student:            ~70-80%\")\n",
    "print(f\"  NB09 Fine-tuned Qwen3:         {acc:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus: Export to GGUF for Local Use\n",
    "\n",
    "One of the great advantages of fine-tuning an open-weights model is that you **own** the result. You can export it in GGUF format and run it locally with tools like [Ollama](https://ollama.com/) or [llama.cpp](https://github.com/ggerganov/llama.cpp) -- no cloud API needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in GGUF format for use with Ollama\n",
    "model.save_pretrained_gguf(\"model_gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "print(\"Model saved in GGUF format!\")\n",
    "print(\"To use with Ollama:\")\n",
    "print(\"  1. Download the .gguf file\")\n",
    "print(\"  2. Create a Modelfile:\")\n",
    "print('     FROM ./model_gguf-unsloth.Q4_K_M.gguf')\n",
    "print(\"  3. ollama create my-classifier -f Modelfile\")\n",
    "print(\"  4. ollama run my-classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Bonus: Deploy as a Gradio App\n\nLet's create an interactive demo of our fine-tuned emotion classifier. Users can type tweets and see the model's predictions in real time.\n\n> **Note:** This runs inference on the GPU, so it works best in the Colab session where the model is loaded.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "try:\n    !pip install gradio -q\n    import gradio as gr\n\n    def classify_emotion(text):\n        \"\"\"Classify emotion in text using the fine-tuned model.\"\"\"\n        if not text.strip():\n            return \"Please enter some text.\"\n        prediction = classify_with_model(text)\n        return f\"## Predicted emotion: **{prediction}**\"\n\n    demo = gr.Interface(\n        fn=classify_emotion,\n        inputs=gr.Textbox(lines=3, placeholder=\"Type a tweet or short text...\"),\n        outputs=gr.Markdown(label=\"Prediction\"),\n        title=\"Emotion Classifier (Fine-tuned Qwen3-4B)\",\n        description=\"Classify the emotion in text as: sadness, joy, love, anger, fear, or surprise. Powered by a LoRA-fine-tuned Qwen3-4B model.\",\n        examples=[\n            [\"I can't believe how beautiful this sunset is tonight!\"],\n            [\"Just got rejected from my dream job. Feeling awful.\"],\n            [\"My best friend surprised me with concert tickets!\"],\n            [\"I'm worried about the exam results coming out tomorrow.\"],\n        ],\n    )\n    demo.launch(share=True)\n\nexcept ImportError:\n    print(\"Gradio not available. Install with: pip install gradio\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Exercise\n\nTry modifying the fine-tuning setup and observe how it affects performance:\n\n1. **Different LoRA rank**: Try `r=4`, `r=8`, or `r=32`. How does this affect the number of trainable parameters and the final accuracy?\n2. **Different number of epochs**: Try `num_train_epochs=1` or `num_train_epochs=5`. Does more training help, or does the model start overfitting?\n3. **More training data**: Increase from 800 to 2000 or 5000 examples. Does the fine-tuned model improve?\n4. **Compare gold vs distilled labels**: If you ran NB08 first, the data cell above loaded distilled labels automatically. Try re-running without the CSV (rename it) to use gold HuggingFace labels instead. How does label noise from distillation affect fine-tuning quality?\n\nUse the skeleton below to experiment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise: Experiment with different LoRA configurations\n",
    "# ============================================================\n",
    "\n",
    "# TODO: Reload the base model (copy from cell above)\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(...)\n",
    "\n",
    "# TODO: Try a different LoRA rank\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r=4,  # <-- try 4, 8, or 32\n",
    "#     ...\n",
    "# )\n",
    "\n",
    "# TODO: Try a different number of epochs in TrainingArguments\n",
    "# args = TrainingArguments(\n",
    "#     num_train_epochs=1,  # <-- try 1 or 5\n",
    "#     ...\n",
    "# )\n",
    "\n",
    "# TODO: Train, evaluate, and compare results\n",
    "# trainer = SFTTrainer(...)\n",
    "# trainer.train()\n",
    "\n",
    "# Record your observations here:\n",
    "# r=4:  trainable params = ???, accuracy = ???\n",
    "# r=16: trainable params = ???, accuracy = ???\n",
    "# r=32: trainable params = ???, accuracy = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Takeaways\n",
    "\n",
    "In this notebook we fine-tuned a 4-billion parameter language model on emotion classification using a free Colab T4 GPU. Here are the key takeaways:\n",
    "\n",
    "- **LoRA makes fine-tuning accessible on free hardware.** By only training ~1-2% of parameters, we reduce memory requirements by ~70%, making it possible to fine-tune large models on a single T4 GPU (16 GB VRAM).\n",
    "\n",
    "- **Qwen3-4B is a capable base model.** Modern open-weights models like Qwen3 provide strong foundations for task-specific fine-tuning. The 4B parameter size hits a sweet spot between capability and efficiency.\n",
    "\n",
    "- **Fine-tuning can significantly improve task-specific performance.** Compared to zero-shot prompting, fine-tuned models produce more consistent outputs, follow the expected format more reliably, and often achieve higher accuracy on emotion detection.\n",
    "\n",
    "- **NB08→NB09 pipeline is a real-world pattern.** Using LLM distillation to create training data (NB08), then fine-tuning a small model on those labels (NB09), is a production-ready approach for building custom classifiers.\n",
    "\n",
    "- **Export to GGUF enables local deployment.** Unlike API-based solutions, fine-tuned open models can be exported and run locally with tools like Ollama -- giving you full control, privacy, and zero per-token costs.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- Try fine-tuning on your own datasets and tasks\n",
    "- Experiment with different base models (Llama, Mistral, Gemma)\n",
    "- Explore more advanced techniques: DPO, RLHF, multi-task fine-tuning\n",
    "- Deploy your fine-tuned model as a local API with Ollama or vLLM"
   ]
  }
 ]
}